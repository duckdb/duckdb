# name: test/sql/copy/s3/glob_s3_paging.test_slow
# description: Test globbing of a large number of parquet files to test the paging mechanism
# group: [s3]

require parquet

require httpfs

require-env S3_TEST_SERVER_AVAILABLE 1

# Require that these environment variables are also set

require-env AWS_DEFAULT_REGION

require-env AWS_ACCESS_KEY_ID

require-env AWS_SECRET_ACCESS_KEY

require-env DUCKDB_S3_ENDPOINT

require-env DUCKDB_S3_USE_SSL

# override the default behaviour of skipping HTTP errors and connection failures: this test fails on connection issues
set ignore_error_messages

statement ok
set http_timeout=120000;

# More retries (longest wait will be 25600ms)
statement ok
set http_retries=6;

# Test should be a bit faster using the metadata cache
statement ok
SET enable_http_metadata_cache=true;

foreach urlstyle path vhost

statement ok
SET s3_url_style='${urlstyle}'

## For both formats we generate 2000 files which we will glob to test the paging mechanism of aws ListObjectV2 call is handled properly
foreach format parquet csv

foreach i 0 1

foreach j 0 1 2 3 4 5 6 7 8 9

foreach k 0 1 2 3 4 5 6 7 8 9

foreach l 0 1 2 3 4 5 6 7 8 9

statement ok
COPY (select (${i}${j}${k}${l})::INT as column0) to 's3://test-bucket/parquet_glob_s3_paging/paging/t${i}${j}${k}${l}-${urlstyle}-urls.${format}';

endloop

endloop

endloop

endloop

# Begin tests
query I
select sum(column0) from 's3://test-bucket/parquet_glob_s3_paging/paging/t*-${urlstyle}-urls.${format}'
----
1999000

endloop

endloop