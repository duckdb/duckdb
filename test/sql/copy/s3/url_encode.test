# name: test/sql/copy/s3/url_encode.test
# description: S3 Url encoding
# group: [s3]

require parquet

require httpfs

require-env S3_TEST_SERVER_AVAILABLE 1

# Require that these environment variables are also set

require-env AWS_DEFAULT_REGION

require-env AWS_ACCESS_KEY_ID

require-env AWS_SECRET_ACCESS_KEY

require-env DUCKDB_S3_ENDPOINT

require-env DUCKDB_S3_USE_SSL

# override the default behaviour of skipping HTTP errors and connection failures: this test fails on connection issues
set ignore_error_messages

statement ok
CREATE TABLE test_1 as (SELECT 1 FROM range(0,5));
CREATE TABLE test_2 as (SELECT 2 FROM range(0,5));
CREATE TABLE test_3 as (SELECT 3 FROM range(0,5));
CREATE TABLE test_4 as (SELECT 4 FROM range(0,5));

statement ok
COPY test_1 TO 's3://test-bucket-public/url_encode/just because you can doesnt mean you should.parquet' (FORMAT 'parquet');

statement ok
COPY test_2 TO 's3://test-bucket-public/url_encode/just+dont+use+plus+or+spaces+please.parquet' (FORMAT 'parquet');

statement ok
COPY test_3 TO 's3://test-bucket-public/url_encode/should:avoid:using:colon:in:paths.parquet' (FORMAT 'parquet');

# For S3 urls spaces are fine
query I
SELECT * FROM "s3://test-bucket-public/url_encode/just because you can doesnt mean you should.parquet" LIMIT 1;
----
1

# In S3 urls, + means a plus symbol
query I
SELECT * FROM "s3://test-bucket-public/url_encode/just+dont+use+plus+or+spaces+please.parquet" LIMIT 1;
----
2

# Colons in S3 urls are encoded by duckdb internaly like boto3 (issue #5502)
query I
SELECT * FROM "s3://test-bucket-public/url_encode/should:avoid:using:colon:in:paths.parquet" LIMIT 1;
----
3

# NOTE! For HTTP(s) urls, the + symbol is not encoded by duckdb, leaving it up to the server to decide if it should be interpreted
# as a space or a plus. In the case of AWS S3, they are interpreted as encoded spaces, however Minio does not
#query I
#SELECT * FROM "http://test-bucket-public.${DUCKDB_S3_ENDPOINT}/url_encode/just+because+you+can+doesnt+mean+you+should.parquet" LIMIT 1;
#----
#1

# For HTTP urls, we also allow regular spaces, which will get encoded to %20 by duckdb
query I
SELECT * FROM "http://test-bucket-public.${DUCKDB_S3_ENDPOINT}/url_encode/just because you can doesnt mean you should.parquet" LIMIT 1;
----
1

# For HTTP urls from AWS with + symbols, encoding them with %2B is required
query I
SELECT * FROM "http://test-bucket-public.${DUCKDB_S3_ENDPOINT}/url_encode/just%2Bdont%2Buse%2Bplus%2Bor%2Bspaces%2Bplease.parquet" LIMIT 1;
----
2

# However Minio interprets them as spaces so this works too
query I
SELECT * FROM "http://test-bucket-public.${DUCKDB_S3_ENDPOINT}/url_encode/just+dont+use+plus+or+spaces+please.parquet" LIMIT 1;
----
2

# Due to our support for query parameters, this will fail
statement error
COPY test_4 TO 's3://test-bucket-public/url_encode/question?marks?are?even?worse.parquet' (FORMAT 'parquet');
----
Invalid query parameters found.

# Enabling url compatibility mode will disable both Globs and query params
# allowing a user to query those hard-to-reach files
statement ok
SET s3_url_compatibility_mode=true;

statement ok
COPY test_4 TO 's3://test-bucket-public/url_encode/question?marks?and*stars[and]brackets.parquet' (FORMAT 'parquet');

query I
SELECT * FROM "s3://test-bucket-public/url_encode/question?marks?and*stars[and]brackets.parquet" LIMIT 1;
----
4

# HTTP urls will be encoded here
query I
SELECT * FROM "http://test-bucket-public.${DUCKDB_S3_ENDPOINT}/url_encode/question%3Fmarks%3Fand%2Astars%5Band%5Dbrackets.parquet" LIMIT 1;
----
4

statement ok
SET s3_url_compatibility_mode=false;