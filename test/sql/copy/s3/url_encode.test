# name: test/sql/copy/s3/url_encode.test
# description: S3 Url encoding
# group: [s3]

require parquet

require httpfs

require-env S3_TEST_SERVER_AVAILABLE 1

# Require that these environment variables are also set

require-env AWS_DEFAULT_REGION

require-env AWS_ACCESS_KEY_ID

require-env AWS_SECRET_ACCESS_KEY

require-env DUCKDB_S3_ENDPOINT

require-env DUCKDB_S3_USE_SSL

# override the default behaviour of skipping HTTP errors and connection failures: this test fails on connection issues
set ignore_error_messages

statement ok
CREATE TABLE test_1 as (SELECT 1 FROM range(0,5));
CREATE TABLE test_2 as (SELECT 2 FROM range(0,5));
CREATE TABLE test_3 as (SELECT 3 FROM range(0,5));
CREATE TABLE test_4 as (SELECT 4 FROM range(0,5));

foreach prefix s3:// r2:// gcs:// s3a:// s3n://

statement ok
COPY test_1 TO '${prefix}test-bucket-public/url_encode/just because you can doesnt mean you should.parquet' (FORMAT 'parquet');

statement ok
COPY test_2 TO '${prefix}test-bucket-public/url_encode/just+dont+use+plus+or+spaces+please.parquet' (FORMAT 'parquet');

statement ok
COPY test_3 TO '${prefix}test-bucket-public/url_encode/should:avoid:using:colon:in:paths.parquet' (FORMAT 'parquet');

# For S3 urls spaces are fine
query I
SELECT * FROM "${prefix}test-bucket-public/url_encode/just because you can doesnt mean you should.parquet" LIMIT 1;
----
1

# In S3 urls, + means a plus symbol
query I
SELECT * FROM "${prefix}test-bucket-public/url_encode/just+dont+use+plus+or+spaces+please.parquet" LIMIT 1;
----
2

# Colons in S3 urls are encoded by duckdb internaly like boto3 (issue #5502)
query I
SELECT * FROM "${prefix}test-bucket-public/url_encode/should:avoid:using:colon:in:paths.parquet" LIMIT 1;
----
3

# NOTE! For HTTP(s) urls, the + symbol is not encoded by duckdb, leaving it up to the server to decide if it should be interpreted
# as a space or a plus. In the case of AWS S3, they are interpreted as encoded spaces, however Minio does not
#query I
#SELECT * FROM "http://test-bucket-public.${DUCKDB_S3_ENDPOINT}/url_encode/just+because+you+can+doesnt+mean+you+should.parquet" LIMIT 1;
#----
#1

# For HTTP urls, we also allow regular spaces, which will get encoded to %20 by duckdb
query I
SELECT * FROM "http://test-bucket-public.${DUCKDB_S3_ENDPOINT}/url_encode/just because you can doesnt mean you should.parquet" LIMIT 1;
----
1

# For HTTP urls from AWS with + symbols, encoding them with %2B is required
query I
SELECT * FROM "http://test-bucket-public.${DUCKDB_S3_ENDPOINT}/url_encode/just%2Bdont%2Buse%2Bplus%2Bor%2Bspaces%2Bplease.parquet" LIMIT 1;
----
2

# However Minio interprets them as spaces so this works too
query I
SELECT * FROM "http://test-bucket-public.${DUCKDB_S3_ENDPOINT}/url_encode/just+dont+use+plus+or+spaces+please.parquet" LIMIT 1;
----
2

# Due to our support for query parameters, this will fail
statement error
COPY test_4 TO '${prefix}test-bucket-public/url_encode/question?marks?are?even?worse.parquet' (FORMAT 'parquet');
----
Invalid query parameters found.

# Enabling url compatibility mode will disable both Globs and query params
# allowing a user to query those hard-to-reach files
statement ok
SET s3_url_compatibility_mode=true;

statement ok
COPY test_4 TO '${prefix}test-bucket-public/url_encode/question?marks?and*stars[and]brackets.parquet' (FORMAT 'parquet');

query I
SELECT * FROM "${prefix}test-bucket-public/url_encode/question?marks?and*stars[and]brackets.parquet" LIMIT 1;
----
4

# HTTP urls will be encoded here
query I
SELECT * FROM "http://test-bucket-public.${DUCKDB_S3_ENDPOINT}/url_encode/question%3Fmarks%3Fand%2Astars%5Band%5Dbrackets.parquet" LIMIT 1;
----
4

statement ok
SET s3_url_compatibility_mode=false;

# Check that the generated urls are correct
statement ok
set s3_endpoint='s3.some.random.endpoint.com';

statement error
SELECT * FROM '${prefix}test-bucket/whatever.parquet';
----
IO Error: Connection error for HTTP HEAD to 'http://test-bucket.s3.some.random.endpoint.com/whatever.parquet'

statement ok
set s3_endpoint='${DUCKDB_S3_ENDPOINT}'

endloop

# Check that the generated urls are correct for an empty endpoint
statement ok
set s3_endpoint='';

statement error
SELECT * FROM 's3://test-bucket/whatever.parquet';
----
IO Error: HTTP GET error on 'http://test-bucket.s3.amazonaws.com/whatever.parquet'

statement error
SELECT * FROM 'r2://test-bucket/whatever.parquet';
----
IO Error: HTTP GET error on 'http://test-bucket.s3.amazonaws.com/whatever.parquet'

statement error
SELECT * FROM 'gcs://test-bucket/whatever.parquet';
----
IO Error: HTTP GET error on 'http://test-bucket.storage.googleapis.com/whatever.parquet'
