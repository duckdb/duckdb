# name: test/sql/copy/s3/metadata_cache.test
# description: Test metadata cache that caches reponses from the initial HEAD requests to open a file.
# group: [s3]

require parquet

require httpfs

require-env S3_TEST_SERVER_AVAILABLE 1

# override the default behaviour of skipping HTTP errors and connection failures: this test fails on connection issues
set ignore_error_messages

statement ok
CREATE TABLE test as SELECT * FROM range(0,10) tbl(i);

statement ok
CREATE TABLE test1 as SELECT * FROM range(10,20) tbl(i);

statement ok
SET s3_secret_access_key='minio_duckdb_user_password';SET s3_access_key_id='minio_duckdb_user';SET s3_region='eu-west-1'; SET s3_endpoint='duckdb-minio.com:9000'; SET s3_use_ssl=false;

query II
EXPLAIN ANALYZE COPY test TO 's3://test-bucket-public/root-dir/metadata_cache/test.parquet';
----
analyzed_plan	<REGEX>:.*HTTP Stats.*\#HEAD\: 1.*GET\: 0.*PUT\: 1.*\#POST\: 2.*

query II
EXPLAIN ANALYZE COPY test TO 's3://test-bucket-public/root-dir/metadata_cache/test1.parquet';
----
analyzed_plan	<REGEX>:.*HTTP Stats.*\#HEAD\: 1.*GET\: 0.*PUT\: 1.*\#POST\: 2.*

# Now we query the file metadata without the global metadata cache: There should be 1 HEAD request for the file size,
# then a GET for the pointer to the parquet metadata, then a GET for the metadata.
query II
EXPLAIN ANALYZE SELECT COUNT(*) FROM 's3://test-bucket-public/root-dir/metadata_cache/test.parquet';
----
analyzed_plan	<REGEX>:.*HTTP Stats.*\#HEAD\: 1.*GET\: 2.*PUT\: 0.*\#POST\: 0.*

# Redoing query should still result in same request count
query II
EXPLAIN ANALYZE SELECT COUNT(*) FROM 's3://test-bucket-public/root-dir/metadata_cache/test.parquet';
----
analyzed_plan	<REGEX>:.*HTTP Stats.*\#HEAD\: 1.*GET\: 2.*PUT\: 0.*\#POST\: 0.*

# Now enable the global metadata cache to store the results of the head requests, saving 1 HEAD per file
statement ok
SET enable_http_metadata_cache=true;

query II
EXPLAIN ANALYZE SELECT COUNT(*) FROM 's3://test-bucket-public/root-dir/metadata_cache/test1.parquet';
----
analyzed_plan	<REGEX>:.*HTTP Stats.*\#HEAD\: 1.*GET\: 2.*PUT\: 0.*\#POST\: 0.*

# Now with global metadata cache, we dont need to do the head request again. noice.
query II
EXPLAIN ANALYZE SELECT COUNT(*) FROM 's3://test-bucket-public/root-dir/metadata_cache/test1.parquet';
----
analyzed_plan	<REGEX>:.*HTTP Stats.*\#HEAD\: 0.*GET\: 2.*PUT\: 0.*\#POST\: 0.*

# Now when we write a file to a cached url, this would break so the cache entry should be invalidated
statement ok
COPY (SELECT * from range(0,100) tbl(i)) TO 's3://test-bucket-public/root-dir/metadata_cache/test1.parquet';

# We need to do a new head request here
query II
EXPLAIN ANALYZE SELECT COUNT(*) FROM 's3://test-bucket-public/root-dir/metadata_cache/test1.parquet';
----
analyzed_plan	<REGEX>:.*HTTP Stats.*\#HEAD\: 1.*GET\: 2.*PUT\: 0.*\#POST\: 0.*

# but now its cached again
query II
EXPLAIN ANALYZE SELECT COUNT(*) FROM 's3://test-bucket-public/root-dir/metadata_cache/test1.parquet';
----
analyzed_plan	<REGEX>:.*HTTP Stats.*\#HEAD\: 0.*GET\: 2.*PUT\: 0.*\#POST\: 0.*
