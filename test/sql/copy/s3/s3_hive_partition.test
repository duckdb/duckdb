# name: test/sql/copy/s3/s3_hive_partition.test
# description: Test the automatic parsing of the hive partitioning scheme
# group: [s3]

require parquet

require httpfs

require-env S3_TEST_SERVER_AVAILABLE 1

## Require that these environment variables are also set
require-env AWS_DEFAULT_REGION

require-env AWS_ACCESS_KEY_ID

require-env AWS_SECRET_ACCESS_KEY

require-env DUCKDB_S3_ENDPOINT

require-env DUCKDB_S3_USE_SSL

# override the default behaviour of skipping HTTP errors and connection failures: this test fails on connection issues
set ignore_error_messages

# Parquet filename name conflict
statement ok
CREATE TABLE test AS SELECT 1 as id, 'value1' as value;
CREATE TABLE test2 AS SELECT 2 as id, 'value2' as value;

statement ok
COPY test TO 's3://test-bucket/hive-partitioning/simple/key_!-_.*()=zisiswurking1/test.parquet';
COPY test2 TO 's3://test-bucket/hive-partitioning/simple/key_!-_.*()=zisiswurking2/test.parquet';

# test parsing hive partitioning scheme, with some common special characters
query III
select id, value, "key_!-_.*()" from parquet_scan('s3://test-bucket/hive-partitioning/simple/*/test.parquet', HIVE_PARTITIONING=1)
----
1	value1	zisiswurking1
2	value2	zisiswurking2

# Test some medium sized files
statement ok
CREATE TABLE test3 as SELECT id FROM range(0,10000) tbl(id);
CREATE TABLE test4 as SELECT id FROM range(10000,20000) tbl(id);

statement ok
COPY test3 TO 's3://test-bucket/hive-partitioning/medium/part=1/part2=1/test.parquet';
COPY test4 TO 's3://test-bucket/hive-partitioning/medium/part=1/part2=2/test.parquet';
COPY test3 TO 's3://test-bucket/hive-partitioning/medium/part=1/part2=1/test.csv' (HEADER 1);
COPY test4 TO 's3://test-bucket/hive-partitioning/medium/part=1/part2=2/test.csv' (HEADER 1);

query II
select min(id), max(id) from parquet_scan('s3://test-bucket/hive-partitioning/medium/*/*/test.parquet', HIVE_PARTITIONING=1) where part2=2
----
10000	19999

query II
select min(id), max(id) from parquet_scan('s3://test-bucket/hive-partitioning/medium/*/*/test.parquet', HIVE_PARTITIONING=1) where part2=1
----
0	9999

query II
select min(id), max(id) from read_csv_auto('s3://test-bucket/hive-partitioning/medium/*/*/test.csv', HIVE_PARTITIONING=1) where part2=2
----
10000	19999

query II
select min(id), max(id) from read_csv_auto('s3://test-bucket/hive-partitioning/medium/*/*/test.csv', HIVE_PARTITIONING=1) where part2=1
----
0	9999

# check cases where there are file filters AND table filters
statement ok
Create table t1 (a int, b int, c int);

foreach i 0 1 2 3 4 5 6 7 8 9

statement ok
insert into t1 (select range, ${i}*10, ${i}*100 from range(0,10));

endloop

statement ok
COPY (SELECT * FROM t1) TO 's3://test-bucket/hive-partitioning/filter-test-parquet' (FORMAT PARQUET, PARTITION_BY c, OVERWRITE_OR_IGNORE);

statement ok
COPY (SELECT * FROM t1) TO 's3://test-bucket/hive-partitioning/filter-test-csv' (FORMAT CSV, PARTITION_BY c, OVERWRITE_OR_IGNORE);

# There should be Table Filters (id < 50) and file filters (c = 500)
query II
EXPLAIN select a from parquet_scan('s3://test-bucket/hive-partitioning/filter-test-parquet/*/*.parquet', HIVE_PARTITIONING=1, HIVE_TYPES_AUTOCAST=0) where c=500 and a < 4;
----
physical_plan	<REGEX>:.*PARQUET_SCAN.*Filters:.*a<4 AND a IS NOT.*NULL.*File Filters: \(CAST\(c AS.*INTEGER\) = 500\).*

# There should be Table Filters (id < 50) and file filters (c = 500)
query II
EXPLAIN select a from read_csv_auto('s3://test-bucket/hive-partitioning/filter-test-csv/*/*.csv', HIVE_PARTITIONING=1, HIVE_TYPES_AUTOCAST=0, columns={'a': 'INT', 'b':'INT', 'c':'INT'}) where c=500 and a < 4;
----
physical_plan	<REGEX>:.*FILTER.*(a < 4).*READ_CSV_AUTO.*File Filters: \(CAST\(c AS.*INTEGER\) = 500\).*

