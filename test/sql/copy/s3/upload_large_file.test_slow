# name: test/sql/copy/s3/upload_large_file.test_slow
# description: Copy large csv/parquet files from and to S3.
# group: [s3]

require tpch

require parquet

require httpfs

require-env S3_TEST_SERVER_AVAILABLE 1

# override the default behaviour of skipping HTTP errors and connection failures: this test fails on connection issues
set ignore_error_messages

# confirm we use a reasonable amount of memory
statement ok
SET memory_limit='2.2GB';

# disable tmp dir to force OOM if we exceed our set limit
statement ok
PRAGMA temp_directory=''

statement ok
SET s3_secret_access_key='minio_duckdb_user_password';SET s3_access_key_id='minio_duckdb_user';SET s3_region='eu-west-1'; SET s3_endpoint='duckdb-minio.com:9000';SET s3_use_ssl=false;

statement ok
SET s3_uploader_thread_limit = 5;

statement ok
CALL DBGEN(sf=1)

query I
SELECT
    sum(l_extendedprice * l_discount) AS revenue
FROM
    lineitem
WHERE
    l_shipdate >= CAST('1994-01-01' AS date)
    AND l_shipdate < CAST('1995-01-01' AS date)
    AND l_discount BETWEEN 0.05
    AND 0.07
    AND l_quantity < 24;
----
123141078.2283

# Parquet file ~300MB
statement ok
COPY lineitem TO 's3://test-bucket/multipart/export_large.parquet' (FORMAT 'parquet');

query I
SELECT
    sum(l_extendedprice * l_discount) AS revenue
FROM
    "s3://test-bucket/multipart/export_large.parquet"
WHERE
    l_shipdate >= CAST('1994-01-01' AS date)
    AND l_shipdate < CAST('1995-01-01' AS date)
    AND l_discount BETWEEN 0.05
    AND 0.07
    AND l_quantity < 24;
----
123141078.2283