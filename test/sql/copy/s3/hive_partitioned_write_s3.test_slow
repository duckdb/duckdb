# name: test/sql/copy/s3/hive_partitioned_write_s3.test_slow
# description: slow test for the hive partitioned write to s3
# group: [s3]

require parquet

require httpfs

require tpch

require-env S3_TEST_SERVER_AVAILABLE 1

require-env AWS_DEFAULT_REGION

require-env AWS_ACCESS_KEY_ID

require-env AWS_SECRET_ACCESS_KEY

require-env DUCKDB_S3_ENDPOINT

require-env DUCKDB_S3_USE_SSL

# override the default behaviour of skipping HTTP errors and connection failures: this test fails on connection issues
set ignore_error_messages

statement ok
pragma memory_limit='100mb'

# around 200MB worth of data, will require the PartitionedColumnData to spill to disk
statement ok
COPY (SELECT i%2::INT32 as part_col, i::INT32 FROM range(0,25000000) tbl(i)) TO 's3://test-bucket/partitioned_memory_spill' (FORMAT parquet, PARTITION_BY part_col, overwrite_or_ignore TRUE);

statement ok
pragma memory_limit='-1'

statement ok
call dbgen(sf=1);

# Partition by 2 columns
statement ok
COPY lineitem TO 's3://test-bucket/lineitem_sf1_partitioned' (FORMAT PARQUET, PARTITION_BY (l_returnflag, l_linestatus), overwrite_or_ignore TRUE);

statement ok
DROP TABLE lineitem;

statement ok
CREATE VIEW lineitem as SELECT * FROM parquet_scan('s3://test-bucket/lineitem_sf1_partitioned/*/*/*.parquet', HIVE_PARTITIONING=1);

loop i 1 9

query I
PRAGMA tpch(${i})
----
<FILE>:extension/tpch/dbgen/answers/sf1/q0${i}.csv

endloop

loop i 10 23

query I
PRAGMA tpch(${i})
----
<FILE>:extension/tpch/dbgen/answers/sf1/q${i}.csv

endloop