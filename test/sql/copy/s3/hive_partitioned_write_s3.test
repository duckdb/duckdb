# name: test/sql/copy/s3/hive_partitioned_write_s3.test
# description: basic tests for the hive partitioned write to s3
# group: [s3]

require parquet

require httpfs

require-env S3_TEST_SERVER_AVAILABLE 1

require-env AWS_DEFAULT_REGION

require-env AWS_ACCESS_KEY_ID

require-env AWS_SECRET_ACCESS_KEY

require-env DUCKDB_S3_ENDPOINT

require-env DUCKDB_S3_USE_SSL

# override the default behaviour of skipping HTTP errors and connection failures: this test fails on connection issues
set ignore_error_messages

# Simple table that is easy to partition
statement ok
CREATE TABLE test as SELECT i%2 as part_col, (i+1)%5 as value_col, i as value2_col from range(0,10) tbl(i);

statement ok
COPY test TO 's3://test-bucket/partitioned1' (FORMAT PARQUET, PARTITION_BY (part_col), OVERWRITE_OR_IGNORE TRUE);

query III
SELECT part_col, value_col, value2_col FROM 's3://test-bucket/partitioned1/part_col=0/*.parquet' ORDER BY value2_col;
----
0	1	0
0	3	2
0	0	4
0	2	6
0	4	8

query III
SELECT part_col, value_col, value2_col FROM 's3://test-bucket/partitioned1/part_col=1/*.parquet' ORDER BY value2_col;
----
1	2	1
1	4	3
1	1	5
1	3	7
1	0	9

# Want a modified version of the partition_col? (for example to do custom string conversion?) No problem:
statement ok
COPY (SELECT * EXCLUDE (part_col), 'prefix-'::VARCHAR || part_col::VARCHAR as part_col FROM test) TO 's3://test-bucket/partitioned2' (FORMAT PARQUET, PARTITION_BY (part_col), OVERWRITE_OR_IGNORE TRUE);

query III
SELECT part_col, value_col, value2_col FROM 's3://test-bucket/partitioned2/part_col=prefix-0/*.parquet' ORDER BY value2_col;
----
prefix-0	1	0
prefix-0	3	2
prefix-0	0	4
prefix-0	2	6
prefix-0	4	8

query III
SELECT part_col, value_col, value2_col FROM 's3://test-bucket/partitioned2/part_col=prefix-1/*.parquet' ORDER BY value2_col;
----
prefix-1	2	1
prefix-1	4	3
prefix-1	1	5
prefix-1	3	7
prefix-1	0	9

# Test partitioning by all
statement ok
COPY test TO 's3://test-bucket/partitioned3' (FORMAT PARQUET, PARTITION_BY '*', OVERWRITE_OR_IGNORE TRUE);

query I
SELECT min(value2_col) as min_val
FROM parquet_scan('s3://test-bucket/partitioned3/part_col=*/value_col=*/value2_col=*/*.parquet', FILENAME=1)
GROUP BY filename
ORDER BY min_val
----
0
1
2
3
4
5
6
7
8
9

# single col as param is also fine
statement ok
COPY test TO 's3://test-bucket/partitioned4' (FORMAT PARQUET, PARTITION_BY part_col, OVERWRITE_OR_IGNORE TRUE);

query III
SELECT part_col, value_col, value2_col FROM parquet_scan('s3://test-bucket/partitioned4/*/*.parquet', HIVE_PARTITIONING=1) WHERE part_col=0 ORDER BY value2_col;
----
0	1	0
0	3	2
0	0	4
0	2	6
0	4	8


# a file already exist and, OVERWRITE_OR_IGNORE is not set, throw error
statement error
COPY test TO 's3://test-bucket/partitioned4' (FORMAT PARQUET, PARTITION_BY part_col);
----
IO Error: Directory

# a file already exist and, OVERWRITE_OR_IGNORE is set to false, throw error
statement error
COPY test TO 's3://test-bucket/partitioned4' (FORMAT PARQUET, PARTITION_BY part_col, OVERWRITE_OR_IGNORE FALSE);
----
IO Error: Directory

# Trailing slash ist auch gut!
statement ok
COPY test TO 's3://test-bucket/partitioned5/' (FORMAT PARQUET, PARTITION_BY part_col, OVERWRITE_OR_IGNORE TRUE);

query III
SELECT part_col, value_col, value2_col FROM 's3://test-bucket/partitioned5/part_col=0/*.parquet' ORDER BY value2_col;
----
0	1	0
0	3	2
0	0	4
0	2	6
0	4	8

# Cannot use the USE_TMP_FILE option simulatiously with partitioning
statement error
COPY test TO 's3://test-bucket/partitioned6' (FORMAT PARQUET, PARTITION_BY part_col, USE_TMP_FILE TRUE);
----
Not implemented Error: Can't combine USE_TMP_FILE and PARTITION_BY for COPY

# Technically it doesn't really matter, as currently out parition_by behaves similarly, but for clarity user should just
# EITHER use partition_by or per_thread_output.
statement error
COPY test TO 's3://test-bucket/partitioned6' (FORMAT PARQUET, PARTITION_BY part_col, PER_THREAD_OUTPUT TRUE);
----
Not implemented Error: Can't combine PER_THREAD_OUTPUT and PARTITION_BY for COPY

# partitioning csv files is also a thing
statement ok
COPY test TO 's3://test-bucket/partitioned7' (FORMAT CSV, HEADER TRUE, PARTITION_BY part_col, OVERWRITE_OR_IGNORE TRUE);

query III
SELECT part_col, value_col, value2_col FROM 's3://test-bucket/partitioned7/part_col=0/*.csv' ORDER BY value2_col;
----
0	1	0
0	3	2
0	0	4
0	2	6
0	4	8

query III
SELECT part_col, value_col, value2_col FROM 's3://test-bucket/partitioned7/part_col=1/*.csv' ORDER BY value2_col;
----
1	2	1
1	4	3
1	1	5
1	3	7
1	0	9
