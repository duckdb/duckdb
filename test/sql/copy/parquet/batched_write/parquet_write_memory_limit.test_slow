# name: test/sql/copy/parquet/batched_write/parquet_write_memory_limit.test_slow
# description: Verify data is streamed and memory limit is not exceeded in Parquet write
# group: [batched_write]

require parquet

require 64bit

load __TEST_DIR__/parquet_write_memory_limit.db

# 100M rows, 2 BIGINT columns = 1.6GB uncompressed
statement ok
COPY (SELECT i, i // 5 AS j FROM range(100000000) t(i)) TO '__TEST_DIR__/large_integers.parquet'

# set threads to 4 and a memory limit of 1GB. the limit in this test used to be 300MB,
# but we weren't using the BufferAllocator for the ColumnDataCollection buffers in ParquetWriteLocalState
statement ok
SET threads=4

statement ok
SET memory_limit='1.1GB'

# stream from one parquet file to another
query I
COPY '__TEST_DIR__/large_integers.parquet' TO '__TEST_DIR__/large_integers2.parquet'
----
100000000

# verify that the file is correctly written
statement ok
SET memory_limit='-1'

query II
SELECT * FROM '__TEST_DIR__/large_integers.parquet' EXCEPT FROM '__TEST_DIR__/large_integers2.parquet'
----
