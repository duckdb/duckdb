# name: test/parquet/constant_dictionary_vector_parquet.test_slow
# description: Test that we retain constant/dictionary compression for strings when writing to Parquet (big data)
# group: [parquet]

require vector_size 2048

require parquet

# low memory limit to test that we don't blow up intermediates
statement ok
set memory_limit='100mb'

# we should be able to do this without spilling
statement ok
set temp_directory=null

# 100k strings of ~50kb = ~5 GB
# the ColumnDataCollection should keep the constant string compressed
# and the Parquet writer will use dictionary compression, not blowing them up there either
statement ok
copy (select repeat('a', 50_000) s from range(100_000)) to '{TEMP_DIR}/cdc_constant.parquet'

# the written file has dictionary compression
# when we copy it over to another file we should still be able to avoid blowing it up
statement ok
copy (from '{TEMP_DIR}/cdc_constant.parquet') to '{TEMP_DIR}/cdc_dictionary.parquet'
