diff --git a/.github/config/extensions/aws.cmake b/.github/config/extensions/aws.cmake
index ee0547d808..de112cafd7 100644
--- a/.github/config/extensions/aws.cmake
+++ b/.github/config/extensions/aws.cmake
@@ -2,6 +2,6 @@ if (NOT MINGW AND NOT ${WASM_ENABLED})
     duckdb_extension_load(aws
             ### TODO: re-enable LOAD_TESTS
             GIT_URL https://github.com/duckdb/duckdb-aws
-            GIT_TAG 18803d5e55b9f9f6dda5047d0fdb4f4238b6801d
+            GIT_TAG 55bf3621fb7db254b473c94ce6360643ca38fac0
             )
 endif()
diff --git a/.github/config/extensions/delta.cmake b/.github/config/extensions/delta.cmake
index be12653130..f6bf91a4c3 100644
--- a/.github/config/extensions/delta.cmake
+++ b/.github/config/extensions/delta.cmake
@@ -1,7 +1,7 @@
 if (NOT MINGW AND NOT ${WASM_ENABLED})
     duckdb_extension_load(delta
             GIT_URL https://github.com/duckdb/duckdb-delta
-            GIT_TAG 0747c23791c6ad53dfc22f58dc73008d49d2a8ae
+            GIT_TAG 6515bb2560772956f9b74a18a47782f18ed4d827
             SUBMODULES extension-ci-tools
             APPLY_PATCHES
     )
diff --git a/.github/config/extensions/httpfs.cmake b/.github/config/extensions/httpfs.cmake
index 169e374ef6..9ffdd5d45d 100644
--- a/.github/config/extensions/httpfs.cmake
+++ b/.github/config/extensions/httpfs.cmake
@@ -1,13 +1,7 @@
-IF (NOT WIN32)
-    set(LOAD_HTTPFS_TESTS "LOAD_TESTS")
-else ()
-    set(LOAD_HTTPFS_TESTS "")
-endif()
 duckdb_extension_load(httpfs
-    # TODO: restore once httpfs is fixed
-    ${LOAD_HTTPFS_TESTS}
+    LOAD_TESTS
     GIT_URL https://github.com/duckdb/duckdb-httpfs
-    GIT_TAG 8356a9017444f54018159718c8017ff7db4ea756
+    GIT_TAG 6c187d86edde066adb2c51a411f3b6020da79e12
     APPLY_PATCHES
     INCLUDE_DIR src/include
 )
diff --git a/.github/config/extensions/postgres_scanner.cmake b/.github/config/extensions/postgres_scanner.cmake
index d99e014da8..58d70ec554 100644
--- a/.github/config/extensions/postgres_scanner.cmake
+++ b/.github/config/extensions/postgres_scanner.cmake
@@ -4,6 +4,6 @@ if (NOT MINGW AND NOT ${WASM_ENABLED})
     duckdb_extension_load(postgres_scanner
             DONT_LINK
             GIT_URL https://github.com/duckdb/duckdb-postgres
-            GIT_TAG f012a4f99cea1d276d1787d0dc84b1f1a0e0f0b2
+            GIT_TAG b63ef4b1eb007320840b6d1760f3c9b139bb3b49
             )
 endif()
diff --git a/.github/patches/extensions/spatial/arrow.patch b/.github/patches/extensions/spatial/arrow.patch
new file mode 100644
index 0000000000..3afd1adf93
--- /dev/null
+++ b/.github/patches/extensions/spatial/arrow.patch
@@ -0,0 +1,19 @@
+diff --git a/src/spatial/modules/gdal/gdal_module.cpp b/src/spatial/modules/gdal/gdal_module.cpp
+index f24f049..1ab9e70 100644
+--- a/src/spatial/modules/gdal/gdal_module.cpp
++++ b/src/spatial/modules/gdal/gdal_module.cpp
+@@ -963,12 +963,12 @@ struct ST_Read : ArrowTableFunction {
+ 			state.all_columns.Reset();
+ 			state.all_columns.SetCardinality(output_size);
+ 			ArrowTableFunction::ArrowToDuckDB(state, data.arrow_table.GetColumns(), state.all_columns,
+-			                                  gstate.lines_read - output_size, false);
++			                                   false);
+ 			output.ReferenceColumns(state.all_columns, gstate.projection_ids);
+ 		} else {
+ 			output.SetCardinality(output_size);
+ 			ArrowTableFunction::ArrowToDuckDB(state, data.arrow_table.GetColumns(), output,
+-			                                  gstate.lines_read - output_size, false);
++			                                   false);
+ 		}
+ 
+ 		if (!data.keep_wkb) {
diff --git a/.github/workflows/Main.yml b/.github/workflows/Main.yml
index 70139773ad..806bde2aa3 100644
--- a/.github/workflows/Main.yml
+++ b/.github/workflows/Main.yml
@@ -373,9 +373,9 @@ jobs:
       with:
         python-version: '3.12'
 
-    - name: Install Ninja
+    - name: Install
       shell: bash
-      run: sudo apt-get update -y -qq && sudo apt-get install -y -qq ninja-build
+      run: sudo apt-get update -y -qq && sudo apt-get install -y -qq ninja-build libcurl4-openssl-dev
 
     - name: Setup Ccache
       uses: hendrikmuhs/ccache-action@main
@@ -391,12 +391,6 @@ jobs:
         GEN: ninja
       run: make
 
-    - name: test/configs/encryption.json
-      if: (success() || failure()) && steps.build.conclusion == 'success'
-      shell: bash
-      run: |
-        ./build/release/test/unittest --test-config test/configs/encryption.json
-
     - name: test/configs/force_storage.json
       if: (success() || failure()) && steps.build.conclusion == 'success'
       shell: bash
@@ -463,6 +457,12 @@ jobs:
       run: |
         ./build/release/test/unittest --test-config test/configs/enable_verification.json
 
+    - name: test/configs/block_allocator_100mib.json
+      if: (success() || failure()) && steps.build.conclusion == 'success'
+      shell: bash
+      run: |
+        ./build/release/test/unittest --test-config test/configs/block_allocator_100mib.json
+
     - name: Test dictionary_expression
       if: (success() || failure()) && steps.build.conclusion == 'success'
       shell: bash
@@ -534,3 +534,21 @@ jobs:
       shell: bash
       run: |
         python3 scripts/test_storage_compatibility.py --versions "1.2.1|1.3.2" --new-unittest build/release/test/unittest
+
+    # TODO: clean this up: we should probably be able to run this whole test suite with httpfs
+    # We want to run the remainder of tests with httpfs
+    - name: Build with httpfs extension
+      id: build-httpfs
+      shell: bash
+      if: (success() || failure()) && steps.build.conclusion == 'success'
+      env:
+        CORE_EXTENSIONS: "json;parquet;icu;tpch;tpcds;httpfs"
+        GEN: ninja
+      run:
+        make
+
+    - name: test/configs/encryption.json
+      if: (success() || failure()) && steps.build.conclusion == 'success'
+      shell: bash
+      run: |
+        ./build/release/test/unittest --test-config test/configs/encryption.json
diff --git a/Makefile b/Makefile
index 650483b249..7d0fff6408 100644
--- a/Makefile
+++ b/Makefile
@@ -312,7 +312,7 @@ ifeq (${EXPORT_DYNAMIC_SYMBOLS}, 1)
 	CMAKE_VARS:=${CMAKE_VARS} -DEXPORT_DYNAMIC_SYMBOLS=1
 endif
 ifneq ("${CMAKE_LLVM_PATH}", "")
-	CMAKE_VARS:=${CMAKE_VARS} -DCMAKE_RANLIB='${CMAKE_LLVM_PATH}/bin/llvm-ranlib' -DCMAKE_AR='${CMAKE_LLVM_PATH}/bin/llvm-ar' -DCMAKE_CXX_COMPILER='${CMAKE_LLVM_PATH}/bin/clang++' -DCMAKE_C_COMPILER='${CMAKE_LLVM_PATH}/bin/clang'
+	CMAKE_VARS:=${CMAKE_VARS} -DCMAKE_RANLIB='${CMAKE_LLVM_PATH}/bin/llvm-ranlib' -DCMAKE_AR='${CMAKE_LLVM_PATH}/bin/llvm-ar' -DCMAKE_CXX_COMPILER='${CMAKE_LLVM_PATH}/bin/clang++' -DCMAKE_C_COMPILER='${CMAKE_LLVM_PATH}/bin/clang' -DCMAKE_EXE_LINKER_FLAGS_INIT='-L${CMAKE_LLVM_PATH}/lib -L${CMAKE_LLVM_PATH}/lib/c++' -DCMAKE_SHARED_LINKER_FLAGS_INIT='-L${CMAKE_LLVM_PATH}/lib -L${CMAKE_LLVM_PATH}/lib/c++'  -DCMAKE_MODULE_LINKER_FLAGS_INIT='-L${CMAKE_LLVM_PATH}/lib -L${CMAKE_LLVM_PATH}/lib/c++'
 endif
 
 CMAKE_VARS:=${CMAKE_VARS} ${COMMON_CMAKE_VARS}
diff --git a/data/attach_test/attach.db b/data/attach_test/attach.db
new file mode 100644
index 0000000000..b0f1e91746
Binary files /dev/null and b/data/attach_test/attach.db differ
diff --git a/data/attach_test/encrypted_ctr_key=abcde.db b/data/attach_test/encrypted_ctr_key=abcde.db
new file mode 100644
index 0000000000..e3853a47fd
Binary files /dev/null and b/data/attach_test/encrypted_ctr_key=abcde.db differ
diff --git a/data/attach_test/encrypted_gcm_key=abcde.db b/data/attach_test/encrypted_gcm_key=abcde.db
new file mode 100644
index 0000000000..77dd1f4353
Binary files /dev/null and b/data/attach_test/encrypted_gcm_key=abcde.db differ
diff --git a/extension/core_functions/scalar/debug/sleep.cpp b/extension/core_functions/scalar/debug/sleep.cpp
index 2be6231abf..795ede1cfa 100644
--- a/extension/core_functions/scalar/debug/sleep.cpp
+++ b/extension/core_functions/scalar/debug/sleep.cpp
@@ -2,8 +2,11 @@
 
 #include "duckdb/common/vector_operations/generic_executor.hpp"
 #include "duckdb/planner/expression/bound_function_expression.hpp"
-#include <thread>
-#include <chrono>
+
+#include "duckdb/common/chrono.hpp"
+#ifndef DUCKDB_NO_THREADS
+#include "duckdb/common/thread.hpp"
+#endif
 
 namespace duckdb {
 
@@ -19,6 +22,7 @@ static void SleepFunction(DataChunk &input, ExpressionState &state, Vector &resu
 	input.Flatten();
 	GenericExecutor::ExecuteUnary<PrimitiveType<int64_t>, NullResultType>(
 	    input.data[0], result, input.size(), [](PrimitiveType<int64_t> input) {
+#ifndef DUCKDB_NO_THREADS
 		    // Sleep for the specified number of milliseconds (clamp negative values to 0)
 		    int64_t sleep_ms = input.val;
 		    if (sleep_ms < 0) {
@@ -26,6 +30,9 @@ static void SleepFunction(DataChunk &input, ExpressionState &state, Vector &resu
 		    }
 		    std::this_thread::sleep_for(std::chrono::milliseconds(sleep_ms));
 		    return NullResultType();
+#else
+	    	throw InvalidInputException("Function sleep() only available if DuckDB is compiled with threads");
+#endif
 	    });
 }
 
diff --git a/extension/parquet/parquet_writer.cpp b/extension/parquet/parquet_writer.cpp
index e6b65631f9..a58cd696e5 100644
--- a/extension/parquet/parquet_writer.cpp
+++ b/extension/parquet/parquet_writer.cpp
@@ -14,6 +14,7 @@
 #include "duckdb/common/serializer/write_stream.hpp"
 #include "duckdb/common/string_util.hpp"
 #include "duckdb/function/table_function.hpp"
+#include "duckdb/main/extension_helper.hpp"
 #include "duckdb/main/client_context.hpp"
 #include "duckdb/main/connection.hpp"
 #include "duckdb/parser/parsed_data/create_copy_function_info.hpp"
@@ -354,6 +355,12 @@ ParquetWriter::ParquetWriter(ClientContext &context, FileSystem &fs, string file
 
 	if (encryption_config) {
 		auto &config = DBConfig::GetConfig(context);
+
+		// To ensure we can write, we need to autoload httpfs
+		if (!config.encryption_util || !config.encryption_util->SupportsEncryption()) {
+			ExtensionHelper::TryAutoLoadExtension(context, "httpfs");
+		}
+
 		if (config.encryption_util && debug_use_openssl) {
 			// Use OpenSSL
 			encryption_util = config.encryption_util;
diff --git a/extension/parquet/zstd_file_system.cpp b/extension/parquet/zstd_file_system.cpp
index 2556dd2f2b..3bddf86613 100644
--- a/extension/parquet/zstd_file_system.cpp
+++ b/extension/parquet/zstd_file_system.cpp
@@ -28,15 +28,7 @@ ZstdStreamWrapper::~ZstdStreamWrapper() {
 	}
 	try {
 		Close();
-	} catch (std::exception &ex) {
-		if (file && file->child_handle) {
-			// FIXME: make more log context available here.
-			ErrorData data(ex);
-			DUCKDB_LOG_ERROR(file->child_handle->logger,
-			                 "ZstdStreamWrapper::~ZstdStreamWrapper()\t\t" + data.Message());
-		}
-	} catch (...) {
-		// NOLINT
+	} catch (...) { // NOLINT: swallow exceptions in destructor
 	}
 }
 
diff --git a/scripts/generate_c_api.py b/scripts/generate_c_api.py
index 7ab49cfb9a..a47116edd1 100644
--- a/scripts/generate_c_api.py
+++ b/scripts/generate_c_api.py
@@ -83,7 +83,6 @@ ORIGINAL_FUNCTION_GROUP_ORDER = [
     'config_options_interface',
     'copy_functions',
     'catalog_interface',
-    'logging',
 ]
 
 # The file that forms the base for the header generation
diff --git a/src/catalog/default/default_table_functions.cpp b/src/catalog/default/default_table_functions.cpp
index c077864740..94079bbcb0 100644
--- a/src/catalog/default/default_table_functions.cpp
+++ b/src/catalog/default/default_table_functions.cpp
@@ -69,7 +69,7 @@ FROM histogram_values(source, col_name, bin_count := bin_count, technique := tec
 	{DEFAULT_SCHEMA, "duckdb_logs_parsed", {"log_type"}, {}, R"(
 SELECT * EXCLUDE (message), UNNEST(parse_duckdb_log_message(log_type, message))
 FROM duckdb_logs(denormalized_table=1)
-WHERE type = log_type
+WHERE type ILIKE log_type
 )"},
 	{nullptr, nullptr, {nullptr}, {{nullptr, nullptr}}, nullptr}
 	};
diff --git a/src/common/adbc/adbc.cpp b/src/common/adbc/adbc.cpp
index b2028b660b..c62b5e9f22 100644
--- a/src/common/adbc/adbc.cpp
+++ b/src/common/adbc/adbc.cpp
@@ -550,7 +550,7 @@ static int get_schema(struct ArrowArrayStream *stream, struct ArrowSchema *out)
 
 	auto arrow_options = duckdb_result_get_arrow_options(&result_wrapper->result);
 
-	auto res = duckdb_to_arrow_schema(arrow_options, &types[0], names.data(), count, out);
+	auto res = duckdb_to_arrow_schema(arrow_options, types.data(), names.data(), count, out);
 	duckdb_destroy_arrow_options(&arrow_options);
 	for (auto &type : types) {
 		duckdb_destroy_logical_type(&type);
@@ -789,9 +789,6 @@ AdbcStatusCode StatementGetParameterSchema(struct AdbcStatement *statement, stru
 		return ADBC_STATUS_INVALID_ARGUMENT;
 	}
 	auto count = prepared_wrapper->statement->data->properties.parameter_count;
-	if (count == 0) {
-		count = 1;
-	}
 	std::vector<duckdb_logical_type> types(count);
 	std::vector<std::string> owned_names;
 	owned_names.reserve(count);
@@ -810,7 +807,7 @@ AdbcStatusCode StatementGetParameterSchema(struct AdbcStatement *statement, stru
 	duckdb_arrow_options arrow_options;
 	duckdb_connection_get_arrow_options(wrapper->connection, &arrow_options);
 
-	auto res = duckdb_to_arrow_schema(arrow_options, &types[0], names.data(), count, schema);
+	auto res = duckdb_to_arrow_schema(arrow_options, types.data(), names.data(), count, schema);
 
 	for (auto &type : types) {
 		duckdb_destroy_logical_type(&type);
@@ -862,7 +859,10 @@ AdbcStatusCode StatementExecuteQuery(struct AdbcStatement *statement, struct Arr
 		return IngestToTableFromBoundStream(wrapper, error);
 	}
 	auto stream_wrapper = static_cast<DuckDBAdbcStreamWrapper *>(malloc(sizeof(DuckDBAdbcStreamWrapper)));
-	if (has_stream) {
+	// Only process the stream if there are parameters to bind
+	auto prepared_statement_params = reinterpret_cast<duckdb::PreparedStatementWrapper *>(wrapper->statement)
+	                                     ->statement->data->properties.parameter_count;
+	if (has_stream && prepared_statement_params > 0) {
 		// A stream was bound to the statement, use that to bind parameters
 		ArrowArrayStream stream = wrapper->ingestion_stream;
 		ConvertedSchemaWrapper out_types;
@@ -879,8 +879,6 @@ AdbcStatusCode StatementExecuteQuery(struct AdbcStatement *statement, struct Arr
 			free(stream_wrapper);
 			return ADBC_STATUS_INTERNAL;
 		}
-		auto prepared_statement_params =
-		    reinterpret_cast<duckdb::PreparedStatementWrapper *>(wrapper->statement)->statement->named_param_map.size();
 
 		duckdb::ArrowArrayWrapper arrow_array_wrapper;
 
diff --git a/src/common/box_renderer.cpp b/src/common/box_renderer.cpp
index 3d04236b1c..f21c8d8e31 100644
--- a/src/common/box_renderer.cpp
+++ b/src/common/box_renderer.cpp
@@ -310,8 +310,7 @@ void BoxRendererImplementation::Render() {
 	RenderFooter(row_count, column_count);
 }
 
-string BoxRendererImplementation::TruncateValue(const string &value, idx_t column_width, idx_t &pos,
-                                                idx_t &current_render_width) {
+string BoxRenderer::TruncateValue(const string &value, idx_t column_width, idx_t &pos, idx_t &current_render_width) {
 	idx_t start_pos = pos;
 	while (pos < value.size()) {
 		if (value[pos] == '\n') {
@@ -322,7 +321,7 @@ string BoxRendererImplementation::TruncateValue(const string &value, idx_t colum
 		}
 		// check if this character fits...
 		auto char_size = Utf8Proc::RenderWidth(value.c_str(), value.size(), pos);
-		if (current_render_width + char_size >= column_width) {
+		if (current_render_width + char_size > column_width) {
 			// it doesn't! stop
 			break;
 		}
@@ -333,6 +332,11 @@ string BoxRendererImplementation::TruncateValue(const string &value, idx_t colum
 	return value.substr(start_pos, pos - start_pos);
 }
 
+string BoxRendererImplementation::TruncateValue(const string &value, idx_t column_width, idx_t &pos,
+                                                idx_t &current_render_width) {
+	return BoxRenderer::TruncateValue(value, column_width, pos, current_render_width);
+}
+
 void BoxRendererImplementation::RenderValue(const string &value, idx_t column_width, ResultRenderType render_mode,
                                             const vector<HighlightingAnnotation> &annotations,
                                             ValueRenderAlignment alignment, optional_idx render_width_input) {
@@ -459,7 +463,7 @@ ValueRenderAlignment BoxRendererImplementation::TypeAlignment(const LogicalType
 	}
 }
 
-string BoxRendererImplementation::TryFormatLargeNumber(const string &numeric) {
+string BoxRenderer::TryFormatLargeNumber(const string &numeric, char decimal_sep) {
 	// we only return a readable rendering if the number is > 1 million
 	if (numeric.size() <= 5) {
 		// number too small for sure
@@ -520,13 +524,17 @@ string BoxRendererImplementation::TryFormatLargeNumber(const string &numeric) {
 		result += "-";
 	}
 	result += decimal_str.substr(0, decimal_str.size() - 2);
-	result += config.decimal_separator == '\0' ? '.' : config.decimal_separator;
+	result += decimal_sep == '\0' ? '.' : decimal_sep;
 	result += decimal_str.substr(decimal_str.size() - 2, 2);
 	result += " ";
 	result += unit;
 	return result;
 }
 
+string BoxRendererImplementation::TryFormatLargeNumber(const string &numeric) {
+	return BoxRenderer::TryFormatLargeNumber(numeric, config.decimal_separator);
+}
+
 list<ColumnDataCollection> BoxRendererImplementation::FetchRenderCollections(const ColumnDataCollection &result,
                                                                              idx_t top_rows, idx_t bottom_rows) {
 	auto column_count = result.ColumnCount();
@@ -1094,58 +1102,77 @@ protected:
 		JSONFormattingResult format_result = JSONFormattingResult::SUCCESS;
 	};
 
-	bool LiteralFits(FormatState &format_state, const string &text) {
+	bool LiteralFits(FormatState &format_state, idx_t render_width) {
 		auto &line_length = format_state.line_length;
-		idx_t render_width = Utf8Proc::RenderWidth(text);
 		if (line_length + render_width > format_state.max_width) {
 			return false;
 		}
 		return true;
 	}
 
-	void AddLiteral(FormatState &format_state, const string &text) {
+	bool LiteralFits(FormatState &format_state, const string &text) {
+		idx_t render_width = Utf8Proc::RenderWidth(text);
+		return LiteralFits(format_state, render_width);
+	}
+
+	void AddLiteral(FormatState &format_state, const string &text, bool skip_adding_if_does_not_fit = false) {
 		auto &result = format_state.result;
 		auto &line_length = format_state.line_length;
+		idx_t render_width = Utf8Proc::RenderWidth(text);
+		if (!LiteralFits(format_state, render_width)) {
+			if (skip_adding_if_does_not_fit) {
+				return;
+			}
+			AddNewline(format_state);
+			if (format_state.format_result != JSONFormattingResult::SUCCESS) {
+				return;
+			}
+		}
 		result += text;
-		line_length += Utf8Proc::RenderWidth(text);
+		line_length += render_width;
 		if (line_length > format_state.max_width) {
 			format_state.format_result = JSONFormattingResult::TOO_WIDE;
 		}
 	}
+	void AddSpace(FormatState &format_state) {
+		AddLiteral(format_state, " ", true);
+	}
 	void AddNewline(FormatState &format_state) {
 		auto &result = format_state.result;
 		auto &depth = format_state.depth;
 		auto &row_count = format_state.row_count;
 		auto &line_length = format_state.line_length;
 		result += '\n';
-		result += string(format_state.indentation_size * depth, ' ');
+		result += string(depth, ' ');
 		row_count++;
 		if (row_count > format_state.max_rows) {
 			format_state.format_result = JSONFormattingResult::TOO_MANY_ROWS;
 			return;
 		}
-		line_length = format_state.indentation_size * depth;
+		line_length = depth;
 		if (line_length > format_state.max_width) {
 			format_state.format_result = JSONFormattingResult::TOO_WIDE;
 		}
 	}
 
-	void FormatComponent(FormatState &format_state, JSONComponent &component, bool inlined) {
+	enum class InlineMode { STANDARD, INLINED_SINGLE_LINE, INLINED_MULTI_LINE };
+
+	void FormatComponent(FormatState &format_state, JSONComponent &component, InlineMode inline_mode) {
 		auto &depth = format_state.depth;
 		auto &line_length = format_state.line_length;
 		auto &max_width = format_state.max_width;
 		auto &c = format_state.component_idx;
 		switch (component.type) {
 		case JSONComponentType::BRACKET_OPEN: {
-			depth++;
+			depth += component.text == "{" ? format_state.indentation_size : 1;
 			AddLiteral(format_state, component.text);
-			if (!inlined) {
+			if (inline_mode == InlineMode::STANDARD) {
 				// not inlined
 				// look forward until the corresponding bracket open - can we inline and not exceed the column width?
 				idx_t peek_depth = 0;
 				idx_t render_size = line_length;
 				idx_t peek_idx;
-				bool inline_bracket = false;
+				InlineMode inline_child_mode = InlineMode::STANDARD;
 				for (peek_idx = c + 1; peek_idx < components.size() && render_size <= max_width; peek_idx++) {
 					auto &peek_component = components[peek_idx];
 					if (peek_component.type == JSONComponentType::BRACKET_OPEN) {
@@ -1153,7 +1180,10 @@ protected:
 					} else if (peek_component.type == JSONComponentType::BRACKET_CLOSE) {
 						if (peek_depth == 0) {
 							// close!
-							inline_bracket = render_size + 1 < max_width;
+							if (render_size + 1 < max_width) {
+								// fits within a single line - inline on a single line
+								inline_child_mode = InlineMode::INLINED_SINGLE_LINE;
+							}
 							break;
 						}
 						peek_depth--;
@@ -1164,11 +1194,42 @@ protected:
 						render_size++;
 					}
 				}
-				if (inline_bracket) {
+				if (component.text == "[") {
+					// for arrays - we always inline them UNLESS there are complex objects INSIDE of the bracket
+					// scan forward until the end of the array to figure out if this is true or not
+					for (peek_idx = c + 1; peek_idx < components.size(); peek_idx++) {
+						auto &peek_component = components[peek_idx];
+						peek_depth = 0;
+						if (peek_component.type == JSONComponentType::BRACKET_OPEN) {
+							if (peek_component.text == "{") {
+								// nested structure within the array
+								break;
+							}
+							peek_depth++;
+						}
+						if (peek_component.type == JSONComponentType::BRACKET_CLOSE) {
+							if (peek_depth == 0) {
+								inline_child_mode = InlineMode::INLINED_MULTI_LINE;
+								break;
+							}
+							peek_depth--;
+						}
+					}
+				}
+				if (inline_child_mode != InlineMode::STANDARD) {
 					// we can inline! do it
 					for (idx_t inline_idx = c + 1; inline_idx <= peek_idx; inline_idx++) {
 						auto &inline_component = components[inline_idx];
-						FormatComponent(format_state, inline_component, true);
+						if (inline_child_mode == InlineMode::INLINED_MULTI_LINE && inline_idx + 1 <= peek_idx) {
+							auto &next_component = components[inline_idx + 1];
+							if (next_component.type == JSONComponentType::COMMA ||
+							    next_component.type == JSONComponentType::BRACKET_CLOSE) {
+								if (!LiteralFits(format_state, inline_component.text + next_component.text)) {
+									AddNewline(format_state);
+								}
+							}
+						}
+						FormatComponent(format_state, inline_component, inline_child_mode);
 					}
 					c = peek_idx;
 					return;
@@ -1184,21 +1245,22 @@ protected:
 			}
 			break;
 		}
-		case JSONComponentType::BRACKET_CLOSE:
-			depth--;
-			if (!inlined) {
+		case JSONComponentType::BRACKET_CLOSE: {
+			idx_t depth_diff = component.text == "}" ? format_state.indentation_size : 1;
+			if (depth < depth_diff) {
+				// shouldn't happen - but guard against underflows
+				depth = 0;
+			} else {
+				depth -= depth_diff;
+			}
+			if (inline_mode == InlineMode::STANDARD) {
 				AddNewline(format_state);
 			}
 			AddLiteral(format_state, component.text);
 			break;
+		}
 		case JSONComponentType::COMMA:
 		case JSONComponentType::COLON:
-			if (format_state.mode != JSONFormattingMode::STANDARD) {
-				// add a newline if the comma does not fit
-				if (!LiteralFits(format_state, component.text)) {
-					AddNewline(format_state);
-				}
-			}
 			AddLiteral(format_state, component.text);
 			bool always_inline;
 			if (format_state.mode == JSONFormattingMode::COMPACT_HORIZONTAL) {
@@ -1208,8 +1270,8 @@ protected:
 				// in normal processing we always inline colons
 				always_inline = component.type == JSONComponentType::COLON;
 			}
-			if (inlined || always_inline) {
-				AddLiteral(format_state, " ");
+			if (inline_mode != InlineMode::STANDARD || always_inline) {
+				AddSpace(format_state);
 			} else {
 				if (format_state.mode != JSONFormattingMode::STANDARD) {
 					// if we are not inlining in compact mode, try to inline until the next comma
@@ -1241,10 +1303,10 @@ protected:
 					}
 					if (inline_comma) {
 						// we can inline until the next comma! do it
-						AddLiteral(format_state, " ");
+						AddSpace(format_state);
 						for (idx_t inline_idx = c + 1; inline_idx < peek_idx; inline_idx++) {
 							auto &inline_component = components[inline_idx];
-							FormatComponent(format_state, inline_component, true);
+							FormatComponent(format_state, inline_component, InlineMode::INLINED_SINGLE_LINE);
 						}
 						c = peek_idx - 1;
 						return;
@@ -1273,7 +1335,7 @@ protected:
 		                                     format_state.format_result == JSONFormattingResult::SUCCESS;
 		     format_state.component_idx++) {
 			auto &component = components[format_state.component_idx];
-			FormatComponent(format_state, component, false);
+			FormatComponent(format_state, component, InlineMode::STANDARD);
 		}
 
 		if (format_state.format_result != JSONFormattingResult::SUCCESS) {
diff --git a/src/common/compressed_file_system.cpp b/src/common/compressed_file_system.cpp
index 79ea32c715..5937862040 100644
--- a/src/common/compressed_file_system.cpp
+++ b/src/common/compressed_file_system.cpp
@@ -15,14 +15,7 @@ CompressedFile::~CompressedFile() {
 	try {
 		// stream_wrapper->Close() might throw
 		CompressedFile::Close();
-	} catch (std::exception &ex) {
-		if (child_handle) {
-			// FIXME: make more log context available here.
-			ErrorData data(ex);
-			DUCKDB_LOG_ERROR(child_handle->logger, "CompressedFile::~CompressedFile()\t\t" + data.Message());
-		}
-	} catch (...) {
-		// NOLINT
+	} catch (...) { // NOLINT - cannot throw in exception
 	}
 }
 
diff --git a/src/common/encryption_key_manager.cpp b/src/common/encryption_key_manager.cpp
index 9d16a159bd..425711a1a8 100644
--- a/src/common/encryption_key_manager.cpp
+++ b/src/common/encryption_key_manager.cpp
@@ -19,7 +19,8 @@ EncryptionKey::EncryptionKey(data_ptr_t encryption_key_p) {
 	D_ASSERT(memcmp(key, encryption_key_p, MainHeader::DEFAULT_ENCRYPTION_KEY_LENGTH) == 0);
 
 	// zero out the encryption key in memory
-	memset(encryption_key_p, 0, MainHeader::DEFAULT_ENCRYPTION_KEY_LENGTH);
+	duckdb_mbedtls::MbedTlsWrapper::AESStateMBEDTLS::SecureClearData(encryption_key_p,
+	                                                                 MainHeader::DEFAULT_ENCRYPTION_KEY_LENGTH);
 	LockEncryptionKey(key);
 }
 
@@ -39,7 +40,7 @@ void EncryptionKey::LockEncryptionKey(data_ptr_t key, idx_t key_len) {
 }
 
 void EncryptionKey::UnlockEncryptionKey(data_ptr_t key, idx_t key_len) {
-	memset(key, 0, key_len);
+	duckdb_mbedtls::MbedTlsWrapper::AESStateMBEDTLS::SecureClearData(key, key_len);
 #if defined(_WIN32)
 	VirtualUnlock(key, key_len);
 #elif defined(__MVS__)
@@ -68,7 +69,8 @@ EncryptionKeyManager &EncryptionKeyManager::Get(DatabaseInstance &db) {
 
 string EncryptionKeyManager::GenerateRandomKeyID() {
 	uint8_t key_id[KEY_ID_BYTES];
-	duckdb_mbedtls::MbedTlsWrapper::AESStateMBEDTLS::GenerateRandomDataStatic(key_id, KEY_ID_BYTES);
+	RandomEngine engine;
+	engine.RandomData(key_id, KEY_ID_BYTES);
 	string key_id_str(reinterpret_cast<const char *>(key_id), KEY_ID_BYTES);
 	return key_id_str;
 }
@@ -76,7 +78,7 @@ string EncryptionKeyManager::GenerateRandomKeyID() {
 void EncryptionKeyManager::AddKey(const string &key_name, data_ptr_t key) {
 	derived_keys.emplace(key_name, EncryptionKey(key));
 	// Zero-out the encryption key
-	std::memset(key, 0, DERIVED_KEY_LENGTH);
+	duckdb_mbedtls::MbedTlsWrapper::AESStateMBEDTLS::SecureClearData(key, DERIVED_KEY_LENGTH);
 }
 
 bool EncryptionKeyManager::HasKey(const string &key_name) const {
@@ -111,7 +113,7 @@ string EncryptionKeyManager::Base64Decode(const string &key) {
 	auto output = duckdb::unique_ptr<unsigned char[]>(new unsigned char[result_size]);
 	Blob::FromBase64(key, output.get(), result_size);
 	string decoded_key(reinterpret_cast<const char *>(output.get()), result_size);
-	memset(output.get(), 0, result_size);
+	duckdb_mbedtls::MbedTlsWrapper::AESStateMBEDTLS::SecureClearData(output.get(), result_size);
 	return decoded_key;
 }
 
@@ -128,10 +130,9 @@ void EncryptionKeyManager::DeriveKey(string &user_key, data_ptr_t salt, data_ptr
 
 	KeyDerivationFunctionSHA256(reinterpret_cast<const_data_ptr_t>(decoded_key.data()), decoded_key.size(), salt,
 	                            derived_key);
-
-	// wipe the original and decoded key
-	std::fill(user_key.begin(), user_key.end(), 0);
-	std::fill(decoded_key.begin(), decoded_key.end(), 0);
+	duckdb_mbedtls::MbedTlsWrapper::AESStateMBEDTLS::SecureClearData(data_ptr_cast(&user_key[0]), user_key.size());
+	duckdb_mbedtls::MbedTlsWrapper::AESStateMBEDTLS::SecureClearData(data_ptr_cast(&decoded_key[0]),
+	                                                                 decoded_key.size());
 	user_key.clear();
 	decoded_key.clear();
 }
diff --git a/src/common/exception.cpp b/src/common/exception.cpp
index d30dbf2984..6f5b44847c 100644
--- a/src/common/exception.cpp
+++ b/src/common/exception.cpp
@@ -334,7 +334,6 @@ InterruptException::InterruptException() : Exception(ExceptionType::INTERRUPT, "
 }
 
 FatalException::FatalException(ExceptionType type, const string &msg) : Exception(type, msg) {
-	// FIXME: Log error here
 }
 
 InternalException::InternalException(const string &msg) : Exception(ExceptionType::INTERNAL, msg) {
diff --git a/src/common/file_buffer.cpp b/src/common/file_buffer.cpp
index 8e108ddc19..94223eed33 100644
--- a/src/common/file_buffer.cpp
+++ b/src/common/file_buffer.cpp
@@ -1,6 +1,6 @@
 #include "duckdb/common/file_buffer.hpp"
 
-#include "duckdb/common/allocator.hpp"
+#include "duckdb/storage/block_allocator.hpp"
 #include "duckdb/common/exception.hpp"
 #include "duckdb/common/file_system.hpp"
 #include "duckdb/common/helper.hpp"
@@ -12,7 +12,7 @@
 
 namespace duckdb {
 
-FileBuffer::FileBuffer(Allocator &allocator, FileBufferType type, uint64_t user_size, idx_t block_header_size)
+FileBuffer::FileBuffer(BlockAllocator &allocator, FileBufferType type, uint64_t user_size, idx_t block_header_size)
     : allocator(allocator), type(type) {
 	Init();
 	if (user_size) {
@@ -20,7 +20,7 @@ FileBuffer::FileBuffer(Allocator &allocator, FileBufferType type, uint64_t user_
 	}
 }
 
-FileBuffer::FileBuffer(Allocator &allocator, FileBufferType type, BlockManager &block_manager)
+FileBuffer::FileBuffer(BlockAllocator &allocator, FileBufferType type, BlockManager &block_manager)
     : allocator(allocator), type(type) {
 	Init();
 	Resize(block_manager);
diff --git a/src/common/gzip_file_system.cpp b/src/common/gzip_file_system.cpp
index 45b9f13edf..92b4e10d2f 100644
--- a/src/common/gzip_file_system.cpp
+++ b/src/common/gzip_file_system.cpp
@@ -93,14 +93,7 @@ MiniZStreamWrapper::~MiniZStreamWrapper() {
 	}
 	try {
 		MiniZStreamWrapper::Close();
-	} catch (std::exception &ex) {
-		if (file && file->child_handle) {
-			// FIXME: make more log context available here.
-			ErrorData data(ex);
-			DUCKDB_LOG_ERROR(file->child_handle->logger,
-			                 "MiniZStreamWrapper::~MiniZStreamWrapper()\t\t" + data.Message())
-		}
-	} catch (...) { // NOLINT
+	} catch (...) { // NOLINT - cannot throw in exception
 	}
 }
 
diff --git a/src/common/pipe_file_system.cpp b/src/common/pipe_file_system.cpp
index dc9b7d1083..6f0a29741a 100644
--- a/src/common/pipe_file_system.cpp
+++ b/src/common/pipe_file_system.cpp
@@ -2,6 +2,7 @@
 #include "duckdb/common/exception.hpp"
 #include "duckdb/common/file_system.hpp"
 #include "duckdb/common/helper.hpp"
+#include "duckdb/common/local_file_system.hpp"
 #include "duckdb/common/numeric_utils.hpp"
 #include "duckdb/main/client_context.hpp"
 
@@ -53,6 +54,11 @@ int64_t PipeFileSystem::GetFileSize(FileHandle &handle) {
 	return 0;
 }
 
+timestamp_t PipeFileSystem::GetLastModifiedTime(FileHandle &handle) {
+	auto &child_handle = *handle.Cast<PipeFile>().child_handle;
+	return child_handle.file_system.GetLastModifiedTime(child_handle);
+}
+
 void PipeFileSystem::FileSync(FileHandle &handle) {
 }
 
diff --git a/src/common/printer.cpp b/src/common/printer.cpp
index e07d3f8f2b..46187ac24c 100644
--- a/src/common/printer.cpp
+++ b/src/common/printer.cpp
@@ -31,8 +31,7 @@ void Printer::RawPrint(OutputStream stream, const string &str) {
 }
 
 void Printer::DefaultLinePrint(OutputStream stream, const string &str) {
-	Printer::RawPrint(stream, str);
-	Printer::RawPrint(stream, "\n");
+	Printer::RawPrint(stream, str + "\n");
 }
 
 line_printer_f Printer::line_printer = Printer::DefaultLinePrint;
diff --git a/src/common/random_engine.cpp b/src/common/random_engine.cpp
index 78403e0301..156b4baec5 100644
--- a/src/common/random_engine.cpp
+++ b/src/common/random_engine.cpp
@@ -82,4 +82,14 @@ void RandomEngine::SetSeed(uint64_t seed) {
 	random_state->pcg.seed(seed);
 }
 
+void RandomEngine::RandomData(duckdb::data_ptr_t data, duckdb::idx_t len) {
+	while (len) {
+		const auto random_integer = NextRandomInteger();
+		const auto next = duckdb::MinValue<duckdb::idx_t>(len, sizeof(random_integer));
+		memcpy(data, duckdb::const_data_ptr_cast(&random_integer), next);
+		data += next;
+		len -= next;
+	}
+}
+
 } // namespace duckdb
diff --git a/src/common/serializer/memory_stream.cpp b/src/common/serializer/memory_stream.cpp
index eb4fb6d576..277b94460c 100644
--- a/src/common/serializer/memory_stream.cpp
+++ b/src/common/serializer/memory_stream.cpp
@@ -59,6 +59,12 @@ MemoryStream &MemoryStream::operator=(MemoryStream &&other) noexcept {
 }
 
 void MemoryStream::WriteData(const_data_ptr_t source, idx_t write_size) {
+	GrowCapacity(write_size);
+	memcpy(data + position, source, write_size);
+	position += write_size;
+}
+
+void MemoryStream::GrowCapacity(idx_t write_size) {
 	const auto old_capacity = capacity;
 	while (position + write_size > capacity) {
 		if (allocator) {
@@ -70,8 +76,6 @@ void MemoryStream::WriteData(const_data_ptr_t source, idx_t write_size) {
 	if (capacity != old_capacity) {
 		data = allocator->ReallocateData(data, old_capacity, capacity);
 	}
-	memcpy(data + position, source, write_size);
-	position += write_size;
 }
 
 void MemoryStream::ReadData(data_ptr_t destination, idx_t read_size) {
diff --git a/src/common/settings.json b/src/common/settings.json
index 81b362edb5..7a377c2a09 100644
--- a/src/common/settings.json
+++ b/src/common/settings.json
@@ -155,6 +155,17 @@
         "type": "BOOLEAN",
         "scope": "global"
     },
+    {
+        "name": "block_allocator_memory",
+        "description": "Physical memory that the block allocator is allowed to use (this memory is never freed and cannot be reduced).",
+        "type": "VARCHAR",
+        "scope": "global",
+        "custom_implementation": [
+            "get",
+            "set",
+            "reset"
+        ]
+    },
     {
         "name": "catalog_error_max_schemas",
         "description": "The maximum number of schemas the system will scan for \\\"did you mean...\\\" style errors in the catalog",
diff --git a/src/common/types/row/tuple_data_collection.cpp b/src/common/types/row/tuple_data_collection.cpp
index a068e0ab4b..0dfdb6810b 100644
--- a/src/common/types/row/tuple_data_collection.cpp
+++ b/src/common/types/row/tuple_data_collection.cpp
@@ -623,6 +623,43 @@ bool TupleDataCollection::Scan(TupleDataParallelScanState &gstate, TupleDataLoca
 	return true;
 }
 
+idx_t TupleDataCollection::Seek(TupleDataScanState &state, const idx_t target_chunk) {
+	D_ASSERT(state.pin_state.properties == TupleDataPinProperties::UNPIN_AFTER_DONE);
+	state.pin_state.row_handles.clear();
+	state.pin_state.heap_handles.clear();
+
+	// early return for empty collection
+	if (segments.empty()) {
+		return 0;
+	}
+
+	idx_t current_chunk = 0;
+	idx_t total_rows = 0;
+	for (idx_t seg_idx = 0; seg_idx < segments.size(); seg_idx++) {
+		auto &segment = segments[seg_idx];
+		idx_t chunk_count = segment->ChunkCount();
+
+		if (current_chunk + chunk_count <= target_chunk) {
+			total_rows += segment->count;
+			current_chunk += chunk_count;
+		} else {
+			idx_t chunk_idx_in_segment = target_chunk - current_chunk;
+			for (idx_t chunk_idx = 0; chunk_idx < chunk_idx_in_segment; chunk_idx++) {
+				total_rows += segment->chunks[chunk_idx]->count;
+			}
+			current_chunk += chunk_count;
+
+			// reset scan state to target segment
+			state.segment_index = seg_idx;
+			state.chunk_index = chunk_idx_in_segment;
+			break;
+		}
+	}
+
+	D_ASSERT(target_chunk < current_chunk);
+	return total_rows;
+}
+
 bool TupleDataCollection::ScanComplete(const TupleDataScanState &state) const {
 	if (Count() == 0) {
 		return true;
diff --git a/src/common/types/vector.cpp b/src/common/types/vector.cpp
index 070907c720..7363f952a8 100644
--- a/src/common/types/vector.cpp
+++ b/src/common/types/vector.cpp
@@ -921,7 +921,6 @@ idx_t Vector::GetAllocationSize(idx_t cardinality) const {
 	}
 	default:
 		throw NotImplementedException("Vector::GetAllocationSize not implemented for type: %s", type.ToString());
-		break;
 	}
 }
 
diff --git a/src/execution/expression_executor/execute_comparison.cpp b/src/execution/expression_executor/execute_comparison.cpp
index fafb24d49b..f55c26df36 100644
--- a/src/execution/expression_executor/execute_comparison.cpp
+++ b/src/execution/expression_executor/execute_comparison.cpp
@@ -138,8 +138,19 @@ static idx_t TemplatedSelectOperation(Vector &left, Vector &right, optional_ptr<
 		                                                      false_sel.get());
 	case PhysicalType::LIST:
 	case PhysicalType::STRUCT:
-	case PhysicalType::ARRAY:
-		return NestedSelectOperation<OP>(left, right, sel, count, true_sel, false_sel, null_mask);
+	case PhysicalType::ARRAY: {
+		auto result_count = NestedSelectOperation<OP>(left, right, sel, count, true_sel, false_sel, null_mask);
+		if (true_sel && result_count > 0) {
+			std::sort(true_sel->data(), true_sel->data() + result_count);
+		}
+		if (false_sel) {
+			idx_t false_count = count - result_count;
+			if (false_count > 0) {
+				std::sort(false_sel->data(), false_sel->data() + false_count);
+			}
+		}
+		return result_count;
+	}
 	default:
 		throw InternalException("Invalid type for comparison");
 	}
diff --git a/src/execution/join_hashtable.cpp b/src/execution/join_hashtable.cpp
index 8327049b95..35cbe456a6 100644
--- a/src/execution/join_hashtable.cpp
+++ b/src/execution/join_hashtable.cpp
@@ -887,6 +887,7 @@ idx_t ScanStructure::ResolvePredicates(DataChunk &keys, SelectionVector &match_s
 	}
 
 	// If there is a matcher for the probing side because of non-equality predicates, use it
+	idx_t result_count;
 	if (ht.needs_chain_matcher) {
 		idx_t no_match_count = 0;
 		auto &matcher = no_match_sel ? ht.row_matcher_probe_no_match_sel : ht.row_matcher_probe;
@@ -894,12 +895,17 @@ idx_t ScanStructure::ResolvePredicates(DataChunk &keys, SelectionVector &match_s
 
 		// we need to only use the vectors with the indices of the columns that are used in the probe phase, namely
 		// the non-equality columns
-		return matcher->Match(keys, key_state.vector_data, match_sel, this->count, pointers, no_match_sel,
-		                      no_match_count);
+		result_count =
+		    matcher->Match(keys, key_state.vector_data, match_sel, this->count, pointers, no_match_sel, no_match_count);
 	} else {
 		// no match sel is the opposite of match sel
-		return this->count;
+		result_count = this->count;
 	}
+
+	// Update total probe match count
+	ht.total_probe_matches.fetch_add(result_count, std::memory_order_relaxed);
+
+	return result_count;
 }
 
 idx_t ScanStructure::ScanInnerJoin(DataChunk &keys, SelectionVector &result_vector) {
diff --git a/src/execution/operator/join/physical_hash_join.cpp b/src/execution/operator/join/physical_hash_join.cpp
index ad9a7841c6..1290b01955 100644
--- a/src/execution/operator/join/physical_hash_join.cpp
+++ b/src/execution/operator/join/physical_hash_join.cpp
@@ -163,6 +163,11 @@ public:
 		}
 	}
 
+	~HashJoinGlobalSinkState() override {
+		DUCKDB_LOG(context, PhysicalOperatorLogType, op, "PhysicalHashJoin", "GetData",
+		           {{"total_probe_matches", to_string(hash_table->total_probe_matches)}});
+	}
+
 	void ScheduleFinalize(Pipeline &pipeline, Event &event);
 	void InitializeProbeSpill();
 
diff --git a/src/execution/operator/join/physical_iejoin.cpp b/src/execution/operator/join/physical_iejoin.cpp
index cffb4eb438..cdca522e10 100644
--- a/src/execution/operator/join/physical_iejoin.cpp
+++ b/src/execution/operator/join/physical_iejoin.cpp
@@ -381,13 +381,7 @@ idx_t IEJoinUnion::AppendKey(ExecutionContext &context, InterruptState &interrup
 
 	DataChunk scanned;
 	source.InitializeScanChunk(scanner, scanned);
-
-	// TODO: Random access into TupleDataCollection (NextScanIndex is private...)
-	idx_t table_idx = 0;
-	for (idx_t i = 0; i < chunk_begin; ++i) {
-		source.Scan(scanner, scanned);
-		table_idx += scanned.size();
-	}
+	idx_t table_idx = source.Seek(scanner, chunk_begin);
 
 	// Writing
 	auto &sort = *marked.sort;
diff --git a/src/execution/operator/scan/physical_table_scan.cpp b/src/execution/operator/scan/physical_table_scan.cpp
index 4d86eace72..bc165aea04 100644
--- a/src/execution/operator/scan/physical_table_scan.cpp
+++ b/src/execution/operator/scan/physical_table_scan.cpp
@@ -362,7 +362,7 @@ bool PhysicalTableScan::Equals(const PhysicalOperator &other_p) const {
 		return false;
 	}
 	auto &other = other_p.Cast<PhysicalTableScan>();
-	if (function.function != other.function.function) {
+	if (function != other.function) {
 		return false;
 	}
 	if (column_ids != other.column_ids) {
diff --git a/src/execution/physical_plan/plan_filter.cpp b/src/execution/physical_plan/plan_filter.cpp
index 292fe1bc84..796e4aeb38 100644
--- a/src/execution/physical_plan/plan_filter.cpp
+++ b/src/execution/physical_plan/plan_filter.cpp
@@ -14,7 +14,6 @@ PhysicalOperator &PhysicalPlanGenerator::CreatePlan(LogicalFilter &op) {
 	D_ASSERT(op.children.size() == 1);
 	reference<PhysicalOperator> plan = CreatePlan(*op.children[0]);
 	if (!op.expressions.empty()) {
-		D_ASSERT(!plan.get().GetTypes().empty());
 		// create a filter if there is anything to filter
 		auto &filter = Make<PhysicalFilter>(plan.get().GetTypes(), std::move(op.expressions), op.estimated_cardinality);
 		filter.children.push_back(plan);
diff --git a/src/function/scalar/compressed_materialization/compress_string.cpp b/src/function/scalar/compressed_materialization/compress_string.cpp
index 21e92c5c3a..3213b2aa25 100644
--- a/src/function/scalar/compressed_materialization/compress_string.cpp
+++ b/src/function/scalar/compressed_materialization/compress_string.cpp
@@ -44,7 +44,7 @@ inline RESULT_TYPE StringCompressInternal(const string_t &input) {
 		memset(result_ptr, '\0', remainder);
 	}
 	result_ptr[0] = UnsafeNumericCast<data_t>(input.GetSize());
-	return result;
+	return BSwapIfBE(result);
 }
 
 template <class RESULT_TYPE>
@@ -55,13 +55,15 @@ inline RESULT_TYPE StringCompress(const string_t &input) {
 
 template <class RESULT_TYPE>
 inline RESULT_TYPE MiniStringCompress(const string_t &input) {
+	RESULT_TYPE result;
 	if (sizeof(RESULT_TYPE) <= string_t::INLINE_LENGTH) {
-		return UnsafeNumericCast<RESULT_TYPE>(input.GetSize() + *const_data_ptr_cast(input.GetPrefix()));
+		result = UnsafeNumericCast<RESULT_TYPE>(input.GetSize() + *const_data_ptr_cast(input.GetPrefix()));
 	} else if (input.GetSize() == 0) {
-		return 0;
+		result = 0;
 	} else {
-		return UnsafeNumericCast<RESULT_TYPE>(input.GetSize() + *const_data_ptr_cast(input.GetPointer()));
+		result = UnsafeNumericCast<RESULT_TYPE>(input.GetSize() + *const_data_ptr_cast(input.GetPointer()));
 	}
+	return BSwapIfBE(result);
 }
 
 template <>
@@ -126,19 +128,20 @@ public:
 
 template <class INPUT_TYPE>
 inline string_t StringDecompress(const INPUT_TYPE &input, ArenaAllocator &allocator) {
-	const auto input_ptr = const_data_ptr_cast(&input);
-	string_t result(input_ptr[0]);
+	const auto le_input = BSwapIfBE(input);
+	const auto le_input_str = const_data_ptr_cast(&le_input);
+	string_t result(le_input_str[0]);
 	if (sizeof(INPUT_TYPE) <= string_t::INLINE_LENGTH) {
 		const auto result_ptr = data_ptr_cast(result.GetPrefixWriteable());
-		TemplatedReverseMemCpy<sizeof(INPUT_TYPE)>(result_ptr, input_ptr);
+		TemplatedReverseMemCpy<sizeof(INPUT_TYPE)>(result_ptr, le_input_str);
 		memset(result_ptr + sizeof(INPUT_TYPE) - 1, '\0', string_t::INLINE_LENGTH - sizeof(INPUT_TYPE) + 1);
 	} else if (result.GetSize() <= string_t::INLINE_LENGTH) {
 		static constexpr auto REMAINDER = sizeof(INPUT_TYPE) - string_t::INLINE_LENGTH;
 		const auto result_ptr = data_ptr_cast(result.GetPrefixWriteable());
-		TemplatedReverseMemCpy<string_t::INLINE_LENGTH>(result_ptr, input_ptr + REMAINDER);
+		TemplatedReverseMemCpy<string_t::INLINE_LENGTH>(result_ptr, le_input_str + REMAINDER);
 	} else {
 		result.SetPointer(char_ptr_cast(allocator.Allocate(sizeof(INPUT_TYPE))));
-		TemplatedReverseMemCpy<sizeof(INPUT_TYPE)>(data_ptr_cast(result.GetPointer()), input_ptr);
+		TemplatedReverseMemCpy<sizeof(INPUT_TYPE)>(data_ptr_cast(result.GetPointer()), le_input_str);
 		memcpy(result.GetPrefixWriteable(), result.GetPointer(), string_t::PREFIX_LENGTH);
 	}
 	return result;
@@ -146,7 +149,8 @@ inline string_t StringDecompress(const INPUT_TYPE &input, ArenaAllocator &alloca
 
 template <class INPUT_TYPE>
 inline string_t MiniStringDecompress(const INPUT_TYPE &input, ArenaAllocator &allocator) {
-	if (input == 0) {
+	const auto le_input = BSwapIfBE(input);
+	if (le_input == 0) {
 		string_t result(uint32_t(0));
 		memset(result.GetPrefixWriteable(), '\0', string_t::INLINE_BYTES);
 		return result;
@@ -155,10 +159,10 @@ inline string_t MiniStringDecompress(const INPUT_TYPE &input, ArenaAllocator &al
 	string_t result(1);
 	if (sizeof(INPUT_TYPE) <= string_t::INLINE_LENGTH) {
 		memset(result.GetPrefixWriteable(), '\0', string_t::INLINE_BYTES);
-		*data_ptr_cast(result.GetPrefixWriteable()) = input - 1;
+		*data_ptr_cast(result.GetPrefixWriteable()) = le_input - 1;
 	} else {
 		result.SetPointer(char_ptr_cast(allocator.Allocate(1)));
-		*data_ptr_cast(result.GetPointer()) = input - 1;
+		*data_ptr_cast(result.GetPointer()) = le_input - 1;
 		memset(result.GetPrefixWriteable(), '\0', string_t::PREFIX_LENGTH);
 		*result.GetPrefixWriteable() = *result.GetPointer();
 	}
diff --git a/src/function/scalar/create_sort_key.cpp b/src/function/scalar/create_sort_key.cpp
index d93e20d71d..9c043d4e69 100644
--- a/src/function/scalar/create_sort_key.cpp
+++ b/src/function/scalar/create_sort_key.cpp
@@ -711,7 +711,7 @@ void FinalizeSortData(Vector &result, idx_t size, const SortKeyLengthInfo &key_l
 	case LogicalTypeId::BIGINT: {
 		auto result_data = FlatVector::GetData<int64_t>(result);
 		for (idx_t r = 0; r < size; r++) {
-			result_data[r] = BSwap(result_data[r]);
+			result_data[r] = BSwapIfLE(result_data[r]);
 		}
 		break;
 	}
@@ -1213,13 +1213,13 @@ static void DecodeSortKeyFunction(DataChunk &args, ExpressionState &state, Vecto
 			for (idx_t i = 0; i < count; i++) {
 				const auto idx = sort_key_vec_format.sel->get_index(i);
 				D_ASSERT(sort_key_vec_format.validity.RowIsValid(idx));
-				bswapped_ints[i] = BSwap(sort_keys[idx]);
+				bswapped_ints[i] = BSwapIfLE(sort_keys[idx]);
 				decode_data[i] = DecodeSortKeyData(bswapped_ints[i]);
 			}
 		} else {
 			for (idx_t i = 0; i < count; i++) {
 				D_ASSERT(sort_key_vec_format.validity.RowIsValid(i));
-				bswapped_ints[i] = BSwap(sort_keys[i]);
+				bswapped_ints[i] = BSwapIfLE(sort_keys[i]);
 				decode_data[i] = DecodeSortKeyData(bswapped_ints[i]);
 			}
 		}
diff --git a/src/function/scalar/system/parse_log_message.cpp b/src/function/scalar/system/parse_log_message.cpp
index e6625a5c25..db2e512d41 100644
--- a/src/function/scalar/system/parse_log_message.cpp
+++ b/src/function/scalar/system/parse_log_message.cpp
@@ -76,8 +76,10 @@ void ParseLogMessageFunction(DataChunk &args, ExpressionState &state, Vector &re
 } // namespace
 
 ScalarFunction ParseLogMessage::GetFunction() {
-	return ScalarFunction({LogicalType::VARCHAR, LogicalType::VARCHAR}, LogicalType::ANY, ParseLogMessageFunction,
-	                      ParseLogMessageBind, nullptr, nullptr, nullptr, LogicalType(LogicalTypeId::INVALID));
+	auto fun = ScalarFunction({LogicalType::VARCHAR, LogicalType::VARCHAR}, LogicalType::ANY, ParseLogMessageFunction,
+	                          ParseLogMessageBind, nullptr, nullptr, nullptr, LogicalType(LogicalTypeId::INVALID));
+	fun.errors = FunctionErrors::CAN_THROW_RUNTIME_ERROR;
+	return fun;
 }
 
 } // namespace duckdb
diff --git a/src/function/table/arrow.cpp b/src/function/table/arrow.cpp
index d41f63b2c4..f2f9327689 100644
--- a/src/function/table/arrow.cpp
+++ b/src/function/table/arrow.cpp
@@ -200,11 +200,11 @@ void ArrowTableFunction::ArrowScanFunction(ClientContext &context, TableFunction
 	if (global_state.CanRemoveFilterColumns()) {
 		state.all_columns.Reset();
 		state.all_columns.SetCardinality(output_size);
-		ArrowToDuckDB(state, data.arrow_table.GetColumns(), state.all_columns, data.lines_read - output_size);
+		ArrowToDuckDB(state, data.arrow_table.GetColumns(), state.all_columns);
 		output.ReferenceColumns(state.all_columns, global_state.projection_ids);
 	} else {
 		output.SetCardinality(output_size);
-		ArrowToDuckDB(state, data.arrow_table.GetColumns(), output, data.lines_read - output_size);
+		ArrowToDuckDB(state, data.arrow_table.GetColumns(), output);
 	}
 
 	output.Verify();
diff --git a/src/function/table/arrow_conversion.cpp b/src/function/table/arrow_conversion.cpp
index 65f617e837..511a272dc3 100644
--- a/src/function/table/arrow_conversion.cpp
+++ b/src/function/table/arrow_conversion.cpp
@@ -409,10 +409,10 @@ static void UUIDConversion(Vector &vector, const ArrowArray &array, idx_t chunk_
 		if (!validity_mask.RowIsValid(row)) {
 			continue;
 		}
-		tgt_ptr[row].lower = static_cast<uint64_t>(BSwap(src_ptr[row].upper));
+		tgt_ptr[row].lower = static_cast<uint64_t>(BSwapIfLE(src_ptr[row].upper));
 		// flip Upper MSD
-		tgt_ptr[row].upper =
-		    static_cast<int64_t>(static_cast<uint64_t>(BSwap(src_ptr[row].lower)) ^ (static_cast<uint64_t>(1) << 63));
+		tgt_ptr[row].upper = static_cast<int64_t>(static_cast<uint64_t>(BSwapIfLE(src_ptr[row].lower)) ^
+		                                          (static_cast<uint64_t>(1) << 63));
 	}
 }
 
@@ -1367,8 +1367,7 @@ void ArrowToDuckDBConversion::ColumnArrowToDuckDBDictionary(Vector &vector, Arro
 }
 
 void ArrowTableFunction::ArrowToDuckDB(ArrowScanLocalState &scan_state, const arrow_column_map_t &arrow_convert_data,
-                                       DataChunk &output, idx_t start, bool arrow_scan_is_projected,
-                                       idx_t rowid_column_index) {
+                                       DataChunk &output, bool arrow_scan_is_projected, idx_t rowid_column_index) {
 	for (idx_t idx = 0; idx < output.ColumnCount(); idx++) {
 		auto col_idx = scan_state.column_ids.empty() ? idx : scan_state.column_ids[idx];
 
diff --git a/src/function/table/direct_file_reader.cpp b/src/function/table/direct_file_reader.cpp
index e28d782184..693c54800d 100644
--- a/src/function/table/direct_file_reader.cpp
+++ b/src/function/table/direct_file_reader.cpp
@@ -1,4 +1,6 @@
 #include "duckdb/function/table/direct_file_reader.hpp"
+
+#include "duckdb/common/serializer/memory_stream.hpp"
 #include "duckdb/function/table/read_file.hpp"
 #include "duckdb/storage/caching_file_system.hpp"
 
@@ -55,7 +57,7 @@ AsyncResult DirectFileReader::Scan(ClientContext &context, GlobalTableFunctionSt
 
 	auto &regular_fs = FileSystem::GetFileSystem(context);
 	auto fs = CachingFileSystem::Get(context);
-	idx_t out_idx = 0;
+	const idx_t out_idx = 0;
 
 	// We utilize projection pushdown here to only read the file content if the 'data' column is requested
 	unique_ptr<CachingFileHandle> file_handle = nullptr;
@@ -91,19 +93,21 @@ AsyncResult DirectFileReader::Scan(ClientContext &context, GlobalTableFunctionSt
 				FlatVector::GetData<string_t>(file_name_vector)[out_idx] = file_name_string;
 			} break;
 			case ReadFileBindData::FILE_CONTENT_COLUMN: {
-				auto file_size_raw = file_handle->GetFileSize();
-				AssertMaxFileSize(file.path, file_size_raw);
-				auto file_size = UnsafeNumericCast<int64_t>(file_size_raw);
-				auto &file_content_vector = output.data[col_idx];
-				auto content_string = StringVector::EmptyString(file_content_vector, file_size_raw);
+				const auto file_size = file_handle->GetFileSize();
+				AssertMaxFileSize(file.path, file_size);
 
-				auto remaining_bytes = UnsafeNumericCast<int64_t>(file_size);
+				// Initialize write stream if not yet done
+				if (!state.stream) {
+					state.stream = make_uniq<MemoryStream>(BufferAllocator::Get(context), NextPowerOfTwo(file_size));
+				}
+				state.stream->Rewind();
 
-				// Read in batches of 100mb
-				constexpr auto MAX_READ_SIZE = 100LL * 1024 * 1024;
+				// Read in batches of 128mb
+				constexpr idx_t MAX_READ_SIZE = 128LL * 1024 * 1024;
+				auto remaining_bytes = file_handle->GetFileHandle().IsPipe() ? MAX_READ_SIZE : file_size;
 				while (remaining_bytes > 0) {
-					const auto bytes_to_read = MinValue<int64_t>(remaining_bytes, MAX_READ_SIZE);
-					const auto content_string_ptr = content_string.GetDataWriteable() + (file_size - remaining_bytes);
+					const auto bytes_to_read = MinValue(remaining_bytes, MAX_READ_SIZE);
+					state.stream->GrowCapacity(bytes_to_read);
 
 					idx_t actually_read;
 					if (file_handle->IsRemoteFile()) {
@@ -111,11 +115,20 @@ AsyncResult DirectFileReader::Scan(ClientContext &context, GlobalTableFunctionSt
 						data_ptr_t read_ptr;
 						actually_read = NumericCast<idx_t>(bytes_to_read);
 						auto buffer_handle = file_handle->Read(read_ptr, actually_read);
-						memcpy(content_string_ptr, read_ptr, actually_read);
+						state.stream->WriteData(read_ptr, actually_read);
 					} else {
 						// Local file: non-caching read
 						actually_read = NumericCast<idx_t>(file_handle->GetFileHandle().Read(
-						    content_string_ptr, UnsafeNumericCast<idx_t>(bytes_to_read)));
+						    state.stream->GetData() + state.stream->GetPosition(), bytes_to_read));
+						state.stream->SetPosition(state.stream->GetPosition() + actually_read);
+					}
+					AssertMaxFileSize(file.path, state.stream->GetPosition());
+
+					if (file_handle->GetFileHandle().IsPipe()) {
+						if (actually_read == 0) {
+							remaining_bytes = 0;
+						}
+						continue;
 					}
 
 					if (actually_read == 0) {
@@ -123,16 +136,17 @@ AsyncResult DirectFileReader::Scan(ClientContext &context, GlobalTableFunctionSt
 						throw IOException("Failed to read file '%s' at offset %lu, unexpected EOF", file.path,
 						                  file_size - remaining_bytes);
 					}
-					remaining_bytes -= NumericCast<int64_t>(actually_read);
+					remaining_bytes -= actually_read;
 				}
 
-				content_string.Finalize();
+				auto &file_content_vector = output.data[col_idx];
+				auto &content_string = FlatVector::GetData<string_t>(file_content_vector)[out_idx];
+				content_string = string_t(char_ptr_cast(state.stream->GetData()),
+				                          NumericCast<uint32_t>(state.stream->GetPosition()));
 
 				if (type == LogicalType::VARCHAR) {
 					VERIFY(file.path, content_string);
 				}
-
-				FlatVector::GetData<string_t>(file_content_vector)[out_idx] = content_string;
 			} break;
 			case ReadFileBindData::FILE_SIZE_COLUMN: {
 				auto &file_size_vector = output.data[col_idx];
diff --git a/src/function/table_function.cpp b/src/function/table_function.cpp
index b3835befc6..a5ac3ba6de 100644
--- a/src/function/table_function.cpp
+++ b/src/function/table_function.cpp
@@ -57,6 +57,30 @@ TableFunction::TableFunction(const vector<LogicalType> &arguments, std::nullptr_
 TableFunction::TableFunction() : TableFunction("", {}, nullptr, nullptr, nullptr, nullptr) {
 }
 
+bool TableFunction::operator==(const TableFunction &rhs) const {
+	return name == rhs.name && arguments == rhs.arguments && varargs == rhs.varargs && bind == rhs.bind &&
+	       bind_replace == rhs.bind_replace && bind_operator == rhs.bind_operator && init_global == rhs.init_global &&
+	       init_local == rhs.init_local && function == rhs.function && in_out_function == rhs.in_out_function &&
+	       in_out_function_final == rhs.in_out_function_final && statistics == rhs.statistics &&
+	       dependency == rhs.dependency && cardinality == rhs.cardinality &&
+	       pushdown_complex_filter == rhs.pushdown_complex_filter && pushdown_expression == rhs.pushdown_expression &&
+	       to_string == rhs.to_string && dynamic_to_string == rhs.dynamic_to_string &&
+	       table_scan_progress == rhs.table_scan_progress && get_partition_data == rhs.get_partition_data &&
+	       get_bind_info == rhs.get_bind_info && type_pushdown == rhs.type_pushdown &&
+	       get_multi_file_reader == rhs.get_multi_file_reader && supports_pushdown_type == rhs.supports_pushdown_type &&
+	       get_partition_info == rhs.get_partition_info && get_partition_stats == rhs.get_partition_stats &&
+	       get_virtual_columns == rhs.get_virtual_columns && get_row_id_columns == rhs.get_row_id_columns &&
+	       serialize == rhs.serialize && deserialize == rhs.deserialize &&
+	       verify_serialization == rhs.verify_serialization && projection_pushdown == rhs.projection_pushdown &&
+	       filter_pushdown == rhs.filter_pushdown && filter_prune == rhs.filter_prune &&
+	       sampling_pushdown == rhs.sampling_pushdown && late_materialization == rhs.late_materialization &&
+	       global_initialization == rhs.global_initialization;
+}
+
+bool TableFunction::operator!=(const TableFunction &rhs) const {
+	return !(*this == rhs);
+}
+
 bool TableFunction::Equal(const TableFunction &rhs) const {
 	// number of types
 	if (this->arguments.size() != rhs.arguments.size()) {
diff --git a/src/include/duckdb.h b/src/include/duckdb.h
index 65691d09cf..57561408b1 100644
--- a/src/include/duckdb.h
+++ b/src/include/duckdb.h
@@ -878,19 +878,6 @@ typedef struct _duckdb_catalog_entry {
 	void *internal_ptr;
 } * duckdb_catalog_entry;
 
-//===--------------------------------------------------------------------===//
-// Logging Types
-//===--------------------------------------------------------------------===//
-
-//! Holds a log storage object.
-typedef struct _duckdb_log_storage {
-	void *internal_ptr;
-} * duckdb_log_storage;
-
-//! This function is missing the logging context which may be added later, or in a secondary function.
-typedef void (*duckdb_logger_write_log_entry_t)(duckdb_timestamp timestamp, const char *level, const char *log_type,
-                                                const char *log_message);
-
 //===--------------------------------------------------------------------===//
 // DuckDB extension access
 //===--------------------------------------------------------------------===//
@@ -6094,45 +6081,6 @@ Note that this does not actually "drop" the catalog entry from the database cata
 */
 DUCKDB_C_API void duckdb_destroy_catalog_entry(duckdb_catalog_entry *entry);
 
-//----------------------------------------------------------------------------------------------------------------------
-// Logging
-//----------------------------------------------------------------------------------------------------------------------
-// DESCRIPTION:
-// Functions that expose the log storage and allow the configuration of a custom logger
-//----------------------------------------------------------------------------------------------------------------------
-
-/*!
-Creates a new log storage object.
-
-* @return A log storage object. Must be destroyed with `duckdb_destroy_log_storage`.
-*/
-DUCKDB_C_API duckdb_log_storage duckdb_create_log_storage();
-
-/*!
-Destroys a log storage object.
-
-* @param storage The log storage object to destroy.
-*/
-DUCKDB_C_API void duckdb_destroy_log_storage(duckdb_log_storage storage);
-
-/*!
-Sets the function to be called when a new log entry is written.
-
-* @param storage The log storage object.
-* @param function The function to call.
-*/
-DUCKDB_C_API void duckdb_log_storage_set_write_log_entry(duckdb_log_storage storage,
-                                                         duckdb_logger_write_log_entry_t function);
-
-/*!
-Sets the storage for the logger.
-
-* @param database A database object.
-* @param name The storage name.
-* @param storage The log storage object.
-*/
-DUCKDB_C_API void duckdb_register_log_storage(duckdb_database database, const char *name, duckdb_log_storage storage);
-
 #endif
 
 #ifdef __cplusplus
diff --git a/src/include/duckdb/common/arrow/appender/scalar_data.hpp b/src/include/duckdb/common/arrow/appender/scalar_data.hpp
index e28c002ee8..60fc7659ac 100644
--- a/src/include/duckdb/common/arrow/appender/scalar_data.hpp
+++ b/src/include/duckdb/common/arrow/appender/scalar_data.hpp
@@ -71,9 +71,10 @@ struct ArrowUUIDBlobConverter {
 	template <class TGT, class SRC>
 	static TGT Operation(hugeint_t input) {
 		// Turn into big-end
-		auto upper = BSwap(input.lower);
+		auto upper = BSwapIfLE(input.lower);
 		// flip Upper MSD
-		auto lower = BSwap(static_cast<int64_t>(static_cast<uint64_t>(input.upper) ^ (static_cast<uint64_t>(1) << 63)));
+		auto lower =
+		    BSwapIfLE(static_cast<int64_t>(static_cast<uint64_t>(input.upper) ^ (static_cast<uint64_t>(1) << 63)));
 		return {static_cast<int64_t>(upper), static_cast<uint64_t>(lower)};
 	}
 
diff --git a/src/include/duckdb/common/box_renderer.hpp b/src/include/duckdb/common/box_renderer.hpp
index 26c12a62e4..38129f62ef 100644
--- a/src/include/duckdb/common/box_renderer.hpp
+++ b/src/include/duckdb/common/box_renderer.hpp
@@ -141,6 +141,9 @@ public:
 	            BaseResultRenderer &ss);
 	void Print(ClientContext &context, const vector<string> &names, const ColumnDataCollection &op);
 
+	static string TryFormatLargeNumber(const string &numeric, char decimal_sep);
+	static string TruncateValue(const string &value, idx_t column_width, idx_t &pos, idx_t &current_render_width);
+
 private:
 	//! The configuration used for rendering
 	BoxRendererConfig config;
diff --git a/src/include/duckdb/common/bswap.hpp b/src/include/duckdb/common/bswap.hpp
index fbcafb8f6d..a1434da73e 100644
--- a/src/include/duckdb/common/bswap.hpp
+++ b/src/include/duckdb/common/bswap.hpp
@@ -11,8 +11,23 @@
 #include "duckdb/common/common.hpp"
 #include "duckdb/common/numeric_utils.hpp"
 
+#include <cstring>
+
 namespace duckdb {
 
+#ifndef DUCKDB_IS_BIG_ENDIAN
+#if defined(__BYTE_ORDER__) && defined(__ORDER_BIG_ENDIAN__) && __BYTE_ORDER__ == __ORDER_BIG_ENDIAN__
+#define DUCKDB_IS_BIG_ENDIAN 1
+#else
+#define DUCKDB_IS_BIG_ENDIAN 0
+#endif
+#endif
+
+#if defined(__clang__) || defined(__GNUC__) && (__GNUC__ > 4 || (__GNUC__ == 4 && __GNUC_MINOR__ >= 3))
+#define BSWAP16(x) __builtin_bswap16(static_cast<uint16_t>(x))
+#define BSWAP32(x) __builtin_bswap32(static_cast<uint32_t>(x))
+#define BSWAP64(x) __builtin_bswap64(static_cast<uint64_t>(x))
+#else
 #define BSWAP16(x) ((uint16_t)((((uint16_t)(x)&0xff00) >> 8) | (((uint16_t)(x)&0x00ff) << 8)))
 
 #define BSWAP32(x)                                                                                                     \
@@ -24,6 +39,11 @@ namespace duckdb {
 	            (((uint64_t)(x)&0x0000ff0000000000ull) >> 24) | (((uint64_t)(x)&0x000000ff00000000ull) >> 8) |         \
 	            (((uint64_t)(x)&0x00000000ff000000ull) << 8) | (((uint64_t)(x)&0x0000000000ff0000ull) << 24) |         \
 	            (((uint64_t)(x)&0x000000000000ff00ull) << 40) | (((uint64_t)(x)&0x00000000000000ffull) << 56)))
+#endif
+
+static inline int8_t BSwap(const int8_t &x) {
+	return x;
+}
 
 static inline uint8_t BSwap(const uint8_t &x) {
 	return x;
@@ -33,10 +53,18 @@ static inline uint16_t BSwap(const uint16_t &x) {
 	return BSWAP16(x);
 }
 
+static inline int16_t BSwap(const int16_t &x) {
+	return static_cast<int16_t>(BSWAP16(x));
+}
+
 static inline uint32_t BSwap(const uint32_t &x) {
 	return BSWAP32(x);
 }
 
+static inline int32_t BSwap(const int32_t &x) {
+	return static_cast<int32_t>(BSWAP32(x));
+}
+
 static inline uint64_t BSwap(const uint64_t &x) {
 	return BSWAP64(x);
 }
@@ -45,4 +73,48 @@ static inline int64_t BSwap(const int64_t &x) {
 	return static_cast<int64_t>(BSWAP64(x));
 }
 
+static inline uhugeint_t BSwap(const uhugeint_t &x) {
+	return uhugeint_t(BSWAP64(x.upper), BSWAP64(x.lower));
+}
+
+static inline hugeint_t BSwap(const hugeint_t &x) {
+	return hugeint_t(static_cast<int64_t>(BSWAP64(x.upper)), BSWAP64(x.lower));
+}
+
+static inline float BSwap(const float &x) {
+	uint32_t temp;
+	std::memcpy(&temp, &x, sizeof(temp));
+	temp = BSWAP32(temp);
+	float result;
+	std::memcpy(&result, &temp, sizeof(result));
+	return result;
+}
+
+static inline double BSwap(const double &x) {
+	uint64_t temp;
+	std::memcpy(&temp, &x, sizeof(temp));
+	temp = BSWAP64(temp);
+	double result;
+	std::memcpy(&result, &temp, sizeof(result));
+	return result;
+}
+
+template <class T>
+static inline T BSwapIfLE(const T &x) {
+#if DUCKDB_IS_BIG_ENDIAN
+	return x;
+#else
+	return BSwap(x);
+#endif
+}
+
+template <class T>
+static inline T BSwapIfBE(const T &x) {
+#if DUCKDB_IS_BIG_ENDIAN
+	return BSwap(x);
+#else
+	return x;
+#endif
+}
+
 } // namespace duckdb
diff --git a/src/include/duckdb/common/encryption_key_manager.hpp b/src/include/duckdb/common/encryption_key_manager.hpp
index 9e41ee8c13..0720edc81b 100644
--- a/src/include/duckdb/common/encryption_key_manager.hpp
+++ b/src/include/duckdb/common/encryption_key_manager.hpp
@@ -64,6 +64,8 @@ public:
 	static void KeyDerivationFunctionSHA256(data_ptr_t user_key, idx_t user_key_size, data_ptr_t salt,
 	                                        data_ptr_t derived_key);
 	static string Base64Decode(const string &key);
+
+	//! Generate a (non-cryptographically secure) random key ID
 	static string GenerateRandomKeyID();
 
 public:
diff --git a/src/include/duckdb/common/encryption_state.hpp b/src/include/duckdb/common/encryption_state.hpp
index 2563c0bdef..4aece4a2fa 100644
--- a/src/include/duckdb/common/encryption_state.hpp
+++ b/src/include/duckdb/common/encryption_state.hpp
@@ -56,6 +56,11 @@ public:
 
 	virtual ~EncryptionUtil() {
 	}
+
+	//! Whether the EncryptionUtil supports encryption (some may only support decryption)
+	DUCKDB_API virtual bool SupportsEncryption() {
+		return true;
+	}
 };
 
 } // namespace duckdb
diff --git a/src/include/duckdb/common/file_buffer.hpp b/src/include/duckdb/common/file_buffer.hpp
index f330854c2f..e0fcfcdece 100644
--- a/src/include/duckdb/common/file_buffer.hpp
+++ b/src/include/duckdb/common/file_buffer.hpp
@@ -13,7 +13,7 @@
 
 namespace duckdb {
 
-class Allocator;
+class BlockAllocator;
 class BlockManager;
 class QueryContext;
 
@@ -30,13 +30,13 @@ public:
 	//! (typically 8 bytes). On return, this->AllocSize() >= this->size >= user_size.
 	//! Our allocation size will always be page-aligned, which is necessary to support
 	//! DIRECT_IO
-	FileBuffer(Allocator &allocator, FileBufferType type, uint64_t user_size, idx_t block_header_size);
-	FileBuffer(Allocator &allocator, FileBufferType type, BlockManager &block_manager);
+	FileBuffer(BlockAllocator &allocator, FileBufferType type, uint64_t user_size, idx_t block_header_size);
+	FileBuffer(BlockAllocator &allocator, FileBufferType type, BlockManager &block_manager);
 	FileBuffer(FileBuffer &source, FileBufferType type, idx_t block_header_size);
 
 	virtual ~FileBuffer();
 
-	Allocator &allocator;
+	BlockAllocator &allocator;
 	//! The buffer that users can write to
 	data_ptr_t buffer;
 	//! The user-facing size of the buffer.
diff --git a/src/include/duckdb/common/http_util.hpp b/src/include/duckdb/common/http_util.hpp
index 51127179dd..11fc26c48e 100644
--- a/src/include/duckdb/common/http_util.hpp
+++ b/src/include/duckdb/common/http_util.hpp
@@ -11,6 +11,7 @@
 #include "duckdb/common/types.hpp"
 #include "duckdb/common/case_insensitive_map.hpp"
 #include "duckdb/common/enums/http_status_code.hpp"
+#include "duckdb/common/types/timestamp.hpp"
 #include <functional>
 
 namespace duckdb {
@@ -143,6 +144,11 @@ struct BaseRequest {
 	//! Whether or not to return failed requests (instead of throwing)
 	bool try_request = false;
 
+	// Requests will optionally contain their timings
+	bool have_request_timing = false;
+	timestamp_t request_start;
+	timestamp_t request_end;
+
 	template <class TARGET>
 	TARGET &Cast() {
 		return reinterpret_cast<TARGET &>(*this);
@@ -210,6 +216,7 @@ struct PostRequestInfo : public BaseRequest {
 class HTTPClient {
 public:
 	virtual ~HTTPClient() = default;
+	virtual void Initialize(HTTPParams &http_params) = 0;
 
 	virtual unique_ptr<HTTPResponse> Get(GetRequestInfo &info) = 0;
 	virtual unique_ptr<HTTPResponse> Put(PutRequestInfo &info) = 0;
diff --git a/src/include/duckdb/common/operator/numeric_cast.hpp b/src/include/duckdb/common/operator/numeric_cast.hpp
index 939b570ecd..d7147bc0d6 100644
--- a/src/include/duckdb/common/operator/numeric_cast.hpp
+++ b/src/include/duckdb/common/operator/numeric_cast.hpp
@@ -32,7 +32,7 @@ static bool TryCastWithOverflowCheck(SRC value, DST &result) {
 		if (NumericLimits<SRC>::IsSigned()) {
 			// signed to unsigned conversion
 			if (NumericLimits<SRC>::Digits() > NumericLimits<DST>::Digits()) {
-				if (value < 0 || value > (SRC)NumericLimits<DST>::Maximum()) {
+				if (value < 0 || value > static_cast<SRC>(NumericLimits<DST>::Maximum())) {
 					return false;
 				}
 			} else {
@@ -40,31 +40,31 @@ static bool TryCastWithOverflowCheck(SRC value, DST &result) {
 					return false;
 				}
 			}
-			result = (DST)value;
+			result = static_cast<DST>(value);
 			return true;
 		} else {
 			// unsigned to signed conversion
 			if (NumericLimits<SRC>::Digits() >= NumericLimits<DST>::Digits()) {
-				if (value <= (SRC)NumericLimits<DST>::Maximum()) {
-					result = (DST)value;
+				if (value <= static_cast<SRC>(NumericLimits<DST>::Maximum())) {
+					result = static_cast<DST>(value);
 					return true;
 				}
 				return false;
 			} else {
-				result = (DST)value;
+				result = static_cast<DST>(value);
 				return true;
 			}
 		}
 	} else {
 		// same sign conversion
 		if (NumericLimits<DST>::Digits() >= NumericLimits<SRC>::Digits()) {
-			result = (DST)value;
+			result = static_cast<DST>(value);
 			return true;
 		} else {
 			if (value < SRC(NumericLimits<DST>::Minimum()) || value > SRC(NumericLimits<DST>::Maximum())) {
 				return false;
 			}
-			result = (DST)value;
+			result = static_cast<DST>(value);
 			return true;
 		}
 	}
diff --git a/src/include/duckdb/common/pipe_file_system.hpp b/src/include/duckdb/common/pipe_file_system.hpp
index 5fb6ba1fdb..e300435a2d 100644
--- a/src/include/duckdb/common/pipe_file_system.hpp
+++ b/src/include/duckdb/common/pipe_file_system.hpp
@@ -20,6 +20,7 @@ public:
 	int64_t Write(FileHandle &handle, void *buffer, int64_t nr_bytes) override;
 
 	int64_t GetFileSize(FileHandle &handle) override;
+	timestamp_t GetLastModifiedTime(FileHandle &handle) override;
 
 	void Reset(FileHandle &handle) override;
 	bool OnDiskFile(FileHandle &handle) override {
diff --git a/src/include/duckdb/common/radix.hpp b/src/include/duckdb/common/radix.hpp
index dc22256c5e..bdf8b8f62b 100644
--- a/src/include/duckdb/common/radix.hpp
+++ b/src/include/duckdb/common/radix.hpp
@@ -24,15 +24,6 @@ namespace duckdb {
 
 struct Radix {
 public:
-	static inline bool IsLittleEndian() {
-		int n = 1;
-		if (*char_ptr_cast(&n) == 1) {
-			return true;
-		} else {
-			return false;
-		}
-	}
-
 	template <class T>
 	static inline void EncodeData(data_ptr_t dataptr, T value) {
 		throw NotImplementedException("Cannot create data from this type");
@@ -177,7 +168,7 @@ void Radix::EncodeSigned(data_ptr_t dataptr, T value) {
 	using UNSIGNED = typename MakeUnsigned<T>::type;
 	UNSIGNED bytes;
 	Store<T>(value, data_ptr_cast(&bytes));
-	Store<UNSIGNED>(BSwap(bytes), dataptr);
+	Store<UNSIGNED>(BSwapIfLE(bytes), dataptr);
 	dataptr[0] = FlipSign(dataptr[0]);
 }
 
@@ -208,17 +199,17 @@ inline void Radix::EncodeData(data_ptr_t dataptr, uint8_t value) {
 
 template <>
 inline void Radix::EncodeData(data_ptr_t dataptr, uint16_t value) {
-	Store<uint16_t>(BSwap(value), dataptr);
+	Store<uint16_t>(BSwapIfLE(value), dataptr);
 }
 
 template <>
 inline void Radix::EncodeData(data_ptr_t dataptr, uint32_t value) {
-	Store<uint32_t>(BSwap(value), dataptr);
+	Store<uint32_t>(BSwapIfLE(value), dataptr);
 }
 
 template <>
 inline void Radix::EncodeData(data_ptr_t dataptr, uint64_t value) {
-	Store<uint64_t>(BSwap(value), dataptr);
+	Store<uint64_t>(BSwapIfLE(value), dataptr);
 }
 
 template <>
@@ -236,13 +227,13 @@ inline void Radix::EncodeData(data_ptr_t dataptr, uhugeint_t value) {
 template <>
 inline void Radix::EncodeData(data_ptr_t dataptr, float value) {
 	uint32_t converted_value = EncodeFloat(value);
-	Store<uint32_t>(BSwap(converted_value), dataptr);
+	Store<uint32_t>(BSwapIfLE(converted_value), dataptr);
 }
 
 template <>
 inline void Radix::EncodeData(data_ptr_t dataptr, double value) {
 	uint64_t converted_value = EncodeDouble(value);
-	Store<uint64_t>(BSwap(converted_value), dataptr);
+	Store<uint64_t>(BSwapIfLE(converted_value), dataptr);
 }
 
 template <>
@@ -266,7 +257,7 @@ T Radix::DecodeSigned(const_data_ptr_t input) {
 	auto bytes_data = data_ptr_cast(&bytes);
 	bytes_data[0] = FlipSign(bytes_data[0]);
 	T result;
-	Store<UNSIGNED>(BSwap(bytes), data_ptr_cast(&result));
+	Store<UNSIGNED>(BSwapIfLE(bytes), data_ptr_cast(&result));
 	return result;
 }
 
@@ -297,17 +288,17 @@ inline uint8_t Radix::DecodeData(const_data_ptr_t input) {
 
 template <>
 inline uint16_t Radix::DecodeData(const_data_ptr_t input) {
-	return BSwap(Load<uint16_t>(input));
+	return BSwapIfLE(Load<uint16_t>(input));
 }
 
 template <>
 inline uint32_t Radix::DecodeData(const_data_ptr_t input) {
-	return BSwap(Load<uint32_t>(input));
+	return BSwapIfLE(Load<uint32_t>(input));
 }
 
 template <>
 inline uint64_t Radix::DecodeData(const_data_ptr_t input) {
-	return BSwap(Load<uint64_t>(input));
+	return BSwapIfLE(Load<uint64_t>(input));
 }
 
 template <>
@@ -328,12 +319,12 @@ inline uhugeint_t Radix::DecodeData(const_data_ptr_t input) {
 
 template <>
 inline float Radix::DecodeData(const_data_ptr_t input) {
-	return DecodeFloat(BSwap(Load<uint32_t>(input)));
+	return DecodeFloat(BSwapIfLE(Load<uint32_t>(input)));
 }
 
 template <>
 inline double Radix::DecodeData(const_data_ptr_t input) {
-	return DecodeDouble(BSwap(Load<uint64_t>(input)));
+	return DecodeDouble(BSwapIfLE(Load<uint64_t>(input)));
 }
 
 template <>
diff --git a/src/include/duckdb/common/random_engine.hpp b/src/include/duckdb/common/random_engine.hpp
index 8a5a3097e4..ec14b42e55 100644
--- a/src/include/duckdb/common/random_engine.hpp
+++ b/src/include/duckdb/common/random_engine.hpp
@@ -38,6 +38,8 @@ public:
 
 	void SetSeed(uint64_t seed);
 
+	void RandomData(duckdb::data_ptr_t data, duckdb::idx_t len);
+
 	static RandomEngine &Get(ClientContext &context);
 
 	mutex lock;
diff --git a/src/include/duckdb/common/serializer/memory_stream.hpp b/src/include/duckdb/common/serializer/memory_stream.hpp
index c53bd3004d..b08b344719 100644
--- a/src/include/duckdb/common/serializer/memory_stream.hpp
+++ b/src/include/duckdb/common/serializer/memory_stream.hpp
@@ -50,6 +50,7 @@ public:
 	//! Write data to the stream.
 	//! Throws if the write would exceed the capacity of the stream and the backing buffer is not owned by the stream
 	void WriteData(const_data_ptr_t buffer, idx_t write_size) override;
+	void GrowCapacity(idx_t write_size);
 
 	//! Read data from the stream.
 	//! Throws if the read would exceed the capacity of the stream
diff --git a/src/include/duckdb/common/sorting/sort_key.hpp b/src/include/duckdb/common/sorting/sort_key.hpp
index ccabcf21b0..68d7594be9 100644
--- a/src/include/duckdb/common/sorting/sort_key.hpp
+++ b/src/include/duckdb/common/sorting/sort_key.hpp
@@ -102,7 +102,7 @@ public:
 	void ByteSwap() {
 		auto &sort_key = static_cast<SORT_KEY &>(*this);
 		for (idx_t i = 0; i < SORT_KEY::PARTS; i++) {
-			(&sort_key.part0)[i] = BSwap((&sort_key.part0)[i]);
+			(&sort_key.part0)[i] = BSwapIfLE((&sort_key.part0)[i]);
 		}
 	}
 
@@ -172,7 +172,7 @@ public:
 	void ByteSwap() {
 		auto &sort_key = static_cast<SORT_KEY &>(*this);
 		for (idx_t i = 0; i < SORT_KEY::PARTS; i++) {
-			(&sort_key.part0)[i] = BSwap((&sort_key.part0)[i]);
+			(&sort_key.part0)[i] = BSwapIfLE((&sort_key.part0)[i]);
 		}
 	}
 
diff --git a/src/include/duckdb/common/types/bit.hpp b/src/include/duckdb/common/types/bit.hpp
index cbf599139e..c1d17095d8 100644
--- a/src/include/duckdb/common/types/bit.hpp
+++ b/src/include/duckdb/common/types/bit.hpp
@@ -101,8 +101,9 @@ template <class T>
 void Bit::NumericToBit(T numeric, bitstring_t &output_str) {
 	D_ASSERT(output_str.GetSize() >= sizeof(T) + 1);
 
+	auto le_numeric = BSwapIfBE(numeric);
 	auto output = output_str.GetDataWriteable();
-	auto data = const_data_ptr_cast(&numeric);
+	auto data = const_data_ptr_cast(&le_numeric);
 
 	*output = 0; // set padding to 0
 	++output;
@@ -141,6 +142,7 @@ void Bit::BitToNumeric(bitstring_t bit, T &output_num) {
 	for (idx_t idx = padded_byte_idx + 1; idx < sizeof(T); ++idx) {
 		output[sizeof(T) - 1 - idx] = data[1 + idx - padded_byte_idx];
 	}
+	output_num = BSwapIfBE(output_num);
 }
 
 } // namespace duckdb
diff --git a/src/include/duckdb/common/types/row/tuple_data_collection.hpp b/src/include/duckdb/common/types/row/tuple_data_collection.hpp
index 1f98a3fe70..981d6a6404 100644
--- a/src/include/duckdb/common/types/row/tuple_data_collection.hpp
+++ b/src/include/duckdb/common/types/row/tuple_data_collection.hpp
@@ -196,6 +196,8 @@ public:
 	bool Scan(TupleDataParallelScanState &gstate, TupleDataLocalScanState &lstate, DataChunk &result);
 	//! Whether the last scan has been completed on this TupleDataCollection
 	bool ScanComplete(const TupleDataScanState &state) const;
+	//! Seeks to the specified chunk index, returning the total row count before it
+	idx_t Seek(TupleDataScanState &state, const idx_t target_chunk);
 
 	//! Gathers a DataChunk from the TupleDataCollection, given the specific row locations (requires full pin)
 	void Gather(Vector &row_locations, const SelectionVector &scan_sel, const idx_t scan_count, DataChunk &result,
diff --git a/src/include/duckdb/common/types/string_type.hpp b/src/include/duckdb/common/types/string_type.hpp
index 59bb3c2930..2d7bbcf56c 100644
--- a/src/include/duckdb/common/types/string_type.hpp
+++ b/src/include/duckdb/common/types/string_type.hpp
@@ -9,6 +9,7 @@
 #pragma once
 
 #include "duckdb/common/assert.hpp"
+#include "duckdb/common/bswap.hpp"
 #include "duckdb/common/constants.hpp"
 #include "duckdb/common/helper.hpp"
 #include "duckdb/common/numeric_utils.hpp"
@@ -194,22 +195,14 @@ public:
 			uint32_t a_prefix = Load<uint32_t>(const_data_ptr_cast(left.GetPrefix()));
 			uint32_t b_prefix = Load<uint32_t>(const_data_ptr_cast(right.GetPrefix()));
 
-			// Utility to move 0xa1b2c3d4 into 0xd4c3b2a1, basically inverting the order byte-a-byte
-			auto byte_swap = [](uint32_t v) -> uint32_t {
-				uint32_t t1 = (v >> 16u) | (v << 16u);
-				uint32_t t2 = t1 & 0x00ff00ff;
-				uint32_t t3 = t1 & 0xff00ff00;
-				return (t2 << 8u) | (t3 >> 8u);
-			};
-
 			// Check on prefix -----
-			// We dont' need to mask since:
+			// We don't need to mask since:
 			//	if the prefix is greater(after bswap), it will stay greater regardless of the extra bytes
 			// 	if the prefix is smaller(after bswap), it will stay smaller regardless of the extra bytes
 			//	if the prefix is equal, the extra bytes are guaranteed to be /0 for the shorter one
 
 			if (a_prefix != b_prefix) {
-				return byte_swap(a_prefix) > byte_swap(b_prefix);
+				return BSwapIfLE(a_prefix) > BSwapIfLE(b_prefix);
 			}
 #endif
 			auto memcmp_res = memcmp(left.GetData(), right.GetData(), min_length);
diff --git a/src/include/duckdb/common/unique_ptr.hpp b/src/include/duckdb/common/unique_ptr.hpp
index f5d0972d7a..f4bdc7b63c 100644
--- a/src/include/duckdb/common/unique_ptr.hpp
+++ b/src/include/duckdb/common/unique_ptr.hpp
@@ -1,3 +1,4 @@
+
 #pragma once
 
 #include "duckdb/common/exception.hpp"
diff --git a/src/include/duckdb/execution/join_hashtable.hpp b/src/include/duckdb/execution/join_hashtable.hpp
index 4d0e6ae47d..0219c6c3ca 100644
--- a/src/include/duckdb/execution/join_hashtable.hpp
+++ b/src/include/duckdb/execution/join_hashtable.hpp
@@ -279,6 +279,8 @@ public:
 	bool single_join_error_on_multiple_rows = true;
 	//! Whether or not to perform deduplication based on join_keys when building ht
 	bool insert_duplicate_keys = true;
+	//! Number of probe matches
+	atomic<idx_t> total_probe_matches {0};
 
 	struct {
 		mutex mj_lock;
diff --git a/src/include/duckdb/function/compression_function.hpp b/src/include/duckdb/function/compression_function.hpp
index 97fff72b1f..68402d5c64 100644
--- a/src/include/duckdb/function/compression_function.hpp
+++ b/src/include/duckdb/function/compression_function.hpp
@@ -206,7 +206,7 @@ typedef unique_ptr<CompressionAppendState> (*compression_init_append_t)(ColumnSe
 typedef idx_t (*compression_append_t)(CompressionAppendState &append_state, ColumnSegment &segment,
                                       SegmentStatistics &stats, UnifiedVectorFormat &data, idx_t offset, idx_t count);
 typedef idx_t (*compression_finalize_append_t)(ColumnSegment &segment, SegmentStatistics &stats);
-typedef void (*compression_revert_append_t)(ColumnSegment &segment, idx_t start_row);
+typedef void (*compression_revert_append_t)(ColumnSegment &segment, idx_t new_count);
 
 //===--------------------------------------------------------------------===//
 // Serialization (optional)
diff --git a/src/include/duckdb/function/table/arrow.hpp b/src/include/duckdb/function/table/arrow.hpp
index a596c86e9d..c758f2ed5d 100644
--- a/src/include/duckdb/function/table/arrow.hpp
+++ b/src/include/duckdb/function/table/arrow.hpp
@@ -212,7 +212,7 @@ public:
 	                                                  vector<LogicalType> &return_types, vector<string> &names);
 	//! Actual conversion from Arrow to DuckDB
 	static void ArrowToDuckDB(ArrowScanLocalState &scan_state, const arrow_column_map_t &arrow_convert_data,
-	                          DataChunk &output, idx_t start, bool arrow_scan_is_projected = true,
+	                          DataChunk &output, bool arrow_scan_is_projected = true,
 	                          idx_t rowid_column_index = COLUMN_IDENTIFIER_ROW_ID);
 
 	//! Get next scan state
diff --git a/src/include/duckdb/function/table/read_file.hpp b/src/include/duckdb/function/table/read_file.hpp
index a0ef222fea..e0d2a51c26 100644
--- a/src/include/duckdb/function/table/read_file.hpp
+++ b/src/include/duckdb/function/table/read_file.hpp
@@ -9,6 +9,7 @@
 #pragma once
 
 #include "duckdb/common/multi_file/multi_file_function.hpp"
+#include "duckdb/common/serializer/memory_stream.hpp"
 #include "utf8proc_wrapper.hpp"
 
 namespace duckdb {
@@ -29,6 +30,8 @@ struct ReadFileGlobalState : public GlobalTableFunctionState {
 	shared_ptr<MultiFileList> file_list;
 	vector<idx_t> column_ids;
 	bool requires_file_open = false;
+
+	unique_ptr<MemoryStream> stream;
 };
 
 } // namespace duckdb
diff --git a/src/include/duckdb/function/table_function.hpp b/src/include/duckdb/function/table_function.hpp
index 4f67d2a7e7..5d480a1493 100644
--- a/src/include/duckdb/function/table_function.hpp
+++ b/src/include/duckdb/function/table_function.hpp
@@ -466,6 +466,8 @@ public:
 	TableFunctionInitialization global_initialization = TableFunctionInitialization::INITIALIZE_ON_EXECUTE;
 
 	DUCKDB_API bool Equal(const TableFunction &rhs) const;
+	DUCKDB_API bool operator==(const TableFunction &rhs) const;
+	DUCKDB_API bool operator!=(const TableFunction &rhs) const;
 };
 
 } // namespace duckdb
diff --git a/src/include/duckdb/logging/log_manager.hpp b/src/include/duckdb/logging/log_manager.hpp
index a749adc010..54f623a55e 100644
--- a/src/include/duckdb/logging/log_manager.hpp
+++ b/src/include/duckdb/logging/log_manager.hpp
@@ -25,7 +25,6 @@ class LogManager {
 	friend class ThreadSafeLogger;
 	friend class ThreadLocalLogger;
 	friend class MutableLogger;
-	friend class CallbackLogger;
 
 public:
 	// Note: two step initialization because Logger needs shared pointer to log manager TODO: can we clean up?
diff --git a/src/include/duckdb/logging/log_type.hpp b/src/include/duckdb/logging/log_type.hpp
index 7ce97e5abc..c2fefc68ca 100644
--- a/src/include/duckdb/logging/log_type.hpp
+++ b/src/include/duckdb/logging/log_type.hpp
@@ -135,7 +135,7 @@ public:
 	                                  idx_t merge_count, idx_t target_count, idx_t merge_rows, idx_t row_start);
 	//! Checkpoint
 	static string ConstructLogMessage(const AttachedDatabase &db, DataTableInfo &table, idx_t segment_idx,
-	                                  RowGroup &row_group);
+	                                  RowGroup &row_group, idx_t row_group_start);
 
 private:
 	static string CreateLog(const AttachedDatabase &db, DataTableInfo &table, const char *op, vector<Value> map_keys,
diff --git a/src/include/duckdb/logging/logger.hpp b/src/include/duckdb/logging/logger.hpp
index bb46dbcb5a..048588958a 100644
--- a/src/include/duckdb/logging/logger.hpp
+++ b/src/include/duckdb/logging/logger.hpp
@@ -8,8 +8,6 @@
 
 #pragma once
 
-#include <duckdb/common/types/timestamp.hpp>
-
 #include "duckdb/logging/logging.hpp"
 #include "duckdb/logging/log_type.hpp"
 #include "duckdb/common/types.hpp"
diff --git a/src/include/duckdb/main/appender.hpp b/src/include/duckdb/main/appender.hpp
index fe8b3bc685..b637717f86 100644
--- a/src/include/duckdb/main/appender.hpp
+++ b/src/include/duckdb/main/appender.hpp
@@ -48,6 +48,8 @@ protected:
 	AppenderType appender_type;
 	//! The amount of rows after which the appender flushes automatically.
 	idx_t flush_count = DEFAULT_FLUSH_COUNT;
+	//! Peak allocation threshold at which to flush the allocator when appender flushs chunk.
+	optional_idx flush_memory_threshold;
 
 protected:
 	DUCKDB_API BaseAppender(Allocator &allocator, const AppenderType type);
@@ -107,6 +109,9 @@ protected:
 	void InitializeChunk();
 	void FlushChunk();
 
+	bool ShouldFlushChunk() const;
+	bool ShouldFlush() const;
+
 	template <class T>
 	void AppendValueInternal(T value);
 	template <class SRC, class DST>
@@ -131,9 +136,11 @@ protected:
 class Appender : public BaseAppender {
 public:
 	DUCKDB_API Appender(Connection &con, const string &database_name, const string &schema_name,
-	                    const string &table_name);
-	DUCKDB_API Appender(Connection &con, const string &schema_name, const string &table_name);
-	DUCKDB_API Appender(Connection &con, const string &table_name);
+	                    const string &table_name, const idx_t flush_memory_threshold = DConstants::INVALID_INDEX);
+	DUCKDB_API Appender(Connection &con, const string &schema_name, const string &table_name,
+	                    const idx_t flush_memory_threshold = DConstants::INVALID_INDEX);
+	DUCKDB_API Appender(Connection &con, const string &table_name,
+	                    const idx_t flush_memory_threshold = DConstants::INVALID_INDEX);
 	DUCKDB_API ~Appender() override;
 
 public:
@@ -162,7 +169,8 @@ protected:
 class QueryAppender : public BaseAppender {
 public:
 	DUCKDB_API QueryAppender(Connection &con, string query, vector<LogicalType> types,
-	                         vector<string> names = vector<string>(), string table_name = string());
+	                         vector<string> names = vector<string>(), string table_name = string(),
+	                         const idx_t flush_memory_threshold = DConstants::INVALID_INDEX);
 	DUCKDB_API ~QueryAppender() override;
 
 private:
@@ -187,7 +195,8 @@ class InternalAppender : public BaseAppender {
 
 public:
 	DUCKDB_API InternalAppender(ClientContext &context, TableCatalogEntry &table,
-	                            const idx_t flush_count = DEFAULT_FLUSH_COUNT);
+	                            const idx_t flush_count = DEFAULT_FLUSH_COUNT,
+	                            const idx_t flush_memory_threshold = DConstants::INVALID_INDEX);
 	DUCKDB_API ~InternalAppender() override;
 
 protected:
diff --git a/src/include/duckdb/main/capi/cast/from_decimal.hpp b/src/include/duckdb/main/capi/cast/from_decimal.hpp
index 2a9bf7644b..34fdcf3675 100644
--- a/src/include/duckdb/main/capi/cast/from_decimal.hpp
+++ b/src/include/duckdb/main/capi/cast/from_decimal.hpp
@@ -20,22 +20,21 @@ bool CastDecimalCInternal(duckdb_result *source, RESULT_TYPE &result, idx_t col,
 	auto &source_type = query_result->types[col];
 	auto width = duckdb::DecimalType::GetWidth(source_type);
 	auto scale = duckdb::DecimalType::GetScale(source_type);
-	void *source_address = UnsafeFetchPtr<hugeint_t>(source, col, row);
-
+	auto source_value = UnsafeFetch<hugeint_t>(source, col, row);
 	CastParameters parameters;
 	switch (source_type.InternalType()) {
 	case duckdb::PhysicalType::INT16:
-		return duckdb::TryCastFromDecimal::Operation<int16_t, RESULT_TYPE>(UnsafeFetchFromPtr<int16_t>(source_address),
-		                                                                   result, parameters, width, scale);
+		return duckdb::TryCastFromDecimal::Operation<int16_t, RESULT_TYPE>(static_cast<int16_t>(source_value), result,
+		                                                                   parameters, width, scale);
 	case duckdb::PhysicalType::INT32:
-		return duckdb::TryCastFromDecimal::Operation<int32_t, RESULT_TYPE>(UnsafeFetchFromPtr<int32_t>(source_address),
-		                                                                   result, parameters, width, scale);
+		return duckdb::TryCastFromDecimal::Operation<int32_t, RESULT_TYPE>(static_cast<int32_t>(source_value), result,
+		                                                                   parameters, width, scale);
 	case duckdb::PhysicalType::INT64:
-		return duckdb::TryCastFromDecimal::Operation<int64_t, RESULT_TYPE>(UnsafeFetchFromPtr<int64_t>(source_address),
-		                                                                   result, parameters, width, scale);
+		return duckdb::TryCastFromDecimal::Operation<int64_t, RESULT_TYPE>(static_cast<int64_t>(source_value), result,
+		                                                                   parameters, width, scale);
 	case duckdb::PhysicalType::INT128:
-		return duckdb::TryCastFromDecimal::Operation<hugeint_t, RESULT_TYPE>(
-		    UnsafeFetchFromPtr<hugeint_t>(source_address), result, parameters, width, scale);
+		return duckdb::TryCastFromDecimal::Operation<hugeint_t, RESULT_TYPE>(source_value, result, parameters, width,
+		                                                                     scale);
 	default:
 		throw duckdb::InternalException("Unimplemented internal type for decimal");
 	}
diff --git a/src/include/duckdb/main/capi/extension_api.hpp b/src/include/duckdb/main/capi/extension_api.hpp
index 9fd9e060ec..a2caad4252 100644
--- a/src/include/duckdb/main/capi/extension_api.hpp
+++ b/src/include/duckdb/main/capi/extension_api.hpp
@@ -592,13 +592,6 @@ typedef struct {
 	int64_t (*duckdb_file_handle_tell)(duckdb_file_handle file_handle);
 	duckdb_state (*duckdb_file_handle_sync)(duckdb_file_handle file_handle);
 	int64_t (*duckdb_file_handle_size)(duckdb_file_handle file_handle);
-	// API to register custom log storage.
-
-	duckdb_log_storage (*duckdb_create_log_storage)();
-	void (*duckdb_destroy_log_storage)(duckdb_log_storage storage);
-	void (*duckdb_log_storage_set_write_log_entry)(duckdb_log_storage storage,
-	                                               duckdb_logger_write_log_entry_t function);
-	void (*duckdb_register_log_storage)(duckdb_database database, const char *name, duckdb_log_storage storage);
 	// New functions around the client context
 
 	idx_t (*duckdb_client_context_get_connection_id)(duckdb_client_context context);
@@ -1159,10 +1152,6 @@ inline duckdb_ext_api_v1 CreateAPIv1() {
 	result.duckdb_file_handle_tell = duckdb_file_handle_tell;
 	result.duckdb_file_handle_sync = duckdb_file_handle_sync;
 	result.duckdb_file_handle_size = duckdb_file_handle_size;
-	result.duckdb_create_log_storage = duckdb_create_log_storage;
-	result.duckdb_destroy_log_storage = duckdb_destroy_log_storage;
-	result.duckdb_log_storage_set_write_log_entry = duckdb_log_storage_set_write_log_entry;
-	result.duckdb_register_log_storage = duckdb_register_log_storage;
 	result.duckdb_client_context_get_connection_id = duckdb_client_context_get_connection_id;
 	result.duckdb_destroy_client_context = duckdb_destroy_client_context;
 	result.duckdb_connection_get_client_context = duckdb_connection_get_client_context;
diff --git a/src/include/duckdb/main/capi/header_generation/apis/v1/unstable/new_logger_functions.json b/src/include/duckdb/main/capi/header_generation/apis/v1/unstable/new_logger_functions.json
deleted file mode 100644
index d9c5f931e1..0000000000
--- a/src/include/duckdb/main/capi/header_generation/apis/v1/unstable/new_logger_functions.json
+++ /dev/null
@@ -1,10 +0,0 @@
-{
-  "version": "unstable_new_logger_functions",
-  "description": "API to register custom log storage.",
-  "entries": [
-    "duckdb_create_log_storage",
-    "duckdb_destroy_log_storage",
-    "duckdb_log_storage_set_write_log_entry",
-    "duckdb_register_log_storage"
-  ]
-}
diff --git a/src/include/duckdb/main/capi/header_generation/functions/logging.json b/src/include/duckdb/main/capi/header_generation/functions/logging.json
deleted file mode 100644
index ab7c21a36b..0000000000
--- a/src/include/duckdb/main/capi/header_generation/functions/logging.json
+++ /dev/null
@@ -1,78 +0,0 @@
-{
-  "group": "logging",
-  "deprecated": false,
-  "description": "Functions that expose the log storage and allow the configuration of a custom logger",
-  "entries": [
-    {
-      "name": "duckdb_create_log_storage",
-      "return_type": "duckdb_log_storage",
-      "comment": {
-        "description": "Creates a new log storage object.\n\n",
-        "return_value": "A log storage object. Must be destroyed with `duckdb_destroy_log_storage`."
-      }
-    },
-    {
-      "name": "duckdb_destroy_log_storage",
-      "return_type": "void",
-      "params": [
-        {
-          "type": "duckdb_log_storage",
-          "name": "storage"
-        }
-      ],
-      "comment": {
-        "description": "Destroys a log storage object.\n\n",
-        "param_comments": {
-          "storage": "The log storage object to destroy."
-        }
-      }
-    },
-    {
-      "name": "duckdb_log_storage_set_write_log_entry",
-      "return_type": "void",
-      "params": [
-        {
-          "type": "duckdb_log_storage",
-          "name": "storage"
-        },
-        {
-          "type": "duckdb_logger_write_log_entry_t",
-          "name": "function"
-        }
-      ],
-      "comment": {
-        "description": "Sets the function to be called when a new log entry is written.\n\n",
-        "param_comments": {
-          "storage": "The log storage object.",
-          "function": "The function to call."
-        }
-      }
-    },
-    {
-      "name": "duckdb_register_log_storage",
-      "return_type": "void",
-      "params": [
-        {
-          "type": "duckdb_database",
-          "name": "database"
-        },
-        {
-          "type": "const char *",
-          "name": "name"
-        },
-        {
-          "type": "duckdb_log_storage",
-          "name": "storage"
-        }
-      ],
-      "comment": {
-        "description": "Sets the storage for the logger.\n\n",
-        "param_comments": {
-          "database": "A database object.",
-          "name": "The storage name.",
-          "storage": "The log storage object."
-        }
-      }
-    }
-  ]
-}
\ No newline at end of file
diff --git a/src/include/duckdb/main/capi/header_generation/header_base.hpp.template b/src/include/duckdb/main/capi/header_generation/header_base.hpp.template
index bf25684340..c6fc4d80d9 100644
--- a/src/include/duckdb/main/capi/header_generation/header_base.hpp.template
+++ b/src/include/duckdb/main/capi/header_generation/header_base.hpp.template
@@ -878,18 +878,6 @@ typedef struct _duckdb_catalog_entry {
 	void *internal_ptr;
 } * duckdb_catalog_entry;
 
-//===--------------------------------------------------------------------===//
-// Logging Types
-//===--------------------------------------------------------------------===//
-
-//! Holds a log storage object.
-typedef struct _duckdb_log_storage {
-	void *internal_ptr;
-} * duckdb_log_storage;
-
-//! This function is missing the logging context which may be added later, or in a secondary function.
-typedef void (*duckdb_logger_write_log_entry_t)(duckdb_timestamp timestamp, const char *level, const char *log_type, const char *log_message);
-
 //===--------------------------------------------------------------------===//
 // DuckDB extension access
 //===--------------------------------------------------------------------===//
diff --git a/src/include/duckdb/main/config.hpp b/src/include/duckdb/main/config.hpp
index 3cfd12dbb4..a2fcc9edf2 100644
--- a/src/include/duckdb/main/config.hpp
+++ b/src/include/duckdb/main/config.hpp
@@ -40,6 +40,7 @@
 
 namespace duckdb {
 
+class BlockAllocator;
 class BufferManager;
 class BufferPool;
 class CastFunctionSet;
@@ -219,6 +220,8 @@ struct DBConfigOptions {
 #endif
 	//! Whether to pin threads to cores (linux only, default AUTOMATIC: on when there are more than 64 cores)
 	ThreadPinMode pin_threads = ThreadPinMode::AUTO;
+	//! Physical memory that the block allocator is allowed to use (this memory is never freed and cannot be reduced)
+	idx_t block_allocator_size = 0;
 
 	bool operator==(const DBConfigOptions &other) const;
 };
@@ -246,6 +249,8 @@ public:
 	unique_ptr<SecretManager> secret_manager;
 	//! The allocator used by the system
 	unique_ptr<Allocator> allocator;
+	//! The block allocator used by the system
+	unique_ptr<BlockAllocator> block_allocator;
 	//! Database configuration options
 	DBConfigOptions options;
 	//! Extensions made to the parser
diff --git a/src/include/duckdb/main/database.hpp b/src/include/duckdb/main/database.hpp
index bf2a57cbe8..7ecb0bc49e 100644
--- a/src/include/duckdb/main/database.hpp
+++ b/src/include/duckdb/main/database.hpp
@@ -72,7 +72,7 @@ public:
 
 	DUCKDB_API SettingLookupResult TryGetCurrentSetting(const string &key, Value &result) const;
 
-	DUCKDB_API shared_ptr<EncryptionUtil> GetEncryptionUtil() const;
+	DUCKDB_API shared_ptr<EncryptionUtil> GetEncryptionUtil();
 
 	shared_ptr<AttachedDatabase> CreateAttachedDatabase(ClientContext &context, AttachInfo &info,
 	                                                    AttachOptions &options);
diff --git a/src/include/duckdb/main/query_result.hpp b/src/include/duckdb/main/query_result.hpp
index 5468e0dc73..305acc35eb 100644
--- a/src/include/duckdb/main/query_result.hpp
+++ b/src/include/duckdb/main/query_result.hpp
@@ -148,6 +148,9 @@ private:
 		T GetValue(idx_t col_idx) const {
 			return iterator.chunk->GetValue(col_idx, row).GetValue<T>();
 		}
+		Value GetBaseValue(idx_t col_idx) const {
+			return iterator.chunk->GetValue(col_idx, row);
+		}
 	};
 	//! The row-based query result iterator. Invoking the
 	class QueryResultIterator {
diff --git a/src/include/duckdb/main/relation.hpp b/src/include/duckdb/main/relation.hpp
index bc383ffe04..94450b0be0 100644
--- a/src/include/duckdb/main/relation.hpp
+++ b/src/include/duckdb/main/relation.hpp
@@ -162,19 +162,27 @@ public:
 
 	//! Insert the data from this relation into a table
 	DUCKDB_API shared_ptr<Relation> InsertRel(const string &schema_name, const string &table_name);
+	DUCKDB_API shared_ptr<Relation> InsertRel(const string &catalog_name, const string &schema_name,
+	                                          const string &table_name);
 	DUCKDB_API void Insert(const string &table_name);
 	DUCKDB_API void Insert(const string &schema_name, const string &table_name);
+	DUCKDB_API void Insert(const string &catalog_name, const string &schema_name, const string &table_name);
 	//! Insert a row (i.e.,list of values) into a table
-	DUCKDB_API void Insert(const vector<vector<Value>> &values);
-	DUCKDB_API void Insert(vector<vector<unique_ptr<ParsedExpression>>> &&expressions);
+	DUCKDB_API virtual void Insert(const vector<vector<Value>> &values);
+	DUCKDB_API virtual void Insert(vector<vector<unique_ptr<ParsedExpression>>> &&expressions);
 	//! Create a table and insert the data from this relation into that table
 	DUCKDB_API shared_ptr<Relation> CreateRel(const string &schema_name, const string &table_name,
 	                                          bool temporary = false,
 	                                          OnCreateConflict on_conflict = OnCreateConflict::ERROR_ON_CONFLICT);
+	DUCKDB_API shared_ptr<Relation> CreateRel(const string &catalog_name, const string &schema_name,
+	                                          const string &table_name, bool temporary = false,
+	                                          OnCreateConflict on_conflict = OnCreateConflict::ERROR_ON_CONFLICT);
 	DUCKDB_API void Create(const string &table_name, bool temporary = false,
 	                       OnCreateConflict on_conflict = OnCreateConflict::ERROR_ON_CONFLICT);
 	DUCKDB_API void Create(const string &schema_name, const string &table_name, bool temporary = false,
 	                       OnCreateConflict on_conflict = OnCreateConflict::ERROR_ON_CONFLICT);
+	DUCKDB_API void Create(const string &catalog_name, const string &schema_name, const string &table_name,
+	                       bool temporary = false, OnCreateConflict on_conflict = OnCreateConflict::ERROR_ON_CONFLICT);
 
 	//! Write a relation to a CSV file
 	DUCKDB_API shared_ptr<Relation>
diff --git a/src/include/duckdb/main/relation/create_table_relation.hpp b/src/include/duckdb/main/relation/create_table_relation.hpp
index 8df59b8d25..cfc0e243ae 100644
--- a/src/include/duckdb/main/relation/create_table_relation.hpp
+++ b/src/include/duckdb/main/relation/create_table_relation.hpp
@@ -16,8 +16,11 @@ class CreateTableRelation : public Relation {
 public:
 	CreateTableRelation(shared_ptr<Relation> child, string schema_name, string table_name, bool temporary,
 	                    OnCreateConflict on_conflict);
+	CreateTableRelation(shared_ptr<Relation> child, string catalog_name, string schema_name, string table_name,
+	                    bool temporary, OnCreateConflict on_conflict);
 
 	shared_ptr<Relation> child;
+	string catalog_name;
 	string schema_name;
 	string table_name;
 	vector<ColumnDefinition> columns;
diff --git a/src/include/duckdb/main/relation/insert_relation.hpp b/src/include/duckdb/main/relation/insert_relation.hpp
index fccb0ae929..41756488fc 100644
--- a/src/include/duckdb/main/relation/insert_relation.hpp
+++ b/src/include/duckdb/main/relation/insert_relation.hpp
@@ -15,8 +15,10 @@ namespace duckdb {
 class InsertRelation : public Relation {
 public:
 	InsertRelation(shared_ptr<Relation> child, string schema_name, string table_name);
+	InsertRelation(shared_ptr<Relation> child, string catalog_name, string schema_name, string table_name);
 
 	shared_ptr<Relation> child;
+	string catalog_name;
 	string schema_name;
 	string table_name;
 	vector<ColumnDefinition> columns;
diff --git a/src/include/duckdb/main/relation/table_relation.hpp b/src/include/duckdb/main/relation/table_relation.hpp
index 9c2fddcecf..a3184dafa9 100644
--- a/src/include/duckdb/main/relation/table_relation.hpp
+++ b/src/include/duckdb/main/relation/table_relation.hpp
@@ -29,6 +29,8 @@ public:
 
 	unique_ptr<TableRef> GetTableRef() override;
 
+	void Insert(const vector<vector<Value>> &values) override;
+	void Insert(vector<vector<unique_ptr<ParsedExpression>>> &&expressions) override;
 	void Update(const string &update, const string &condition = string()) override;
 	void Update(vector<string> column_names, vector<unique_ptr<ParsedExpression>> &&update,
 	            unique_ptr<ParsedExpression> condition = nullptr) override;
diff --git a/src/include/duckdb/main/settings.hpp b/src/include/duckdb/main/settings.hpp
index 217a4fb852..bbb9909e5f 100644
--- a/src/include/duckdb/main/settings.hpp
+++ b/src/include/duckdb/main/settings.hpp
@@ -249,6 +249,17 @@ struct AutoloadKnownExtensionsSetting {
 	static Value GetSetting(const ClientContext &context);
 };
 
+struct BlockAllocatorMemorySetting {
+	using RETURN_TYPE = string;
+	static constexpr const char *Name = "block_allocator_memory";
+	static constexpr const char *Description = "Physical memory that the block allocator is allowed to use (this "
+	                                           "memory is never freed and cannot be reduced).";
+	static constexpr const char *InputType = "VARCHAR";
+	static void SetGlobal(DatabaseInstance *db, DBConfig &config, const Value &parameter);
+	static void ResetGlobal(DatabaseInstance *db, DBConfig &config);
+	static Value GetSetting(const ClientContext &context);
+};
+
 struct CatalogErrorMaxSchemasSetting {
 	using RETURN_TYPE = idx_t;
 	static constexpr const char *Name = "catalog_error_max_schemas";
diff --git a/src/include/duckdb/storage/block.hpp b/src/include/duckdb/storage/block.hpp
index 12fd7f8187..794fd6ff54 100644
--- a/src/include/duckdb/storage/block.hpp
+++ b/src/include/duckdb/storage/block.hpp
@@ -14,14 +14,15 @@
 
 namespace duckdb {
 
+class BlockAllocator;
 class Serializer;
 class Deserializer;
 
 class Block : public FileBuffer {
 public:
-	Block(Allocator &allocator, const block_id_t id, const idx_t block_size, const idx_t block_header_size);
-	Block(Allocator &allocator, block_id_t id, uint32_t internal_size, idx_t block_header_size);
-	Block(Allocator &allocator, const block_id_t id, BlockManager &block_manager);
+	Block(BlockAllocator &allocator, const block_id_t id, const idx_t block_size, const idx_t block_header_size);
+	Block(BlockAllocator &allocator, block_id_t id, uint32_t internal_size, idx_t block_header_size);
+	Block(BlockAllocator &allocator, const block_id_t id, BlockManager &block_manager);
 	Block(FileBuffer &source, block_id_t id, idx_t block_header_size);
 
 	block_id_t id;
diff --git a/src/include/duckdb/storage/block_allocator.hpp b/src/include/duckdb/storage/block_allocator.hpp
new file mode 100644
index 0000000000..1fe2987522
--- /dev/null
+++ b/src/include/duckdb/storage/block_allocator.hpp
@@ -0,0 +1,91 @@
+//===----------------------------------------------------------------------===//
+//                         DuckDB
+//
+// duckdb/storage/block_allocator.hpp
+//
+//
+//===----------------------------------------------------------------------===//
+
+#pragma once
+
+#include "duckdb/common/atomic.hpp"
+#include "duckdb/common/common.hpp"
+#include "duckdb/common/mutex.hpp"
+#include "duckdb/common/optional_idx.hpp"
+
+namespace duckdb {
+
+class Allocator;
+class AttachedDatabase;
+class DatabaseInstance;
+class BlockAllocatorThreadLocalState;
+struct BlockQueue;
+
+class BlockAllocator {
+	friend class BlockAllocatorThreadLocalState;
+
+public:
+	BlockAllocator(Allocator &allocator, idx_t block_size, idx_t virtual_memory_size, idx_t physical_memory_size);
+	~BlockAllocator();
+
+public:
+	static BlockAllocator &Get(DatabaseInstance &db);
+	static BlockAllocator &Get(AttachedDatabase &db);
+
+	//! Resize physical memory (can only be increased)
+	void Resize(idx_t new_physical_memory_size);
+
+	//! Allocation functions (same API as Allocator)
+	data_ptr_t AllocateData(idx_t size) const;
+	void FreeData(data_ptr_t pointer, idx_t size) const;
+	data_ptr_t ReallocateData(data_ptr_t pointer, idx_t old_size, idx_t new_size) const;
+
+	//! Flush outstanding allocations
+	bool SupportsFlush() const;
+	void ThreadFlush(bool allocator_background_threads, idx_t threshold, idx_t thread_count) const;
+	void FlushAll(optional_idx extra_memory = optional_idx()) const;
+
+private:
+	bool IsActive() const;
+	bool IsEnabled() const;
+	bool IsInPool(data_ptr_t pointer) const;
+
+	idx_t ModuloBlockSize(idx_t n) const;
+	idx_t DivBlockSize(idx_t n) const;
+
+	uint32_t GetBlockID(data_ptr_t pointer) const;
+	data_ptr_t GetPointer(uint32_t block_id) const;
+
+	void VerifyBlockID(uint32_t block_id) const;
+
+	void FreeInternal(idx_t extra_memory) const;
+	void FreeContiguousBlocks(uint32_t block_id_start, uint32_t block_id_end_including) const;
+
+private:
+	//! Identifier
+	const hugeint_t uuid;
+	//! Fallback allocator
+	Allocator &allocator;
+
+	//! Block size (power of two)
+	const idx_t block_size;
+	//! Shift for dividing by block size
+	const idx_t block_size_div_shift;
+
+	//! Size of the virtual memory
+	const idx_t virtual_memory_size;
+	//! Pointer to the start of the virtual memory
+	const data_ptr_t virtual_memory_space;
+
+	//! Mutex for modifying physical memory size
+	mutex physical_memory_lock;
+	//! Size of the physical memory
+	atomic<idx_t> physical_memory_size;
+
+	//! Untouched block IDs
+	unsafe_unique_ptr<BlockQueue> untouched;
+	//! Touched by block IDs
+	unsafe_unique_ptr<BlockQueue> touched;
+};
+
+} // namespace duckdb
diff --git a/src/include/duckdb/storage/buffer/buffer_pool.hpp b/src/include/duckdb/storage/buffer/buffer_pool.hpp
index 9df6743e21..a77b34ef77 100644
--- a/src/include/duckdb/storage/buffer/buffer_pool.hpp
+++ b/src/include/duckdb/storage/buffer/buffer_pool.hpp
@@ -41,7 +41,8 @@ class BufferPool {
 	friend class StandardBufferManager;
 
 public:
-	BufferPool(idx_t maximum_memory, bool track_eviction_timestamps, idx_t allocator_bulk_deallocation_flush_threshold);
+	BufferPool(BlockAllocator &block_allocator, idx_t maximum_memory, bool track_eviction_timestamps,
+	           idx_t allocator_bulk_deallocation_flush_threshold);
 	virtual ~BufferPool();
 
 	//! Set a new memory limit to the buffer pool, throws an exception if the new limit is too low and not enough
@@ -160,6 +161,8 @@ protected:
 	//! and only updates the global counter when the cache value exceeds a threshold.
 	//! Therefore, the statistics may have slight differences from the actual memory usage.
 	mutable MemoryUsage memory_usage;
+	//! The block allocator
+	BlockAllocator &block_allocator;
 };
 
 } // namespace duckdb
diff --git a/src/include/duckdb/storage/compression/alp/alp_compress.hpp b/src/include/duckdb/storage/compression/alp/alp_compress.hpp
index 38ed76769e..7e84bf1c07 100644
--- a/src/include/duckdb/storage/compression/alp/alp_compress.hpp
+++ b/src/include/duckdb/storage/compression/alp/alp_compress.hpp
@@ -34,7 +34,7 @@ public:
 	AlpCompressionState(ColumnDataCheckpointData &checkpoint_data, AlpAnalyzeState<T> *analyze_state)
 	    : CompressionState(analyze_state->info), checkpoint_data(checkpoint_data),
 	      function(checkpoint_data.GetCompressionFunction(CompressionType::COMPRESSION_ALP)) {
-		CreateEmptySegment(checkpoint_data.GetRowGroup().start);
+		CreateEmptySegment();
 
 		//! Combinations found on the analyze step are needed for compression
 		state.best_k_combinations = analyze_state->state.best_k_combinations;
@@ -88,12 +88,12 @@ public:
 		state.Reset();
 	}
 
-	void CreateEmptySegment(idx_t row_start) {
+	void CreateEmptySegment() {
 		auto &db = checkpoint_data.GetDatabase();
 		auto &type = checkpoint_data.GetType();
 
-		auto compressed_segment = ColumnSegment::CreateTransientSegment(db, function, type, row_start,
-		                                                                info.GetBlockSize(), info.GetBlockManager());
+		auto compressed_segment =
+		    ColumnSegment::CreateTransientSegment(db, function, type, info.GetBlockSize(), info.GetBlockManager());
 		current_segment = std::move(compressed_segment);
 
 		auto &buffer_manager = BufferManager::GetBufferManager(current_segment->db);
@@ -113,9 +113,8 @@ public:
 		alp::AlpCompression<T, false>::Compress(input_vector, vector_idx, vector_null_positions, nulls_idx, state);
 		//! Check if the compressed vector fits on current segment
 		if (!HasEnoughSpace()) {
-			auto row_start = current_segment->start + current_segment->count;
 			FlushSegment();
-			CreateEmptySegment(row_start);
+			CreateEmptySegment();
 		}
 
 		if (vector_idx != nulls_idx) { //! At least there is one valid value in the vector
diff --git a/src/include/duckdb/storage/compression/alprd/alprd_compress.hpp b/src/include/duckdb/storage/compression/alprd/alprd_compress.hpp
index 2eff3f97d7..3f2534cbeb 100644
--- a/src/include/duckdb/storage/compression/alprd/alprd_compress.hpp
+++ b/src/include/duckdb/storage/compression/alprd/alprd_compress.hpp
@@ -45,7 +45,7 @@ public:
 		next_vector_byte_index_start = AlpRDConstants::HEADER_SIZE + actual_dictionary_size_bytes;
 		memcpy((void *)state.left_parts_dict, (void *)analyze_state->state.left_parts_dict,
 		       actual_dictionary_size_bytes);
-		CreateEmptySegment(checkpoint_data.GetRowGroup().start);
+		CreateEmptySegment();
 	}
 
 	ColumnDataCheckpointData &checkpoint_data;
@@ -98,12 +98,12 @@ public:
 		state.Reset();
 	}
 
-	void CreateEmptySegment(idx_t row_start) {
+	void CreateEmptySegment() {
 		auto &db = checkpoint_data.GetDatabase();
 		auto &type = checkpoint_data.GetType();
 
-		auto compressed_segment = ColumnSegment::CreateTransientSegment(db, function, type, row_start,
-		                                                                info.GetBlockSize(), info.GetBlockManager());
+		auto compressed_segment =
+		    ColumnSegment::CreateTransientSegment(db, function, type, info.GetBlockSize(), info.GetBlockManager());
 		current_segment = std::move(compressed_segment);
 
 		auto &buffer_manager = BufferManager::GetBufferManager(db);
@@ -125,9 +125,8 @@ public:
 		alp::AlpRDCompression<T, false>::Compress(input_vector, vector_idx, state);
 		//! Check if the compressed vector fits on current segment
 		if (!HasEnoughSpace()) {
-			auto row_start = current_segment->start + current_segment->count;
 			FlushSegment();
-			CreateEmptySegment(row_start);
+			CreateEmptySegment();
 		}
 		if (vector_idx != nulls_idx) { //! At least there is one valid value in the vector
 			for (idx_t i = 0; i < vector_idx; i++) {
diff --git a/src/include/duckdb/storage/compression/dict_fsst/compression.hpp b/src/include/duckdb/storage/compression/dict_fsst/compression.hpp
index d8718eb36b..94e26c5b45 100644
--- a/src/include/duckdb/storage/compression/dict_fsst/compression.hpp
+++ b/src/include/duckdb/storage/compression/dict_fsst/compression.hpp
@@ -39,7 +39,7 @@ public:
 	~DictFSSTCompressionState() override;
 
 public:
-	void CreateEmptySegment(idx_t row_start);
+	void CreateEmptySegment();
 	idx_t Finalize();
 
 	bool AllUnique() const;
diff --git a/src/include/duckdb/storage/compression/dictionary/compression.hpp b/src/include/duckdb/storage/compression/dictionary/compression.hpp
index 97f33bf240..be255fea02 100644
--- a/src/include/duckdb/storage/compression/dictionary/compression.hpp
+++ b/src/include/duckdb/storage/compression/dictionary/compression.hpp
@@ -28,7 +28,7 @@ public:
 	                                   idx_t max_unique_count_across_all_segments);
 
 public:
-	void CreateEmptySegment(idx_t row_start);
+	void CreateEmptySegment();
 	void Verify() override;
 	bool LookupString(string_t str) override;
 	void AddNewString(string_t str) override;
diff --git a/src/include/duckdb/storage/compression/empty_validity.hpp b/src/include/duckdb/storage/compression/empty_validity.hpp
index 476faf89fc..26af6fa4b1 100644
--- a/src/include/duckdb/storage/compression/empty_validity.hpp
+++ b/src/include/duckdb/storage/compression/empty_validity.hpp
@@ -58,11 +58,10 @@ public:
 
 		auto &db = checkpoint_data.GetDatabase();
 		auto &type = checkpoint_data.GetType();
-		auto row_start = checkpoint_data.GetRowGroup().start;
 
 		auto &info = state.info;
-		auto compressed_segment = ColumnSegment::CreateTransientSegment(db, *state.function, type, row_start,
-		                                                                info.GetBlockSize(), info.GetBlockManager());
+		auto compressed_segment = ColumnSegment::CreateTransientSegment(db, *state.function, type, info.GetBlockSize(),
+		                                                                info.GetBlockManager());
 		compressed_segment->count = state.count;
 		if (state.non_nulls != state.count) {
 			compressed_segment->stats.statistics.SetHasNullFast();
diff --git a/src/include/duckdb/storage/compression/roaring/roaring.hpp b/src/include/duckdb/storage/compression/roaring/roaring.hpp
index da2fb5710d..f6056ba9f1 100644
--- a/src/include/duckdb/storage/compression/roaring/roaring.hpp
+++ b/src/include/duckdb/storage/compression/roaring/roaring.hpp
@@ -337,7 +337,7 @@ public:
 	idx_t GetRemainingSpace();
 	bool CanStore(idx_t container_size, const ContainerMetadata &metadata);
 	void InitializeContainer();
-	void CreateEmptySegment(idx_t row_start);
+	void CreateEmptySegment();
 	void FlushSegment();
 	void Finalize();
 	void FlushContainer();
diff --git a/src/include/duckdb/storage/storage_options.hpp b/src/include/duckdb/storage/storage_options.hpp
index 786924b2cf..4cf1f539ba 100644
--- a/src/include/duckdb/storage/storage_options.hpp
+++ b/src/include/duckdb/storage/storage_options.hpp
@@ -40,11 +40,4 @@ struct StorageOptions {
 	void Initialize(const unordered_map<string, Value> &options);
 };
 
-inline void ClearUserKey(shared_ptr<string> const &encryption_key) {
-	if (encryption_key && !encryption_key->empty()) {
-		memset(&(*encryption_key)[0], 0, encryption_key->size());
-		encryption_key->clear();
-	}
-}
-
 } // namespace duckdb
diff --git a/src/include/duckdb/storage/table/append_state.hpp b/src/include/duckdb/storage/table/append_state.hpp
index 203263f70a..f3e3a4c5c1 100644
--- a/src/include/duckdb/storage/table/append_state.hpp
+++ b/src/include/duckdb/storage/table/append_state.hpp
@@ -68,6 +68,7 @@ struct TableAppendState {
 	row_t current_row;
 	//! The total number of rows appended by the append operation
 	idx_t total_append_count;
+	idx_t row_group_start;
 	//! The first row-group that has been appended to
 	optional_ptr<SegmentNode<RowGroup>> start_row_group;
 	//! The transaction data
diff --git a/src/include/duckdb/storage/table/array_column_data.hpp b/src/include/duckdb/storage/table/array_column_data.hpp
index c246d68b6d..c88c14c68f 100644
--- a/src/include/duckdb/storage/table/array_column_data.hpp
+++ b/src/include/duckdb/storage/table/array_column_data.hpp
@@ -16,8 +16,8 @@ namespace duckdb {
 //! List column data represents a list
 class ArrayColumnData : public ColumnData {
 public:
-	ArrayColumnData(BlockManager &block_manager, DataTableInfo &info, idx_t column_index, idx_t start_row,
-	                LogicalType type, optional_ptr<ColumnData> parent = nullptr);
+	ArrayColumnData(BlockManager &block_manager, DataTableInfo &info, idx_t column_index, LogicalType type,
+	                ColumnDataType data_type, optional_ptr<ColumnData> parent);
 
 	//! The child-column of the list
 	unique_ptr<ColumnData> child_column;
@@ -25,7 +25,7 @@ public:
 	ValidityColumnData validity;
 
 public:
-	void SetStart(idx_t new_start) override;
+	void SetDataType(ColumnDataType data_type) override;
 	FilterPropagateResult CheckZonemap(ColumnScanState &state, TableFilter &filter) override;
 
 	void InitializePrefetch(PrefetchState &prefetch_state, ColumnScanState &scan_state, idx_t rows) override;
@@ -44,14 +44,15 @@ public:
 
 	void InitializeAppend(ColumnAppendState &state) override;
 	void Append(BaseStatistics &stats, ColumnAppendState &state, Vector &vector, idx_t count) override;
-	void RevertAppend(row_t start_row) override;
+	void RevertAppend(row_t new_count) override;
 	idx_t Fetch(ColumnScanState &state, row_t row_id, Vector &result) override;
 	void FetchRow(TransactionData transaction, ColumnFetchState &state, row_t row_id, Vector &result,
 	              idx_t result_idx) override;
 	void Update(TransactionData transaction, DataTable &data_table, idx_t column_index, Vector &update_vector,
-	            row_t *row_ids, idx_t update_count) override;
+	            row_t *row_ids, idx_t update_count, idx_t row_group_start) override;
 	void UpdateColumn(TransactionData transaction, DataTable &data_table, const vector<column_t> &column_path,
-	                  Vector &update_vector, row_t *row_ids, idx_t update_count, idx_t depth) override;
+	                  Vector &update_vector, row_t *row_ids, idx_t update_count, idx_t depth,
+	                  idx_t row_group_start) override;
 	unique_ptr<BaseStatistics> GetUpdateStatistics() override;
 
 	void CommitDropColumn() override;
diff --git a/src/include/duckdb/storage/table/column_data.hpp b/src/include/duckdb/storage/table/column_data.hpp
index ab8a5970e9..d442000e63 100644
--- a/src/include/duckdb/storage/table/column_data.hpp
+++ b/src/include/duckdb/storage/table/column_data.hpp
@@ -51,16 +51,16 @@ private:
 	RowGroupWriteInfo &info;
 };
 
+enum class ColumnDataType { MAIN_TABLE, INITIAL_TRANSACTION_LOCAL, TRANSACTION_LOCAL };
+
 class ColumnData {
 	friend class ColumnDataCheckpointer;
 
 public:
-	ColumnData(BlockManager &block_manager, DataTableInfo &info, idx_t column_index, idx_t start_row, LogicalType type,
-	           optional_ptr<ColumnData> parent);
+	ColumnData(BlockManager &block_manager, DataTableInfo &info, idx_t column_index, LogicalType type,
+	           ColumnDataType data_type, optional_ptr<ColumnData> parent);
 	virtual ~ColumnData();
 
-	//! The start row
-	idx_t start;
 	//! The count of the column data
 	atomic<idx_t> count;
 	//! The block manager
@@ -89,6 +89,10 @@ public:
 	optional_ptr<const CompressionFunction> GetCompressionFunction() const {
 		return compression.get();
 	}
+	virtual void SetDataType(ColumnDataType data_type);
+	ColumnDataType GetDataType() const {
+		return data_type;
+	}
 
 	bool HasParent() const {
 		return parent != nullptr;
@@ -98,7 +102,6 @@ public:
 		return *parent;
 	}
 
-	virtual void SetStart(idx_t new_start);
 	//! The root type of the column
 	const LogicalType &RootType() const;
 	//! Whether or not the column has any updates
@@ -148,7 +151,7 @@ public:
 	void Append(ColumnAppendState &state, Vector &vector, idx_t count);
 	virtual void AppendData(BaseStatistics &stats, ColumnAppendState &state, UnifiedVectorFormat &vdata, idx_t count);
 	//! Revert a set of appends to the ColumnData
-	virtual void RevertAppend(row_t start_row);
+	virtual void RevertAppend(row_t new_count);
 
 	//! Fetch the vector from the column data that belongs to this specific row
 	virtual idx_t Fetch(ColumnScanState &state, row_t row_id, Vector &result);
@@ -157,9 +160,10 @@ public:
 	                      idx_t result_idx);
 
 	virtual void Update(TransactionData transaction, DataTable &data_table, idx_t column_index, Vector &update_vector,
-	                    row_t *row_ids, idx_t update_count);
+	                    row_t *row_ids, idx_t update_count, idx_t row_group_start);
 	virtual void UpdateColumn(TransactionData transaction, DataTable &data_table, const vector<column_t> &column_path,
-	                          Vector &update_vector, row_t *row_ids, idx_t update_count, idx_t depth);
+	                          Vector &update_vector, row_t *row_ids, idx_t update_count, idx_t depth,
+	                          idx_t row_group_start);
 	virtual unique_ptr<BaseStatistics> GetUpdateStatistics();
 
 	virtual void CommitDropColumn();
@@ -168,8 +172,7 @@ public:
 	                                                                PartialBlockManager &partial_block_manager);
 	virtual unique_ptr<ColumnCheckpointState> Checkpoint(RowGroup &row_group, ColumnCheckpointInfo &info);
 
-	virtual void CheckpointScan(ColumnSegment &segment, ColumnScanState &state, idx_t row_group_start, idx_t count,
-	                            Vector &scan_vector);
+	virtual void CheckpointScan(ColumnSegment &segment, ColumnScanState &state, idx_t count, Vector &scan_vector);
 
 	virtual bool IsPersistent();
 	vector<DataPointer> GetDataPointers();
@@ -178,7 +181,7 @@ public:
 	void InitializeColumn(PersistentColumnData &column_data);
 	virtual void InitializeColumn(PersistentColumnData &column_data, BaseStatistics &target_stats);
 	static shared_ptr<ColumnData> Deserialize(BlockManager &block_manager, DataTableInfo &info, idx_t column_index,
-	                                          idx_t start_row, ReadStream &source, const LogicalType &type);
+	                                          ReadStream &source, const LogicalType &type);
 
 	virtual void GetColumnSegmentInfo(const QueryContext &context, idx_t row_group_index, vector<idx_t> col_path,
 	                                  vector<ColumnSegmentInfo> &result);
@@ -187,10 +190,12 @@ public:
 	FilterPropagateResult CheckZonemap(TableFilter &filter);
 
 	static shared_ptr<ColumnData> CreateColumn(BlockManager &block_manager, DataTableInfo &info, idx_t column_index,
-	                                           idx_t start_row, const LogicalType &type,
+	                                           const LogicalType &type,
+	                                           ColumnDataType data_type = ColumnDataType::MAIN_TABLE,
 	                                           optional_ptr<ColumnData> parent = nullptr);
 	static unique_ptr<ColumnData> CreateColumnUnique(BlockManager &block_manager, DataTableInfo &info,
-	                                                 idx_t column_index, idx_t start_row, const LogicalType &type,
+	                                                 idx_t column_index, const LogicalType &type,
+	                                                 ColumnDataType data_type = ColumnDataType::MAIN_TABLE,
 	                                                 optional_ptr<ColumnData> parent = nullptr);
 
 	void MergeStatistics(const BaseStatistics &other);
@@ -221,8 +226,8 @@ protected:
 	                  bool allow_updates, bool scan_committed);
 	void FetchUpdateRow(TransactionData transaction, row_t row_id, Vector &result, idx_t result_idx);
 	void UpdateInternal(TransactionData transaction, DataTable &data_table, idx_t column_index, Vector &update_vector,
-	                    row_t *row_ids, idx_t update_count, Vector &base_vector);
-	idx_t FetchUpdateData(ColumnScanState &state, row_t *row_ids, Vector &base_vector);
+	                    row_t *row_ids, idx_t update_count, Vector &base_vector, idx_t row_group_start);
+	idx_t FetchUpdateData(ColumnScanState &state, row_t *row_ids, Vector &base_vector, idx_t row_group_start);
 
 	idx_t GetVectorCount(idx_t vector_index) const;
 
@@ -244,6 +249,8 @@ protected:
 	atomic<idx_t> allocation_size;
 
 private:
+	//! Whether or not this column data belongs to a main table or if it is transaction local
+	atomic<ColumnDataType> data_type;
 	//! The parent column (if any)
 	optional_ptr<ColumnData> parent;
 	//!	The compression function used by the ColumnData
diff --git a/src/include/duckdb/storage/table/column_segment.hpp b/src/include/duckdb/storage/table/column_segment.hpp
index e996649586..432cf83488 100644
--- a/src/include/duckdb/storage/table/column_segment.hpp
+++ b/src/include/duckdb/storage/table/column_segment.hpp
@@ -42,24 +42,23 @@ class ColumnSegment : public SegmentBase<ColumnSegment> {
 public:
 	//! Construct a column segment.
 	ColumnSegment(DatabaseInstance &db, shared_ptr<BlockHandle> block, const LogicalType &type,
-	              const ColumnSegmentType segment_type, const idx_t start, const idx_t count,
-	              CompressionFunction &function_p, BaseStatistics statistics, const block_id_t block_id_p,
-	              const idx_t offset, const idx_t segment_size_p,
-	              unique_ptr<ColumnSegmentState> segment_state_p = nullptr);
+	              const ColumnSegmentType segment_type, const idx_t count, CompressionFunction &function_p,
+	              BaseStatistics statistics, const block_id_t block_id_p, const idx_t offset,
+	              const idx_t segment_size_p, unique_ptr<ColumnSegmentState> segment_state_p = nullptr);
 	//! Construct a column segment from another column segment.
 	//! The other column segment becomes invalid (std::move).
-	ColumnSegment(ColumnSegment &other, const idx_t start);
+	ColumnSegment(ColumnSegment &other);
 	~ColumnSegment();
 
 public:
 	static unique_ptr<ColumnSegment> CreatePersistentSegment(DatabaseInstance &db, BlockManager &block_manager,
 	                                                         block_id_t id, idx_t offset, const LogicalType &type_p,
-	                                                         idx_t start, idx_t count, CompressionType compression_type,
+	                                                         idx_t count, CompressionType compression_type,
 	                                                         BaseStatistics statistics,
 	                                                         unique_ptr<ColumnSegmentState> segment_state);
 	static unique_ptr<ColumnSegment> CreateTransientSegment(DatabaseInstance &db, CompressionFunction &function,
-	                                                        const LogicalType &type, const idx_t start,
-	                                                        const idx_t segment_size, BlockManager &block_manager);
+	                                                        const LogicalType &type, const idx_t segment_size,
+	                                                        BlockManager &block_manager);
 
 public:
 	void InitializePrefetch(PrefetchState &prefetch_state, ColumnScanState &scan_state);
@@ -96,7 +95,7 @@ public:
 	//! Returns the number of bytes occupied within the segment
 	idx_t FinalizeAppend(ColumnAppendState &state);
 	//! Revert an append made to this segment
-	void RevertAppend(idx_t start_row);
+	void RevertAppend(idx_t new_count);
 
 	//! Convert a transient in-memory segment to a persistent segment backed by an on-disk block.
 	//! Only used during checkpointing.
@@ -106,7 +105,7 @@ public:
 	void MarkAsPersistent(shared_ptr<BlockHandle> block, uint32_t offset_in_block);
 	void SetBlock(shared_ptr<BlockHandle> block, uint32_t offset);
 	//! Gets a data pointer from a persistent column segment
-	DataPointer GetDataPointer();
+	DataPointer GetDataPointer(idx_t row_start);
 
 	block_id_t GetBlockId() {
 		D_ASSERT(segment_type == ColumnSegmentType::PERSISTENT);
@@ -125,12 +124,6 @@ public:
 		return offset;
 	}
 
-	idx_t GetRelativeIndex(idx_t row_index) {
-		D_ASSERT(row_index >= this->start);
-		D_ASSERT(row_index <= this->start + this->count);
-		return row_index - this->start;
-	}
-
 	optional_ptr<CompressedSegmentState> GetSegmentState() {
 		return segment_state.get();
 	}
diff --git a/src/include/duckdb/storage/table/list_column_data.hpp b/src/include/duckdb/storage/table/list_column_data.hpp
index 621ece4517..9282f5a334 100644
--- a/src/include/duckdb/storage/table/list_column_data.hpp
+++ b/src/include/duckdb/storage/table/list_column_data.hpp
@@ -16,8 +16,8 @@ namespace duckdb {
 //! List column data represents a list
 class ListColumnData : public ColumnData {
 public:
-	ListColumnData(BlockManager &block_manager, DataTableInfo &info, idx_t column_index, idx_t start_row,
-	               LogicalType type, optional_ptr<ColumnData> parent = nullptr);
+	ListColumnData(BlockManager &block_manager, DataTableInfo &info, idx_t column_index, LogicalType type,
+	               ColumnDataType data_type, optional_ptr<ColumnData> parent);
 
 	//! The child-column of the list
 	unique_ptr<ColumnData> child_column;
@@ -25,7 +25,7 @@ public:
 	ValidityColumnData validity;
 
 public:
-	void SetStart(idx_t new_start) override;
+	void SetDataType(ColumnDataType data_type) override;
 	FilterPropagateResult CheckZonemap(ColumnScanState &state, TableFilter &filter) override;
 
 	void InitializePrefetch(PrefetchState &prefetch_state, ColumnScanState &scan_state, idx_t rows) override;
@@ -42,14 +42,15 @@ public:
 
 	void InitializeAppend(ColumnAppendState &state) override;
 	void Append(BaseStatistics &stats, ColumnAppendState &state, Vector &vector, idx_t count) override;
-	void RevertAppend(row_t start_row) override;
+	void RevertAppend(row_t new_count) override;
 	idx_t Fetch(ColumnScanState &state, row_t row_id, Vector &result) override;
 	void FetchRow(TransactionData transaction, ColumnFetchState &state, row_t row_id, Vector &result,
 	              idx_t result_idx) override;
 	void Update(TransactionData transaction, DataTable &data_table, idx_t column_index, Vector &update_vector,
-	            row_t *row_ids, idx_t update_count) override;
+	            row_t *row_ids, idx_t update_count, idx_t row_group_start) override;
 	void UpdateColumn(TransactionData transaction, DataTable &data_table, const vector<column_t> &column_path,
-	                  Vector &update_vector, row_t *row_ids, idx_t update_count, idx_t depth) override;
+	                  Vector &update_vector, row_t *row_ids, idx_t update_count, idx_t depth,
+	                  idx_t row_group_start) override;
 	unique_ptr<BaseStatistics> GetUpdateStatistics() override;
 
 	void CommitDropColumn() override;
diff --git a/src/include/duckdb/storage/table/row_group.hpp b/src/include/duckdb/storage/table/row_group.hpp
index 2498cca708..6423451684 100644
--- a/src/include/duckdb/storage/table/row_group.hpp
+++ b/src/include/duckdb/storage/table/row_group.hpp
@@ -52,6 +52,7 @@ class ScanFilterInfo;
 class StorageCommitState;
 template <class T>
 struct SegmentNode;
+enum class ColumnDataType;
 
 struct RowGroupWriteInfo {
 	RowGroupWriteInfo(PartialBlockManager &manager, const vector<CompressionType> &compression_types,
@@ -85,7 +86,7 @@ public:
 	friend class ColumnData;
 
 public:
-	RowGroup(RowGroupCollection &collection, idx_t start, idx_t count);
+	RowGroup(RowGroupCollection &collection, idx_t count);
 	RowGroup(RowGroupCollection &collection, RowGroupPointer pointer);
 	RowGroup(RowGroupCollection &collection, PersistentRowGroupData &data);
 	~RowGroup();
@@ -101,7 +102,7 @@ private:
 	vector<shared_ptr<ColumnData>> columns;
 
 public:
-	void MoveToCollection(RowGroupCollection &collection, idx_t new_start);
+	void MoveToCollection(RowGroupCollection &collection);
 	RowGroupCollection &GetCollection() {
 		return collection.get();
 	}
@@ -127,7 +128,7 @@ public:
 	void CommitDrop();
 	void CommitDropColumn(const idx_t column_index);
 
-	void InitializeEmpty(const vector<LogicalType> &types);
+	void InitializeEmpty(const vector<LogicalType> &types, ColumnDataType data_type);
 	bool HasChanges() const;
 
 	//! Initialize a scan over this row_group
@@ -157,12 +158,12 @@ public:
 	//! Commit a previous append made by RowGroup::AppendVersionInfo
 	void CommitAppend(transaction_t commit_id, idx_t start, idx_t count);
 	//! Revert a previous append made by RowGroup::AppendVersionInfo
-	void RevertAppend(idx_t start);
+	void RevertAppend(idx_t new_count);
 	//! Clean up append states that can either be compressed or deleted
 	void CleanupAppend(transaction_t lowest_transaction, idx_t start, idx_t count);
 
 	//! Delete the given set of rows in the version manager
-	idx_t Delete(TransactionData transaction, DataTable &table, row_t *row_ids, idx_t count);
+	idx_t Delete(TransactionData transaction, DataTable &table, row_t *row_ids, idx_t count, idx_t row_group_start);
 
 	static vector<RowGroupWriteData> WriteToDisk(RowGroupWriteInfo &info,
 	                                             const vector<reference<RowGroup>> &row_groups);
@@ -170,19 +171,20 @@ public:
 	//! Returns the number of committed rows (count - committed deletes)
 	idx_t GetCommittedRowCount();
 	RowGroupWriteData WriteToDisk(RowGroupWriter &writer);
-	RowGroupPointer Checkpoint(RowGroupWriteData write_data, RowGroupWriter &writer, TableStatistics &global_stats);
+	RowGroupPointer Checkpoint(RowGroupWriteData write_data, RowGroupWriter &writer, TableStatistics &global_stats,
+	                           idx_t row_group_start);
 	bool IsPersistent() const;
-	PersistentRowGroupData SerializeRowGroupInfo() const;
+	PersistentRowGroupData SerializeRowGroupInfo(idx_t row_group_start) const;
 
 	void InitializeAppend(RowGroupAppendState &append_state);
 	void Append(RowGroupAppendState &append_state, DataChunk &chunk, idx_t append_count);
 
 	void Update(TransactionData transaction, DataTable &data_table, DataChunk &updates, row_t *ids, idx_t offset,
-	            idx_t count, const vector<PhysicalIndex> &column_ids);
+	            idx_t count, const vector<PhysicalIndex> &column_ids, idx_t row_group_start);
 	//! Update a single column; corresponds to DataTable::UpdateColumn
 	//! This method should only be called from the WAL
 	void UpdateColumn(TransactionData transaction, DataTable &data_table, DataChunk &updates, Vector &row_ids,
-	                  idx_t offset, idx_t count, const vector<column_t> &column_path);
+	                  idx_t offset, idx_t count, const vector<column_t> &column_path, idx_t row_group_start);
 
 	void MergeStatistics(idx_t column_idx, const BaseStatistics &other);
 	void MergeIntoStatistics(idx_t column_idx, BaseStatistics &other);
@@ -190,7 +192,7 @@ public:
 	unique_ptr<BaseStatistics> GetStatistics(idx_t column_idx);
 
 	void GetColumnSegmentInfo(const QueryContext &context, idx_t row_group_index, vector<ColumnSegmentInfo> &result);
-	PartitionStatistics GetPartitionStats() const;
+	PartitionStatistics GetPartitionStats(idx_t row_group_start) const;
 
 	idx_t GetAllocationSize() const {
 		return allocation_size;
diff --git a/src/include/duckdb/storage/table/row_group_collection.hpp b/src/include/duckdb/storage/table/row_group_collection.hpp
index ddd22d29d7..216114b3fe 100644
--- a/src/include/duckdb/storage/table/row_group_collection.hpp
+++ b/src/include/duckdb/storage/table/row_group_collection.hpp
@@ -150,6 +150,7 @@ public:
 	idx_t GetRowGroupSize() const {
 		return row_group_size;
 	}
+	idx_t GetBaseRowId() const;
 	void SetAppendRequiresNewRowGroup();
 
 private:
@@ -168,7 +169,6 @@ private:
 	shared_ptr<DataTableInfo> info;
 	//! The column types of the row group collection
 	vector<LogicalType> types;
-	idx_t row_start;
 	//! The segment trees holding the various row_groups of the table
 	shared_ptr<RowGroupSegmentTree> row_groups;
 	//! Table statistics
diff --git a/src/include/duckdb/storage/table/row_group_segment_tree.hpp b/src/include/duckdb/storage/table/row_group_segment_tree.hpp
index d936101381..1d80670bf1 100644
--- a/src/include/duckdb/storage/table/row_group_segment_tree.hpp
+++ b/src/include/duckdb/storage/table/row_group_segment_tree.hpp
@@ -18,7 +18,7 @@ class MetadataReader;
 
 class RowGroupSegmentTree : public SegmentTree<RowGroup, true> {
 public:
-	explicit RowGroupSegmentTree(RowGroupCollection &collection);
+	RowGroupSegmentTree(RowGroupCollection &collection, idx_t base_row_id);
 	~RowGroupSegmentTree() override;
 
 	void Initialize(PersistentTableData &data);
diff --git a/src/include/duckdb/storage/table/row_id_column_data.hpp b/src/include/duckdb/storage/table/row_id_column_data.hpp
index f839c6b246..47119d4a8e 100644
--- a/src/include/duckdb/storage/table/row_id_column_data.hpp
+++ b/src/include/duckdb/storage/table/row_id_column_data.hpp
@@ -14,7 +14,7 @@ namespace duckdb {
 
 class RowIdColumnData : public ColumnData {
 public:
-	RowIdColumnData(BlockManager &block_manager, DataTableInfo &info, idx_t start_row);
+	RowIdColumnData(BlockManager &block_manager, DataTableInfo &info);
 
 public:
 	void InitializePrefetch(PrefetchState &prefetch_state, ColumnScanState &scan_state, idx_t rows) override;
@@ -46,12 +46,13 @@ public:
 	void InitializeAppend(ColumnAppendState &state) override;
 	void Append(BaseStatistics &stats, ColumnAppendState &state, Vector &vector, idx_t count) override;
 	void AppendData(BaseStatistics &stats, ColumnAppendState &state, UnifiedVectorFormat &vdata, idx_t count) override;
-	void RevertAppend(row_t start_row) override;
+	void RevertAppend(row_t new_count) override;
 
 	void Update(TransactionData transaction, DataTable &data_table, idx_t column_index, Vector &update_vector,
-	            row_t *row_ids, idx_t update_count) override;
+	            row_t *row_ids, idx_t update_count, idx_t row_group_start) override;
 	void UpdateColumn(TransactionData transaction, DataTable &data_table, const vector<column_t> &column_path,
-	                  Vector &update_vector, row_t *row_ids, idx_t update_count, idx_t depth) override;
+	                  Vector &update_vector, row_t *row_ids, idx_t update_count, idx_t depth,
+	                  idx_t row_group_start) override;
 
 	void CommitDropColumn() override;
 
@@ -59,8 +60,7 @@ public:
 	                                                        PartialBlockManager &partial_block_manager) override;
 	unique_ptr<ColumnCheckpointState> Checkpoint(RowGroup &row_group, ColumnCheckpointInfo &info) override;
 
-	void CheckpointScan(ColumnSegment &segment, ColumnScanState &state, idx_t row_group_start, idx_t count,
-	                    Vector &scan_vector) override;
+	void CheckpointScan(ColumnSegment &segment, ColumnScanState &state, idx_t count, Vector &scan_vector) override;
 
 	bool IsPersistent() override;
 };
diff --git a/src/include/duckdb/storage/table/row_version_manager.hpp b/src/include/duckdb/storage/table/row_version_manager.hpp
index 49ab2f40b0..0bac257767 100644
--- a/src/include/duckdb/storage/table/row_version_manager.hpp
+++ b/src/include/duckdb/storage/table/row_version_manager.hpp
@@ -37,7 +37,7 @@ public:
 
 	void AppendVersionInfo(TransactionData transaction, idx_t count, idx_t row_group_start, idx_t row_group_end);
 	void CommitAppend(transaction_t commit_id, idx_t row_group_start, idx_t count);
-	void RevertAppend(idx_t start_row);
+	void RevertAppend(idx_t new_count);
 	void CleanupAppend(transaction_t lowest_active_transaction, idx_t row_group_start, idx_t count);
 
 	idx_t DeleteRows(idx_t vector_idx, transaction_t transaction_id, row_t rows[], idx_t count);
diff --git a/src/include/duckdb/storage/table/scan_state.hpp b/src/include/duckdb/storage/table/scan_state.hpp
index fd8896c2d3..81c05cad32 100644
--- a/src/include/duckdb/storage/table/scan_state.hpp
+++ b/src/include/duckdb/storage/table/scan_state.hpp
@@ -80,20 +80,24 @@ struct IndexScanState {
 typedef unordered_map<block_id_t, BufferHandle> buffer_handle_set_t;
 
 struct ColumnScanState {
+	explicit ColumnScanState(optional_ptr<CollectionScanState> parent_p) : parent(parent_p) {
+	}
+
+	optional_ptr<CollectionScanState> parent;
 	//! The query context for this scan
 	QueryContext context;
 	//! The column segment that is currently being scanned
 	optional_ptr<SegmentNode<ColumnSegment>> current;
 	//! Column segment tree
 	ColumnSegmentTree *segment_tree = nullptr;
-	//! The current row index of the scan
-	idx_t row_index = 0;
+	//! The current row offset in the column
+	idx_t offset_in_column = 0;
 	//! The internal row index (i.e. the position of the SegmentScanState)
 	idx_t internal_index = 0;
 	//! Segment scan state
 	unique_ptr<SegmentScanState> scan_state;
 	//! Child states of the vector
-	vector<ColumnScanState> child_states;
+	unsafe_vector<ColumnScanState> child_states;
 	//! Whether or not InitializeState has been called for this segment
 	bool initialized = false;
 	//! If this segment has already been checked for skipping purposes
@@ -116,6 +120,8 @@ public:
 	void Next(idx_t count);
 	//! Move ONLY this state forward by "count" rows (i.e. not the child states)
 	void NextInternal(idx_t count);
+	//! Returns the current row position in the segment
+	idx_t GetPositionInSegment() const;
 };
 
 struct ColumnFetchState {
@@ -125,6 +131,8 @@ struct ColumnFetchState {
 	buffer_handle_set_t handles;
 	//! Any child states of the fetch
 	vector<unique_ptr<ColumnFetchState>> child_states;
+	//! The current row group we are fetching from
+	optional_ptr<SegmentNode<RowGroup>> row_group;
 
 	BufferHandle &GetOrInsertHandle(ColumnSegment &segment);
 };
@@ -219,7 +227,7 @@ public:
 	//! The maximum row within the row group
 	idx_t max_row_group_row;
 	//! Child column scans
-	unsafe_unique_array<ColumnScanState> column_scans;
+	unsafe_vector<ColumnScanState> column_scans;
 	//! Row group segment tree
 	RowGroupSegmentTree *row_groups;
 	//! The total maximum row index
diff --git a/src/include/duckdb/storage/table/segment_base.hpp b/src/include/duckdb/storage/table/segment_base.hpp
index 57002ebb4c..1a45c26e97 100644
--- a/src/include/duckdb/storage/table/segment_base.hpp
+++ b/src/include/duckdb/storage/table/segment_base.hpp
@@ -16,11 +16,9 @@ namespace duckdb {
 template <class T>
 class SegmentBase {
 public:
-	SegmentBase(idx_t start, idx_t count) : start(start), count(count) {
+	explicit SegmentBase(idx_t count) : count(count) {
 	}
 
-	//! The start row id of this chunk
-	idx_t start;
 	//! The amount of entries in this storage chunk
 	atomic<idx_t> count;
 };
diff --git a/src/include/duckdb/storage/table/segment_tree.hpp b/src/include/duckdb/storage/table/segment_tree.hpp
index 9a0427391d..96f872bc62 100644
--- a/src/include/duckdb/storage/table/segment_tree.hpp
+++ b/src/include/duckdb/storage/table/segment_tree.hpp
@@ -52,7 +52,7 @@ private:
 	class SegmentNodeIterationHelper;
 
 public:
-	explicit SegmentTree() : finished_loading(true) {
+	explicit SegmentTree(idx_t base_row_id = 0) : finished_loading(true), base_row_id(base_row_id) {
 	}
 	virtual ~SegmentTree() {
 	}
@@ -171,11 +171,11 @@ public:
 	}
 
 	//! Append a column segment to the tree
-	void AppendSegmentInternal(SegmentLock &l, unique_ptr<T> segment) {
+	void AppendSegmentInternal(SegmentLock &l, unique_ptr<T> segment, idx_t row_start) {
 		D_ASSERT(segment);
 		// add the node to the list of nodes
 		auto node = make_uniq<SegmentNode<T>>();
-		node->row_start = segment->start;
+		node->row_start = row_start;
 		node->node = std::move(segment);
 		node->index = nodes.size();
 		node->next = nullptr;
@@ -184,6 +184,16 @@ public:
 		}
 		nodes.push_back(std::move(node));
 	}
+	void AppendSegmentInternal(SegmentLock &l, unique_ptr<T> segment) {
+		idx_t row_start;
+		if (nodes.empty()) {
+			row_start = base_row_id;
+		} else {
+			auto &last_node = nodes.back();
+			row_start = last_node->row_start + last_node->node->count;
+		}
+		AppendSegmentInternal(l, std::move(segment), row_start);
+	}
 	void AppendSegment(unique_ptr<T> segment) {
 		auto l = Lock();
 		AppendSegment(l, std::move(segment));
@@ -192,6 +202,10 @@ public:
 		LoadAllSegments(l);
 		AppendSegmentInternal(l, std::move(segment));
 	}
+	void AppendSegment(SegmentLock &l, unique_ptr<T> segment, idx_t row_start) {
+		LoadAllSegments(l);
+		AppendSegmentInternal(l, std::move(segment), row_start);
+	}
 	//! Debug method, check whether the segment is in the segment tree
 	bool HasSegment(SegmentNode<T> &segment) {
 		auto l = Lock();
@@ -249,7 +263,6 @@ public:
 				                        segments);
 			}
 			auto &entry = *nodes[index];
-			D_ASSERT(entry.row_start == entry.node->start);
 			if (row_number < entry.row_start) {
 				upper = index - 1;
 			} else if (row_number >= entry.row_start + entry.node->count) {
@@ -264,10 +277,9 @@ public:
 
 	void Verify(SegmentLock &) {
 #ifdef DEBUG
-		idx_t base_start = nodes.empty() ? 0 : nodes[0]->node->start;
+		idx_t base_start = nodes.empty() ? 0 : nodes[0]->row_start;
 		for (idx_t i = 0; i < nodes.size(); i++) {
-			D_ASSERT(nodes[i]->row_start == nodes[i]->node->start);
-			D_ASSERT(nodes[i]->node->start == base_start);
+			D_ASSERT(nodes[i]->row_start == base_start);
 			base_start += nodes[i]->node->count;
 		}
 #endif
@@ -279,6 +291,10 @@ public:
 #endif
 	}
 
+	idx_t GetBaseRowId() const {
+		return base_row_id;
+	}
+
 	SegmentIterationHelper Segments() {
 		return SegmentIterationHelper(*this);
 	}
@@ -295,20 +311,6 @@ public:
 		return SegmentNodeIterationHelper(*this, l);
 	}
 
-	void Reinitialize() {
-		if (nodes.empty()) {
-			return;
-		}
-		idx_t offset = nodes[0]->node->start;
-		for (auto &entry : nodes) {
-			if (entry->node->start != offset) {
-				throw InternalException("In SegmentTree::Reinitialize - gap found between nodes!");
-			}
-			entry->row_start = offset;
-			offset += entry->node->count;
-		}
-	}
-
 protected:
 	atomic<bool> finished_loading;
 
@@ -326,6 +328,8 @@ private:
 	vector<unique_ptr<SegmentNode<T>>> nodes;
 	//! Lock to access or modify the nodes
 	mutable mutex node_lock;
+	//! Base row id (row id of the first segment)
+	idx_t base_row_id;
 
 private:
 	class BaseSegmentIterator {
diff --git a/src/include/duckdb/storage/table/standard_column_data.hpp b/src/include/duckdb/storage/table/standard_column_data.hpp
index ec06eb30a4..5376eebb6c 100644
--- a/src/include/duckdb/storage/table/standard_column_data.hpp
+++ b/src/include/duckdb/storage/table/standard_column_data.hpp
@@ -16,14 +16,14 @@ namespace duckdb {
 //! Standard column data represents a regular flat column (e.g. a column of type INTEGER or STRING)
 class StandardColumnData : public ColumnData {
 public:
-	StandardColumnData(BlockManager &block_manager, DataTableInfo &info, idx_t column_index, idx_t start_row,
-	                   LogicalType type, optional_ptr<ColumnData> parent = nullptr);
+	StandardColumnData(BlockManager &block_manager, DataTableInfo &info, idx_t column_index, LogicalType type,
+	                   ColumnDataType data_type, optional_ptr<ColumnData> parent);
 
 	//! The validity column data
 	ValidityColumnData validity;
 
 public:
-	void SetStart(idx_t new_start) override;
+	void SetDataType(ColumnDataType data_type) override;
 
 	ScanVectorType GetVectorScanType(ColumnScanState &state, idx_t scan_count, Vector &result) override;
 	void InitializePrefetch(PrefetchState &prefetch_state, ColumnScanState &scan_state, idx_t rows) override;
@@ -43,14 +43,15 @@ public:
 
 	void InitializeAppend(ColumnAppendState &state) override;
 	void AppendData(BaseStatistics &stats, ColumnAppendState &state, UnifiedVectorFormat &vdata, idx_t count) override;
-	void RevertAppend(row_t start_row) override;
+	void RevertAppend(row_t new_count) override;
 	idx_t Fetch(ColumnScanState &state, row_t row_id, Vector &result) override;
 	void FetchRow(TransactionData transaction, ColumnFetchState &state, row_t row_id, Vector &result,
 	              idx_t result_idx) override;
 	void Update(TransactionData transaction, DataTable &data_table, idx_t column_index, Vector &update_vector,
-	            row_t *row_ids, idx_t update_count) override;
+	            row_t *row_ids, idx_t update_count, idx_t row_group_start) override;
 	void UpdateColumn(TransactionData transaction, DataTable &data_table, const vector<column_t> &column_path,
-	                  Vector &update_vector, row_t *row_ids, idx_t update_count, idx_t depth) override;
+	                  Vector &update_vector, row_t *row_ids, idx_t update_count, idx_t depth,
+	                  idx_t row_group_start) override;
 	unique_ptr<BaseStatistics> GetUpdateStatistics() override;
 
 	void CommitDropColumn() override;
@@ -58,8 +59,7 @@ public:
 	unique_ptr<ColumnCheckpointState> CreateCheckpointState(RowGroup &row_group,
 	                                                        PartialBlockManager &partial_block_manager) override;
 	unique_ptr<ColumnCheckpointState> Checkpoint(RowGroup &row_group, ColumnCheckpointInfo &info) override;
-	void CheckpointScan(ColumnSegment &segment, ColumnScanState &state, idx_t row_group_start, idx_t count,
-	                    Vector &scan_vector) override;
+	void CheckpointScan(ColumnSegment &segment, ColumnScanState &state, idx_t count, Vector &scan_vector) override;
 
 	void GetColumnSegmentInfo(const QueryContext &context, duckdb::idx_t row_group_index,
 	                          vector<duckdb::idx_t> col_path, vector<duckdb::ColumnSegmentInfo> &result) override;
diff --git a/src/include/duckdb/storage/table/struct_column_data.hpp b/src/include/duckdb/storage/table/struct_column_data.hpp
index 798a213269..5012b98c24 100644
--- a/src/include/duckdb/storage/table/struct_column_data.hpp
+++ b/src/include/duckdb/storage/table/struct_column_data.hpp
@@ -16,8 +16,8 @@ namespace duckdb {
 //! Struct column data represents a struct
 class StructColumnData : public ColumnData {
 public:
-	StructColumnData(BlockManager &block_manager, DataTableInfo &info, idx_t column_index, idx_t start_row,
-	                 LogicalType type, optional_ptr<ColumnData> parent = nullptr);
+	StructColumnData(BlockManager &block_manager, DataTableInfo &info, idx_t column_index, LogicalType type,
+	                 ColumnDataType data_type, optional_ptr<ColumnData> parent);
 
 	//! The sub-columns of the struct
 	vector<unique_ptr<ColumnData>> sub_columns;
@@ -25,7 +25,7 @@ public:
 	ValidityColumnData validity;
 
 public:
-	void SetStart(idx_t new_start) override;
+	void SetDataType(ColumnDataType data_type) override;
 	idx_t GetMaxEntry() override;
 
 	void InitializePrefetch(PrefetchState &prefetch_state, ColumnScanState &scan_state, idx_t rows) override;
@@ -42,14 +42,15 @@ public:
 
 	void InitializeAppend(ColumnAppendState &state) override;
 	void Append(BaseStatistics &stats, ColumnAppendState &state, Vector &vector, idx_t count) override;
-	void RevertAppend(row_t start_row) override;
+	void RevertAppend(row_t new_count) override;
 	idx_t Fetch(ColumnScanState &state, row_t row_id, Vector &result) override;
 	void FetchRow(TransactionData transaction, ColumnFetchState &state, row_t row_id, Vector &result,
 	              idx_t result_idx) override;
 	void Update(TransactionData transaction, DataTable &data_table, idx_t column_index, Vector &update_vector,
-	            row_t *row_ids, idx_t update_count) override;
+	            row_t *row_ids, idx_t update_count, idx_t row_group_start) override;
 	void UpdateColumn(TransactionData transaction, DataTable &data_table, const vector<column_t> &column_path,
-	                  Vector &update_vector, row_t *row_ids, idx_t update_count, idx_t depth) override;
+	                  Vector &update_vector, row_t *row_ids, idx_t update_count, idx_t depth,
+	                  idx_t row_group_start) override;
 	unique_ptr<BaseStatistics> GetUpdateStatistics() override;
 
 	void CommitDropColumn() override;
diff --git a/src/include/duckdb/storage/table/update_segment.hpp b/src/include/duckdb/storage/table/update_segment.hpp
index 3f5b9d2119..6cc88457c7 100644
--- a/src/include/duckdb/storage/table/update_segment.hpp
+++ b/src/include/duckdb/storage/table/update_segment.hpp
@@ -39,7 +39,7 @@ public:
 	void FetchCommitted(idx_t vector_index, Vector &result);
 	void FetchCommittedRange(idx_t start_row, idx_t count, Vector &result);
 	void Update(TransactionData transaction, DataTable &data_table, idx_t column_index, Vector &update, row_t *ids,
-	            idx_t count, Vector &base_data);
+	            idx_t count, Vector &base_data, idx_t row_group_start);
 	void FetchRow(TransactionData transaction, idx_t row_id, Vector &result, idx_t result_idx);
 
 	void RollbackUpdate(UpdateInfo &info);
@@ -70,7 +70,7 @@ public:
 	                                             UnifiedVectorFormat &update, const SelectionVector &sel);
 	typedef void (*merge_update_function_t)(UpdateInfo &base_info, Vector &base_data, UpdateInfo &update_info,
 	                                        UnifiedVectorFormat &update, row_t *ids, idx_t count,
-	                                        const SelectionVector &sel);
+	                                        const SelectionVector &sel, idx_t row_group_start);
 	typedef void (*fetch_update_function_t)(transaction_t start_time, transaction_t transaction_id, UpdateInfo &info,
 	                                        Vector &result);
 	typedef void (*fetch_committed_function_t)(UpdateInfo &info, Vector &result);
diff --git a/src/include/duckdb/storage/table/validity_column_data.hpp b/src/include/duckdb/storage/table/validity_column_data.hpp
index 286a5343b1..ee0cc9d1e8 100644
--- a/src/include/duckdb/storage/table/validity_column_data.hpp
+++ b/src/include/duckdb/storage/table/validity_column_data.hpp
@@ -17,8 +17,7 @@ class ValidityColumnData : public ColumnData {
 	friend class StandardColumnData;
 
 public:
-	ValidityColumnData(BlockManager &block_manager, DataTableInfo &info, idx_t column_index, idx_t start_row,
-	                   ColumnData &parent);
+	ValidityColumnData(BlockManager &block_manager, DataTableInfo &info, idx_t column_index, ColumnData &parent);
 
 public:
 	FilterPropagateResult CheckZonemap(ColumnScanState &state, TableFilter &filter) override;
diff --git a/src/include/duckdb/transaction/duck_transaction.hpp b/src/include/duckdb/transaction/duck_transaction.hpp
index b9080f192f..368d98f440 100644
--- a/src/include/duckdb/transaction/duck_transaction.hpp
+++ b/src/include/duckdb/transaction/duck_transaction.hpp
@@ -74,7 +74,7 @@ public:
 	                idx_t base_row);
 	void PushSequenceUsage(SequenceCatalogEntry &entry, const SequenceData &data);
 	void PushAppend(DataTable &table, idx_t row_start, idx_t row_count);
-	UndoBufferReference CreateUpdateInfo(idx_t type_size, DataTable &data_table, idx_t entries);
+	UndoBufferReference CreateUpdateInfo(idx_t type_size, DataTable &data_table, idx_t entries, idx_t row_group_start);
 
 	bool IsDuckTransaction() const override {
 		return true;
diff --git a/src/include/duckdb/transaction/update_info.hpp b/src/include/duckdb/transaction/update_info.hpp
index 5eb1392611..cde47e2b42 100644
--- a/src/include/duckdb/transaction/update_info.hpp
+++ b/src/include/duckdb/transaction/update_info.hpp
@@ -31,6 +31,8 @@ struct UpdateInfo {
 	DataTable *table;
 	//! The column index of which column we are updating
 	idx_t column_index;
+	//! The start index of the row group
+	idx_t row_group_start;
 	//! The version number
 	atomic<transaction_t> version_number;
 	//! The vector index within the uncompressed segment
@@ -90,7 +92,8 @@ struct UpdateInfo {
 	//! Returns the total allocation size for an UpdateInfo entry, together with space for the tuple data
 	static idx_t GetAllocSize(idx_t type_size);
 	//! Initialize an UpdateInfo struct that has been allocated using GetAllocSize (i.e. has extra space after it)
-	static void Initialize(UpdateInfo &info, DataTable &data_table, transaction_t transaction_id);
+	static void Initialize(UpdateInfo &info, DataTable &data_table, transaction_t transaction_id,
+	                       idx_t row_group_start);
 };
 
 } // namespace duckdb
diff --git a/src/include/duckdb_extension.h b/src/include/duckdb_extension.h
index 4af8815b67..a5b20e3804 100644
--- a/src/include/duckdb_extension.h
+++ b/src/include/duckdb_extension.h
@@ -677,15 +677,6 @@ typedef struct {
 	int64_t (*duckdb_file_handle_size)(duckdb_file_handle file_handle);
 #endif
 
-// API to register custom log storage.
-#ifdef DUCKDB_EXTENSION_API_VERSION_UNSTABLE
-	duckdb_log_storage (*duckdb_create_log_storage)();
-	void (*duckdb_destroy_log_storage)(duckdb_log_storage storage);
-	void (*duckdb_log_storage_set_write_log_entry)(duckdb_log_storage storage,
-	                                               duckdb_logger_write_log_entry_t function);
-	void (*duckdb_register_log_storage)(duckdb_database database, const char *name, duckdb_log_storage storage);
-#endif
-
 // New functions around the client context
 #ifdef DUCKDB_EXTENSION_API_VERSION_UNSTABLE
 	idx_t (*duckdb_client_context_get_connection_id)(duckdb_client_context context);
@@ -1290,12 +1281,6 @@ typedef struct {
 #define duckdb_file_handle_sync               duckdb_ext_api.duckdb_file_handle_sync
 #define duckdb_file_handle_close              duckdb_ext_api.duckdb_file_handle_close
 
-// Version unstable_new_logger_functions
-#define duckdb_create_log_storage              duckdb_ext_api.duckdb_create_log_storage
-#define duckdb_destroy_log_storage             duckdb_ext_api.duckdb_destroy_log_storage
-#define duckdb_log_storage_set_write_log_entry duckdb_ext_api.duckdb_log_storage_set_write_log_entry
-#define duckdb_register_log_storage            duckdb_ext_api.duckdb_register_log_storage
-
 // Version unstable_new_open_connect_functions
 #define duckdb_connection_get_client_context    duckdb_ext_api.duckdb_connection_get_client_context
 #define duckdb_connection_get_arrow_options     duckdb_ext_api.duckdb_connection_get_arrow_options
diff --git a/src/logging/log_manager.cpp b/src/logging/log_manager.cpp
index 9a55dd280d..07b31f7af8 100644
--- a/src/logging/log_manager.cpp
+++ b/src/logging/log_manager.cpp
@@ -205,7 +205,7 @@ void LogManager::SetEnableStructuredLoggers(vector<string> &enabled_logger_types
 			throw InvalidInputException("Unknown log type: '%s'", enabled_logger_type);
 		}
 
-		new_config.enabled_log_types.insert(enabled_logger_type);
+		new_config.enabled_log_types.insert(lookup->name);
 
 		min_log_level = MinValue(min_log_level, lookup->level);
 	}
diff --git a/src/logging/log_types.cpp b/src/logging/log_types.cpp
index 4d2e0dea28..4441581be2 100644
--- a/src/logging/log_types.cpp
+++ b/src/logging/log_types.cpp
@@ -59,6 +59,8 @@ LogicalType HTTPLogType::GetLogType() {
 	child_list_t<LogicalType> request_child_list = {
 	    {"type", LogicalType::VARCHAR},
 	    {"url", LogicalType::VARCHAR},
+	    {"start_time", LogicalType::TIMESTAMP_TZ},
+	    {"duration_ms", LogicalType::BIGINT},
 	    {"headers", LogicalType::MAP(LogicalType::VARCHAR, LogicalType::VARCHAR)},
 	};
 	auto request_type = LogicalType::STRUCT(request_child_list);
@@ -91,7 +93,10 @@ string HTTPLogType::ConstructLogMessage(BaseRequest &request, optional_ptr<HTTPR
 	    {"type", Value(EnumUtil::ToString(request.type))},
 	    {"url", Value(request.url)},
 	    {"headers", CreateHTTPHeadersValue(request.headers)},
-	};
+	    {"start_time", request.have_request_timing ? Value::TIMESTAMP(request.request_start) : Value()},
+	    {"duration_ms", request.have_request_timing ? Value::BIGINT(Timestamp::GetEpochMs(request.request_end) -
+	                                                                Timestamp::GetEpochMs(request.request_start))
+	                                                : Value()}};
 	auto request_value = Value::STRUCT(request_child_list);
 	Value response_value;
 	if (response) {
@@ -211,9 +216,9 @@ string CheckpointLogType::ConstructLogMessage(const AttachedDatabase &db, DataTa
 }
 
 string CheckpointLogType::ConstructLogMessage(const AttachedDatabase &db, DataTableInfo &table, idx_t segment_idx,
-                                              RowGroup &row_group) {
+                                              RowGroup &row_group, idx_t row_group_start) {
 	vector<Value> map_keys = {"segment_idx", "start", "count"};
-	vector<Value> map_values = {to_string(segment_idx), to_string(row_group.start), to_string(row_group.count.load())};
+	vector<Value> map_values = {to_string(segment_idx), to_string(row_group_start), to_string(row_group.count.load())};
 	return CreateLog(db, table, "checkpoint", std::move(map_keys), std::move(map_values));
 }
 
diff --git a/src/main/appender.cpp b/src/main/appender.cpp
index 5d8b87cb31..57a337a5dd 100644
--- a/src/main/appender.cpp
+++ b/src/main/appender.cpp
@@ -41,7 +41,6 @@ void BaseAppender::Destructor() {
 	try {
 		Close();
 	} catch (...) { // NOLINT
-		            // FIXME: Log error here
 	}
 }
 
@@ -67,7 +66,7 @@ void BaseAppender::EndRow() {
 	}
 	column = 0;
 	chunk.SetCardinality(chunk.size() + 1);
-	if (chunk.size() >= STANDARD_VECTOR_SIZE) {
+	if (ShouldFlushChunk()) {
 		FlushChunk();
 	}
 }
@@ -339,7 +338,7 @@ void BaseAppender::AppendDataChunk(DataChunk &chunk_p) {
 	// Early-out, if types match.
 	if (chunk_types == appender_types) {
 		collection->Append(chunk_p);
-		if (collection->Count() >= flush_count) {
+		if (ShouldFlush()) {
 			Flush();
 		}
 		return;
@@ -372,7 +371,7 @@ void BaseAppender::AppendDataChunk(DataChunk &chunk_p) {
 	}
 
 	collection->Append(cast_chunk);
-	if (collection->Count() >= flush_count) {
+	if (ShouldFlush()) {
 		Flush();
 	}
 }
@@ -383,7 +382,7 @@ void BaseAppender::FlushChunk() {
 	}
 	collection->Append(chunk);
 	chunk.Reset();
-	if (collection->Count() >= flush_count) {
+	if (ShouldFlush()) {
 		Flush();
 	}
 }
@@ -423,8 +422,13 @@ void BaseAppender::ClearColumns() {
 //===--------------------------------------------------------------------===//
 // Table Appender
 //===--------------------------------------------------------------------===//
-Appender::Appender(Connection &con, const string &database_name, const string &schema_name, const string &table_name)
+Appender::Appender(Connection &con, const string &database_name, const string &schema_name, const string &table_name,
+                   const idx_t flush_memory_threshold_p)
     : BaseAppender(Allocator::DefaultAllocator(), AppenderType::LOGICAL), context(con.context) {
+	flush_memory_threshold = (flush_memory_threshold_p == DConstants::INVALID_INDEX)
+	                             ? optional_idx::Invalid()
+	                             : optional_idx(flush_memory_threshold_p);
+
 	description = con.TableInfo(database_name, schema_name, table_name);
 	if (!description) {
 		throw CatalogException(
@@ -480,12 +484,13 @@ Appender::Appender(Connection &con, const string &database_name, const string &s
 	collection = make_uniq<ColumnDataCollection>(allocator, GetActiveTypes());
 }
 
-Appender::Appender(Connection &con, const string &schema_name, const string &table_name)
-    : Appender(con, INVALID_CATALOG, schema_name, table_name) {
+Appender::Appender(Connection &con, const string &schema_name, const string &table_name,
+                   const idx_t flush_memory_threshold_p)
+    : Appender(con, INVALID_CATALOG, schema_name, table_name, flush_memory_threshold_p) {
 }
 
-Appender::Appender(Connection &con, const string &table_name)
-    : Appender(con, INVALID_CATALOG, DEFAULT_SCHEMA, table_name) {
+Appender::Appender(Connection &con, const string &table_name, const idx_t flush_memory_threshold_p)
+    : Appender(con, INVALID_CATALOG, DEFAULT_SCHEMA, table_name, flush_memory_threshold_p) {
 }
 
 Appender::~Appender() {
@@ -577,12 +582,15 @@ void Appender::ClearColumns() {
 // Query Appender
 //===--------------------------------------------------------------------===//
 QueryAppender::QueryAppender(Connection &con, string query_p, vector<LogicalType> types_p, vector<string> names_p,
-                             string table_name_p)
+                             string table_name_p, const idx_t flush_memory_threshold_p)
     : BaseAppender(Allocator::DefaultAllocator(), AppenderType::LOGICAL), context(con.context),
       query(std::move(query_p)), names(std::move(names_p)), table_name(std::move(table_name_p)) {
 	types = std::move(types_p);
 	InitializeChunk();
 	collection = make_uniq<ColumnDataCollection>(allocator, GetActiveTypes());
+	flush_memory_threshold = (flush_memory_threshold_p == DConstants::INVALID_INDEX)
+	                             ? optional_idx::Invalid()
+	                             : optional_idx(flush_memory_threshold_p);
 }
 
 QueryAppender::~QueryAppender() {
@@ -599,9 +607,13 @@ void QueryAppender::FlushInternal(ColumnDataCollection &collection) {
 //===--------------------------------------------------------------------===//
 // Internal Appender
 //===--------------------------------------------------------------------===//
-InternalAppender::InternalAppender(ClientContext &context_p, TableCatalogEntry &table_p, const idx_t flush_count_p)
+InternalAppender::InternalAppender(ClientContext &context_p, TableCatalogEntry &table_p, const idx_t flush_count_p,
+                                   const idx_t flush_memory_threshold_p)
     : BaseAppender(Allocator::DefaultAllocator(), table_p.GetTypes(), AppenderType::PHYSICAL, flush_count_p),
       context(context_p), table(table_p) {
+	flush_memory_threshold = (flush_memory_threshold_p == DConstants::INVALID_INDEX)
+	                             ? optional_idx::Invalid()
+	                             : optional_idx(flush_memory_threshold_p);
 }
 
 InternalAppender::~InternalAppender() {
@@ -630,4 +642,28 @@ void BaseAppender::Clear() {
 	column = 0;
 }
 
+bool BaseAppender::ShouldFlushChunk() const {
+	if (chunk.size() >= STANDARD_VECTOR_SIZE) {
+		return true;
+	}
+
+	if (!flush_memory_threshold.IsValid()) {
+		return false;
+	}
+
+	return (collection->AllocationSize() >= flush_memory_threshold.GetIndex());
+}
+
+bool BaseAppender::ShouldFlush() const {
+	if (collection->Count() >= flush_count) {
+		return true;
+	}
+
+	if (!flush_memory_threshold.IsValid()) {
+		return false;
+	}
+
+	return (collection->AllocationSize() >= flush_memory_threshold.GetIndex());
+}
+
 } // namespace duckdb
diff --git a/src/main/attached_database.cpp b/src/main/attached_database.cpp
index 3ad8116a40..706b49baae 100644
--- a/src/main/attached_database.cpp
+++ b/src/main/attached_database.cpp
@@ -11,6 +11,7 @@
 #include "duckdb/transaction/duck_transaction_manager.hpp"
 #include "duckdb/main/database_path_and_type.hpp"
 #include "duckdb/main/valid_checker.hpp"
+#include "duckdb/storage/block_allocator.hpp"
 
 namespace duckdb {
 
@@ -274,9 +275,6 @@ void AttachedDatabase::Close() {
 					options.wal_action = CheckpointWALAction::DELETE_WAL;
 					storage->CreateCheckpoint(QueryContext(), options);
 				}
-			} catch (std::exception &ex) { // NOLINT
-				ErrorData data(ex);
-				DUCKDB_LOG_ERROR(db, "AttachedDatabase::Close()\t\t" + data.Message());
 			} catch (...) { // NOLINT
 			}
 		}
diff --git a/src/main/capi/CMakeLists.txt b/src/main/capi/CMakeLists.txt
index 860fe39950..6114633038 100644
--- a/src/main/capi/CMakeLists.txt
+++ b/src/main/capi/CMakeLists.txt
@@ -17,7 +17,6 @@ add_library_unity(
   expression-c.cpp
   helper-c.cpp
   hugeint-c.cpp
-  logging-c.cpp
   logical_types-c.cpp
   pending-c.cpp
   prepared-c.cpp
diff --git a/src/main/capi/arrow-c.cpp b/src/main/capi/arrow-c.cpp
index f3f2f7fe52..f7865bb3f7 100644
--- a/src/main/capi/arrow-c.cpp
+++ b/src/main/capi/arrow-c.cpp
@@ -18,9 +18,14 @@ using duckdb::QueryResultType;
 
 duckdb_error_data duckdb_to_arrow_schema(duckdb_arrow_options arrow_options, duckdb_logical_type *types,
                                          const char **names, idx_t column_count, struct ArrowSchema *out_schema) {
-	if (!types || !names || !arrow_options || !out_schema) {
+	if (!arrow_options || !out_schema) {
 		return duckdb_create_error_data(DUCKDB_ERROR_INVALID_INPUT, "Invalid argument(s) to duckdb_to_arrow_schema");
 	}
+	// types and names can be nullptr when column_count is 0
+	if (column_count > 0 && (!types || !names)) {
+		return duckdb_create_error_data(DUCKDB_ERROR_INVALID_INPUT, "Invalid argument(s) to duckdb_to_arrow_schema");
+	}
+
 	duckdb::vector<LogicalType> schema_types;
 	duckdb::vector<std::string> schema_names;
 	for (idx_t i = 0; i < column_count; i++) {
diff --git a/src/main/capi/cast/from_decimal-c.cpp b/src/main/capi/cast/from_decimal-c.cpp
index e6bc6f98c6..a18d0a4603 100644
--- a/src/main/capi/cast/from_decimal-c.cpp
+++ b/src/main/capi/cast/from_decimal-c.cpp
@@ -13,23 +13,22 @@ bool CastDecimalCInternal(duckdb_result *source, duckdb_string &result, idx_t co
 	auto scale = duckdb::DecimalType::GetScale(source_type);
 	duckdb::Vector result_vec(duckdb::LogicalType::VARCHAR, false, false);
 	duckdb::string_t result_string;
-	void *source_address = UnsafeFetchPtr<hugeint_t>(source, col, row);
+	auto source_value = UnsafeFetch<hugeint_t>(source, col, row);
 	switch (source_type.InternalType()) {
 	case duckdb::PhysicalType::INT16:
-		result_string = duckdb::StringCastFromDecimal::Operation<int16_t>(UnsafeFetchFromPtr<int16_t>(source_address),
-		                                                                  width, scale, result_vec);
+		result_string = duckdb::StringCastFromDecimal::Operation<int16_t>(static_cast<int16_t>(source_value), width,
+		                                                                  scale, result_vec);
 		break;
 	case duckdb::PhysicalType::INT32:
-		result_string = duckdb::StringCastFromDecimal::Operation<int32_t>(UnsafeFetchFromPtr<int32_t>(source_address),
-		                                                                  width, scale, result_vec);
+		result_string = duckdb::StringCastFromDecimal::Operation<int32_t>(static_cast<int32_t>(source_value), width,
+		                                                                  scale, result_vec);
 		break;
 	case duckdb::PhysicalType::INT64:
-		result_string = duckdb::StringCastFromDecimal::Operation<int64_t>(UnsafeFetchFromPtr<int64_t>(source_address),
-		                                                                  width, scale, result_vec);
+		result_string = duckdb::StringCastFromDecimal::Operation<int64_t>(static_cast<int64_t>(source_value), width,
+		                                                                  scale, result_vec);
 		break;
 	case duckdb::PhysicalType::INT128:
-		result_string = duckdb::StringCastFromDecimal::Operation<hugeint_t>(
-		    UnsafeFetchFromPtr<hugeint_t>(source_address), width, scale, result_vec);
+		result_string = duckdb::StringCastFromDecimal::Operation<hugeint_t>(source_value, width, scale, result_vec);
 		break;
 	default:
 		throw duckdb::InternalException("Unimplemented internal type for decimal");
@@ -48,10 +47,11 @@ duckdb_hugeint FetchInternals(void *source_address) {
 
 template <>
 duckdb_hugeint FetchInternals<int16_t>(void *source_address) {
+	const int16_t source_value = static_cast<int16_t>(UnsafeFetchFromPtr<int64_t>(source_address));
 	duckdb_hugeint result;
 	int16_t intermediate_result;
 
-	if (!TryCast::Operation<int16_t, int16_t>(UnsafeFetchFromPtr<int16_t>(source_address), intermediate_result)) {
+	if (!TryCast::Operation<int16_t, int16_t>(source_value, intermediate_result)) {
 		intermediate_result = FetchDefaultValue::Operation<int16_t>();
 	}
 	hugeint_t hugeint_result = Hugeint::Cast<int16_t>(intermediate_result);
@@ -61,10 +61,11 @@ duckdb_hugeint FetchInternals<int16_t>(void *source_address) {
 }
 template <>
 duckdb_hugeint FetchInternals<int32_t>(void *source_address) {
+	const int32_t source_value = static_cast<int32_t>(UnsafeFetchFromPtr<int64_t>(source_address));
 	duckdb_hugeint result;
 	int32_t intermediate_result;
 
-	if (!TryCast::Operation<int32_t, int32_t>(UnsafeFetchFromPtr<int32_t>(source_address), intermediate_result)) {
+	if (!TryCast::Operation<int32_t, int32_t>(source_value, intermediate_result)) {
 		intermediate_result = FetchDefaultValue::Operation<int32_t>();
 	}
 	hugeint_t hugeint_result = Hugeint::Cast<int32_t>(intermediate_result);
@@ -74,10 +75,11 @@ duckdb_hugeint FetchInternals<int32_t>(void *source_address) {
 }
 template <>
 duckdb_hugeint FetchInternals<int64_t>(void *source_address) {
+	const int64_t source_value = UnsafeFetchFromPtr<int64_t>(source_address);
 	duckdb_hugeint result;
 	int64_t intermediate_result;
 
-	if (!TryCast::Operation<int64_t, int64_t>(UnsafeFetchFromPtr<int64_t>(source_address), intermediate_result)) {
+	if (!TryCast::Operation<int64_t, int64_t>(source_value, intermediate_result)) {
 		intermediate_result = FetchDefaultValue::Operation<int64_t>();
 	}
 	hugeint_t hugeint_result = Hugeint::Cast<int64_t>(intermediate_result);
@@ -87,10 +89,11 @@ duckdb_hugeint FetchInternals<int64_t>(void *source_address) {
 }
 template <>
 duckdb_hugeint FetchInternals<hugeint_t>(void *source_address) {
+	const hugeint_t source_value = UnsafeFetchFromPtr<hugeint_t>(source_address);
 	duckdb_hugeint result;
 	hugeint_t intermediate_result;
 
-	if (!TryCast::Operation<hugeint_t, hugeint_t>(UnsafeFetchFromPtr<hugeint_t>(source_address), intermediate_result)) {
+	if (!TryCast::Operation<hugeint_t, hugeint_t>(source_value, intermediate_result)) {
 		intermediate_result = FetchDefaultValue::Operation<hugeint_t>();
 	}
 	result.lower = intermediate_result.lower;
diff --git a/src/main/capi/logging-c.cpp b/src/main/capi/logging-c.cpp
deleted file mode 100644
index 3f259d8123..0000000000
--- a/src/main/capi/logging-c.cpp
+++ /dev/null
@@ -1,85 +0,0 @@
-#include "duckdb/main/capi/capi_internal.hpp"
-#include "duckdb/logging/log_storage.hpp"
-
-namespace duckdb {
-
-class CallbackLogStorage : public LogStorage {
-public:
-	CallbackLogStorage(const string &name, duckdb_logger_write_log_entry_t write_log_entry_fun)
-	    : name(name), write_log_entry_fun(write_log_entry_fun) {
-	}
-
-	void WriteLogEntry(timestamp_t timestamp, LogLevel level, const string &log_type, const string &log_message,
-	                   const RegisteredLoggingContext &context) override {
-		if (write_log_entry_fun == nullptr) {
-			return;
-		}
-
-		auto c_timestamp = reinterpret_cast<duckdb_timestamp *>(&timestamp);
-		write_log_entry_fun(*c_timestamp, EnumUtil::ToChars(level), log_type.c_str(), log_message.c_str());
-	};
-
-	void WriteLogEntries(DataChunk &chunk, const RegisteredLoggingContext &context) override {};
-
-	void Flush(LoggingTargetTable table) override {};
-
-	void FlushAll() override {};
-
-	bool IsEnabled(LoggingTargetTable table) override {
-		return true;
-	}
-
-	const string GetStorageName() override {
-		return name;
-	}
-
-private:
-	const string name;
-	duckdb_logger_write_log_entry_t write_log_entry_fun;
-};
-
-struct LogStorageWrapper {
-	duckdb_logger_write_log_entry_t write_log_entry;
-};
-
-} // namespace duckdb
-
-using duckdb::DatabaseWrapper;
-using duckdb::LogStorageWrapper;
-
-duckdb_log_storage duckdb_create_log_storage() {
-	auto clog_storage = new LogStorageWrapper();
-	return reinterpret_cast<duckdb_log_storage>(clog_storage);
-}
-
-void duckdb_destroy_log_storage(duckdb_log_storage storage) {
-	if (!storage) {
-		return;
-	}
-	auto clog_storage = reinterpret_cast<LogStorageWrapper *>(storage);
-	delete clog_storage;
-}
-
-void duckdb_log_storage_set_write_log_entry(duckdb_log_storage storage, duckdb_logger_write_log_entry_t function) {
-	if (!storage || !function) {
-		return;
-	}
-
-	auto clog_storage = reinterpret_cast<LogStorageWrapper *>(storage);
-	clog_storage->write_log_entry = function;
-}
-
-void duckdb_register_log_storage(duckdb_database database, const char *name, duckdb_log_storage storage) {
-	if (!database || !name || !storage) {
-		return;
-	}
-
-	const auto db_wrapper = reinterpret_cast<DatabaseWrapper *>(database);
-	auto cast_storage = reinterpret_cast<LogStorageWrapper *>(storage);
-
-	const auto &db = *db_wrapper->database;
-
-	auto shared_storage_ptr = duckdb::make_shared_ptr<duckdb::CallbackLogStorage>(name, cast_storage->write_log_entry);
-	duckdb::shared_ptr<duckdb::LogStorage> storage_ptr = shared_storage_ptr;
-	db.instance->GetLogManager().RegisterLogStorage(name, storage_ptr);
-}
diff --git a/src/main/config.cpp b/src/main/config.cpp
index a1878da94f..08d8701774 100644
--- a/src/main/config.cpp
+++ b/src/main/config.cpp
@@ -78,6 +78,7 @@ static const ConfigurationOption internal_options[] = {
     DUCKDB_GLOBAL(AutoinstallExtensionRepositorySetting),
     DUCKDB_GLOBAL(AutoinstallKnownExtensionsSetting),
     DUCKDB_GLOBAL(AutoloadKnownExtensionsSetting),
+    DUCKDB_GLOBAL(BlockAllocatorMemorySetting),
     DUCKDB_SETTING(CatalogErrorMaxSchemasSetting),
     DUCKDB_GLOBAL(CheckpointThresholdSetting),
     DUCKDB_GLOBAL(CustomExtensionRepositorySetting),
@@ -183,12 +184,12 @@ static const ConfigurationOption internal_options[] = {
     DUCKDB_GLOBAL(ZstdMinStringLengthSetting),
     FINAL_SETTING};
 
-static const ConfigurationAlias setting_aliases[] = {DUCKDB_SETTING_ALIAS("memory_limit", 86),
-                                                     DUCKDB_SETTING_ALIAS("null_order", 36),
-                                                     DUCKDB_SETTING_ALIAS("profiling_output", 105),
-                                                     DUCKDB_SETTING_ALIAS("user", 120),
-                                                     DUCKDB_SETTING_ALIAS("wal_autocheckpoint", 21),
-                                                     DUCKDB_SETTING_ALIAS("worker_threads", 119),
+static const ConfigurationAlias setting_aliases[] = {DUCKDB_SETTING_ALIAS("memory_limit", 87),
+                                                     DUCKDB_SETTING_ALIAS("null_order", 37),
+                                                     DUCKDB_SETTING_ALIAS("profiling_output", 106),
+                                                     DUCKDB_SETTING_ALIAS("user", 121),
+                                                     DUCKDB_SETTING_ALIAS("wal_autocheckpoint", 22),
+                                                     DUCKDB_SETTING_ALIAS("worker_threads", 120),
                                                      FINAL_ALIAS};
 
 vector<ConfigurationOption> DBConfig::GetOptions() {
diff --git a/src/main/database.cpp b/src/main/database.cpp
index bb36e25dec..6e68d4e8d8 100644
--- a/src/main/database.cpp
+++ b/src/main/database.cpp
@@ -23,6 +23,7 @@
 #include "duckdb/storage/object_cache.hpp"
 #include "duckdb/storage/standard_buffer_manager.hpp"
 #include "duckdb/storage/storage_extension.hpp"
+#include "duckdb/storage/block_allocator.hpp"
 #include "duckdb/storage/storage_manager.hpp"
 #include "duckdb/transaction/transaction_manager.hpp"
 #include "duckdb/main/capi/extension_api.hpp"
@@ -93,9 +94,7 @@ DatabaseInstance::~DatabaseInstance() {
 	buffer_manager.reset();
 
 	// flush allocations and disable the background thread
-	if (Allocator::SupportsFlush()) {
-		Allocator::FlushAll();
-	}
+	config.block_allocator->FlushAll();
 	Allocator::SetBackgroundThreads(false);
 	// after all destruction is complete clear the cache entry
 	config.db_cache_entry.reset();
@@ -462,6 +461,9 @@ void DatabaseInstance::Configure(DBConfig &new_config, const char *database_path
 	if (!config.allocator) {
 		config.allocator = make_uniq<Allocator>();
 	}
+	config.block_allocator = make_uniq<BlockAllocator>(*config.allocator, config.options.default_block_alloc_size,
+	                                                   DBConfig::GetSystemAvailableMemory(*config.file_system) * 8 / 10,
+	                                                   config.options.block_allocator_size);
 	config.replacement_scans = std::move(new_config.replacement_scans);
 	config.parser_extensions = std::move(new_config.parser_extensions);
 	config.error_manager = std::move(new_config.error_manager);
@@ -474,7 +476,7 @@ void DatabaseInstance::Configure(DBConfig &new_config, const char *database_path
 	if (new_config.buffer_pool) {
 		config.buffer_pool = std::move(new_config.buffer_pool);
 	} else {
-		config.buffer_pool = make_shared_ptr<BufferPool>(config.options.maximum_memory,
+		config.buffer_pool = make_shared_ptr<BufferPool>(*config.block_allocator, config.options.maximum_memory,
 		                                                 config.options.buffer_manager_track_eviction_timestamps,
 		                                                 config.options.allocator_bulk_deallocation_flush_threshold);
 	}
@@ -512,12 +514,18 @@ SettingLookupResult DatabaseInstance::TryGetCurrentSetting(const string &key, Va
 	return db_config.TryGetCurrentSetting(key, result);
 }
 
-shared_ptr<EncryptionUtil> DatabaseInstance::GetEncryptionUtil() const {
+shared_ptr<EncryptionUtil> DatabaseInstance::GetEncryptionUtil() {
+	if (!config.encryption_util || !config.encryption_util->SupportsEncryption()) {
+		ExtensionHelper::TryAutoLoadExtension(*this, "httpfs");
+	}
+
 	if (config.encryption_util) {
 		return config.encryption_util;
 	}
 
-	return make_shared_ptr<duckdb_mbedtls::MbedTlsWrapper::AESStateMBEDTLSFactory>();
+	auto result = make_shared_ptr<duckdb_mbedtls::MbedTlsWrapper::AESStateMBEDTLSFactory>();
+
+	return std::move(result);
 }
 
 ValidChecker &DatabaseInstance::GetValidChecker() {
diff --git a/src/main/http/http_util.cpp b/src/main/http/http_util.cpp
index 554346489d..fb5a9491fc 100644
--- a/src/main/http/http_util.cpp
+++ b/src/main/http/http_util.cpp
@@ -130,9 +130,12 @@ BaseRequest::BaseRequest(RequestType type, const string &url, const HTTPHeaders
 class HTTPLibClient : public HTTPClient {
 public:
 	HTTPLibClient(HTTPParams &http_params, const string &proto_host_port) {
+		client = make_uniq<duckdb_httplib::Client>(proto_host_port);
+		Initialize(http_params);
+	}
+	void Initialize(HTTPParams &http_params) override {
 		auto sec = static_cast<time_t>(http_params.timeout);
 		auto usec = static_cast<time_t>(http_params.timeout_usec);
-		client = make_uniq<duckdb_httplib::Client>(proto_host_port);
 		client->set_follow_location(http_params.follow_location);
 		client->set_keep_alive(http_params.keep_alive);
 		client->set_write_timeout(sec, usec);
@@ -228,12 +231,27 @@ unique_ptr<HTTPResponse> HTTPUtil::SendRequest(BaseRequest &request, unique_ptr<
 
 	std::function<unique_ptr<HTTPResponse>(void)> on_request([&]() {
 		unique_ptr<HTTPResponse> response;
+
+		// When logging is enabled, we collect request timings
+		if (request.params.logger) {
+			request.have_request_timing = request.params.logger->ShouldLog(HTTPLogType::NAME, HTTPLogType::LEVEL);
+		}
+
 		try {
+			if (request.have_request_timing) {
+				request.request_start = Timestamp::GetCurrentTimestamp();
+			}
 			response = client->Request(request);
 		} catch (...) {
+			if (request.have_request_timing) {
+				request.request_end = Timestamp::GetCurrentTimestamp();
+			}
 			LogRequest(request, nullptr);
 			throw;
 		}
+		if (request.have_request_timing) {
+			request.request_end = Timestamp::GetCurrentTimestamp();
+		}
 		LogRequest(request, response ? response.get() : nullptr);
 		return response;
 	});
diff --git a/src/main/relation.cpp b/src/main/relation.cpp
index b9e4d50ff4..9bd687b827 100644
--- a/src/main/relation.cpp
+++ b/src/main/relation.cpp
@@ -241,7 +241,12 @@ BoundStatement Relation::Bind(Binder &binder) {
 }
 
 shared_ptr<Relation> Relation::InsertRel(const string &schema_name, const string &table_name) {
-	return make_shared_ptr<InsertRelation>(shared_from_this(), schema_name, table_name);
+	return InsertRel(INVALID_CATALOG, schema_name, table_name);
+}
+
+shared_ptr<Relation> Relation::InsertRel(const string &catalog_name, const string &schema_name,
+                                         const string &table_name) {
+	return make_shared_ptr<InsertRelation>(shared_from_this(), catalog_name, schema_name, table_name);
 }
 
 void Relation::Insert(const string &table_name) {
@@ -249,7 +254,11 @@ void Relation::Insert(const string &table_name) {
 }
 
 void Relation::Insert(const string &schema_name, const string &table_name) {
-	auto insert = InsertRel(schema_name, table_name);
+	Insert(INVALID_CATALOG, schema_name, table_name);
+}
+
+void Relation::Insert(const string &catalog_name, const string &schema_name, const string &table_name) {
+	auto insert = InsertRel(catalog_name, schema_name, table_name);
 	auto res = insert->Execute();
 	if (res->HasError()) {
 		const string prepended_message = "Failed to insert into table '" + table_name + "': ";
@@ -258,30 +267,37 @@ void Relation::Insert(const string &schema_name, const string &table_name) {
 }
 
 void Relation::Insert(const vector<vector<Value>> &values) {
-	vector<string> column_names;
-	auto rel = make_shared_ptr<ValueRelation>(context->GetContext(), values, std::move(column_names), "values");
-	rel->Insert(GetAlias());
+	throw InvalidInputException("INSERT with values can only be used on base tables!");
 }
 
 void Relation::Insert(vector<vector<unique_ptr<ParsedExpression>>> &&expressions) {
-	vector<string> column_names;
-	auto rel = make_shared_ptr<ValueRelation>(context->GetContext(), std::move(expressions), std::move(column_names),
-	                                          "values");
-	rel->Insert(GetAlias());
+	(void)std::move(expressions);
+	throw InvalidInputException("INSERT with expressions can only be used on base tables!");
 }
 
 shared_ptr<Relation> Relation::CreateRel(const string &schema_name, const string &table_name, bool temporary,
                                          OnCreateConflict on_conflict) {
-	return make_shared_ptr<CreateTableRelation>(shared_from_this(), schema_name, table_name, temporary, on_conflict);
+	return CreateRel(INVALID_CATALOG, schema_name, table_name, temporary, on_conflict);
+}
+
+shared_ptr<Relation> Relation::CreateRel(const string &catalog_name, const string &schema_name,
+                                         const string &table_name, bool temporary, OnCreateConflict on_conflict) {
+	return make_shared_ptr<CreateTableRelation>(shared_from_this(), catalog_name, schema_name, table_name, temporary,
+	                                            on_conflict);
 }
 
 void Relation::Create(const string &table_name, bool temporary, OnCreateConflict on_conflict) {
-	Create(INVALID_SCHEMA, table_name, temporary, on_conflict);
+	Create(INVALID_CATALOG, INVALID_SCHEMA, table_name, temporary, on_conflict);
 }
 
 void Relation::Create(const string &schema_name, const string &table_name, bool temporary,
                       OnCreateConflict on_conflict) {
-	auto create = CreateRel(schema_name, table_name, temporary, on_conflict);
+	Create(INVALID_CATALOG, schema_name, table_name, temporary, on_conflict);
+}
+
+void Relation::Create(const string &catalog_name, const string &schema_name, const string &table_name, bool temporary,
+                      OnCreateConflict on_conflict) {
+	auto create = CreateRel(catalog_name, schema_name, table_name, temporary, on_conflict);
 	auto res = create->Execute();
 	if (res->HasError()) {
 		const string prepended_message = "Failed to create table '" + table_name + "': ";
diff --git a/src/main/relation/create_table_relation.cpp b/src/main/relation/create_table_relation.cpp
index 39aa65e369..2a08194c0c 100644
--- a/src/main/relation/create_table_relation.cpp
+++ b/src/main/relation/create_table_relation.cpp
@@ -14,12 +14,21 @@ CreateTableRelation::CreateTableRelation(shared_ptr<Relation> child_p, string sc
 	TryBindRelation(columns);
 }
 
+CreateTableRelation::CreateTableRelation(shared_ptr<Relation> child_p, string catalog_name, string schema_name,
+                                         string table_name, bool temporary_p, OnCreateConflict on_conflict)
+    : Relation(child_p->context, RelationType::CREATE_TABLE_RELATION), child(std::move(child_p)),
+      catalog_name(std::move(catalog_name)), schema_name(std::move(schema_name)), table_name(std::move(table_name)),
+      temporary(temporary_p), on_conflict(on_conflict) {
+	TryBindRelation(columns);
+}
+
 BoundStatement CreateTableRelation::Bind(Binder &binder) {
 	auto select = make_uniq<SelectStatement>();
 	select->node = child->GetQueryNode();
 
 	CreateStatement stmt;
 	auto info = make_uniq<CreateTableInfo>();
+	info->catalog = catalog_name;
 	info->schema = schema_name;
 	info->table = table_name;
 	info->query = std::move(select);
diff --git a/src/main/relation/insert_relation.cpp b/src/main/relation/insert_relation.cpp
index 84ef16ec6e..461133255c 100644
--- a/src/main/relation/insert_relation.cpp
+++ b/src/main/relation/insert_relation.cpp
@@ -13,11 +13,18 @@ InsertRelation::InsertRelation(shared_ptr<Relation> child_p, string schema_name,
 	TryBindRelation(columns);
 }
 
+InsertRelation::InsertRelation(shared_ptr<Relation> child_p, string catalog_name, string schema_name, string table_name)
+    : Relation(child_p->context, RelationType::INSERT_RELATION), child(std::move(child_p)),
+      catalog_name(std::move(catalog_name)), schema_name(std::move(schema_name)), table_name(std::move(table_name)) {
+	TryBindRelation(columns);
+}
+
 BoundStatement InsertRelation::Bind(Binder &binder) {
 	InsertStatement stmt;
 	auto select = make_uniq<SelectStatement>();
 	select->node = child->GetQueryNode();
 
+	stmt.catalog = catalog_name;
 	stmt.schema = schema_name;
 	stmt.table = table_name;
 	stmt.select_statement = std::move(select);
diff --git a/src/main/relation/table_relation.cpp b/src/main/relation/table_relation.cpp
index c82ace698b..78d5aaaa4e 100644
--- a/src/main/relation/table_relation.cpp
+++ b/src/main/relation/table_relation.cpp
@@ -3,6 +3,7 @@
 #include "duckdb/parser/query_node/select_node.hpp"
 #include "duckdb/parser/expression/star_expression.hpp"
 #include "duckdb/main/relation/delete_relation.hpp"
+#include "duckdb/main/relation/value_relation.hpp"
 #include "duckdb/main/relation/update_relation.hpp"
 #include "duckdb/parser/parser.hpp"
 #include "duckdb/main/client_context.hpp"
@@ -87,4 +88,17 @@ void TableRelation::Delete(const string &condition) {
 	del->Execute();
 }
 
+void TableRelation::Insert(const vector<vector<Value>> &values) {
+	vector<string> column_names;
+	auto rel = make_shared_ptr<ValueRelation>(context->GetContext(), values, std::move(column_names), "values");
+	rel->Insert(description->database, description->schema, description->table);
+}
+
+void TableRelation::Insert(vector<vector<unique_ptr<ParsedExpression>>> &&expressions) {
+	vector<string> column_names;
+	auto rel = make_shared_ptr<ValueRelation>(context->GetContext(), std::move(expressions), std::move(column_names),
+	                                          "values");
+	rel->Insert(description->database, description->schema, description->table);
+}
+
 } // namespace duckdb
diff --git a/src/main/secret/secret_manager.cpp b/src/main/secret/secret_manager.cpp
index 8788b595b9..7b132baf8b 100644
--- a/src/main/secret/secret_manager.cpp
+++ b/src/main/secret/secret_manager.cpp
@@ -49,7 +49,10 @@ void SecretManager::Initialize(DatabaseInstance &db) {
 	for (auto &path_ele : path_components) {
 		config.default_secret_path = fs.JoinPath(config.default_secret_path, path_ele);
 	}
-	config.secret_path = config.default_secret_path;
+	// Use default path if none has been specified by the user configuration
+	if (config.secret_path.empty()) {
+		config.secret_path = config.default_secret_path;
+	}
 
 	// Set the defaults for persistent storage
 	config.default_persistent_storage = LOCAL_FILE_STORAGE_NAME;
diff --git a/src/main/settings/custom_settings.cpp b/src/main/settings/custom_settings.cpp
index 377a1a5b55..ad10f57a89 100644
--- a/src/main/settings/custom_settings.cpp
+++ b/src/main/settings/custom_settings.cpp
@@ -14,6 +14,7 @@
 #include "duckdb/common/enums/access_mode.hpp"
 #include "duckdb/catalog/catalog_search_path.hpp"
 #include "duckdb/common/string_util.hpp"
+#include "duckdb/common/operator/double_cast_operator.hpp"
 #include "duckdb/main/attached_database.hpp"
 #include "duckdb/main/client_context.hpp"
 #include "duckdb/main/client_data.hpp"
@@ -31,6 +32,7 @@
 #include "duckdb/storage/storage_manager.hpp"
 #include "duckdb/logging/logger.hpp"
 #include "duckdb/logging/log_manager.hpp"
+#include "duckdb/storage/block_allocator.hpp"
 
 namespace duckdb {
 
@@ -314,6 +316,41 @@ Value AllowedPathsSetting::GetSetting(const ClientContext &context) {
 	return Value::LIST(LogicalType::VARCHAR, std::move(allowed_paths));
 }
 
+//===----------------------------------------------------------------------===//
+// Block Allocator Memory
+//===----------------------------------------------------------------------===//
+void BlockAllocatorMemorySetting::SetGlobal(DatabaseInstance *db, DBConfig &config, const Value &input) {
+	const auto input_string = input.ToString();
+	idx_t size;
+	if (!input_string.empty() && input_string.back() == '%') {
+		double percentage;
+		if (!TryDoubleCast(input_string.c_str(), input_string.size() - 1, percentage, false) || percentage < 0 ||
+		    percentage > 100) {
+			throw InvalidInputException("Unable to parse valid percentage (input: %s)", input_string);
+		}
+		size = LossyNumericCast<idx_t>(percentage) * config.options.maximum_memory / 100;
+	} else {
+		size = DBConfig::ParseMemoryLimit(input_string);
+	}
+	if (db) {
+		BlockAllocator::Get(*db).Resize(size);
+	}
+	config.options.block_allocator_size = size;
+}
+
+void BlockAllocatorMemorySetting::ResetGlobal(DatabaseInstance *db, DBConfig &config) {
+	const auto size = DBConfigOptions().block_allocator_size;
+	if (db) {
+		BlockAllocator::Get(*db).Resize(size);
+	}
+	config.options.block_allocator_size = size;
+}
+
+Value BlockAllocatorMemorySetting::GetSetting(const ClientContext &context) {
+	auto &config = DBConfig::GetConfig(context);
+	return StringUtil::BytesToHumanReadableString(config.options.block_allocator_size);
+}
+
 //===----------------------------------------------------------------------===//
 // Checkpoint Threshold
 //===----------------------------------------------------------------------===//
diff --git a/src/optimizer/topn_optimizer.cpp b/src/optimizer/topn_optimizer.cpp
index 6b8b68ae66..eb6d22de27 100644
--- a/src/optimizer/topn_optimizer.cpp
+++ b/src/optimizer/topn_optimizer.cpp
@@ -5,8 +5,10 @@
 #include "duckdb/planner/operator/logical_limit.hpp"
 #include "duckdb/planner/operator/logical_order.hpp"
 #include "duckdb/planner/operator/logical_top_n.hpp"
+#include "duckdb/planner/filter/conjunction_filter.hpp"
 #include "duckdb/planner/filter/constant_filter.hpp"
 #include "duckdb/planner/filter/dynamic_filter.hpp"
+#include "duckdb/planner/filter/null_filter.hpp"
 #include "duckdb/planner/filter/optional_filter.hpp"
 #include "duckdb/execution/operator/join/join_filter_pushdown.hpp"
 #include "duckdb/optimizer/join_filter_pushdown_optimizer.hpp"
@@ -104,11 +106,7 @@ bool TopN::CanOptimize(LogicalOperator &op, optional_ptr<ClientContext> context)
 
 void TopN::PushdownDynamicFilters(LogicalTopN &op) {
 	// pushdown dynamic filters through the Top-N operator
-	if (op.orders[0].null_order == OrderByNullType::NULLS_FIRST) {
-		// FIXME: not supported for NULLS FIRST quite yet
-		// we can support NULLS FIRST by doing (x IS NULL) OR [boundary value]
-		return;
-	}
+	bool nulls_first = op.orders[0].null_order == OrderByNullType::NULLS_FIRST;
 	auto &type = op.orders[0].expression->return_type;
 	if (!TypeIsIntegral(type.InternalType()) && type.id() != LogicalTypeId::VARCHAR) {
 		// only supported for integral types currently
@@ -155,8 +153,8 @@ void TopN::PushdownDynamicFilters(LogicalTopN &op) {
 	op.dynamic_filter = filter_data;
 
 	bool use_limit = false;
-	bool use_custom_rowgroup_order =
-	    CanReorderRowGroups(op, use_limit) && (colref.return_type.IsNumeric() || colref.return_type.IsTemporal());
+	bool use_custom_rowgroup_order = !nulls_first && CanReorderRowGroups(op, use_limit) &&
+	                                 (colref.return_type.IsNumeric() || colref.return_type.IsTemporal());
 
 	for (auto &target : pushdown_targets) {
 		auto &get = target.get;
@@ -165,7 +163,14 @@ void TopN::PushdownDynamicFilters(LogicalTopN &op) {
 
 		// create the actual dynamic filter
 		auto dynamic_filter = make_uniq<DynamicFilter>(filter_data);
-		auto optional_filter = make_uniq<OptionalFilter>(std::move(dynamic_filter));
+		unique_ptr<TableFilter> pushed_filter = std::move(dynamic_filter);
+		if (nulls_first) {
+			auto or_filter = make_uniq<ConjunctionOrFilter>();
+			or_filter->child_filters.push_back(make_uniq<IsNullFilter>());
+			or_filter->child_filters.push_back(std::move(pushed_filter));
+			pushed_filter = std::move(or_filter);
+		}
+		auto optional_filter = make_uniq<OptionalFilter>(std::move(pushed_filter));
 
 		// push the filter into the table scan
 		auto &column_index = get.GetColumnIds()[col_idx];
diff --git a/src/parallel/async_result.cpp b/src/parallel/async_result.cpp
index a32086b846..394ab140a3 100644
--- a/src/parallel/async_result.cpp
+++ b/src/parallel/async_result.cpp
@@ -180,6 +180,8 @@ AsyncResultsExecutionMode
 AsyncResult::ConvertToAsyncResultExecutionMode(const PhysicalTableScanExecutionStrategy &execution_mode) {
 	switch (execution_mode) {
 	case PhysicalTableScanExecutionStrategy::DEFAULT:
+		// FIXME: Once PositionalJoin logic has been fixed, this needs to revert to be TASK_EXECUTOR by default
+		return AsyncResultsExecutionMode::SYNCHRONOUS;
 	case PhysicalTableScanExecutionStrategy::TASK_EXECUTOR:
 	case PhysicalTableScanExecutionStrategy::TASK_EXECUTOR_BUT_FORCE_SYNC_CHECKS:
 		return AsyncResultsExecutionMode::TASK_EXECUTOR;
diff --git a/src/parallel/task_scheduler.cpp b/src/parallel/task_scheduler.cpp
index 9d8f94b651..b5ed2db248 100644
--- a/src/parallel/task_scheduler.cpp
+++ b/src/parallel/task_scheduler.cpp
@@ -5,6 +5,7 @@
 #include "duckdb/common/numeric_utils.hpp"
 #include "duckdb/main/client_context.hpp"
 #include "duckdb/main/database.hpp"
+#include "duckdb/storage/block_allocator.hpp"
 #ifndef DUCKDB_NO_THREADS
 #include "concurrentqueue.h"
 #include "duckdb/common/thread.hpp"
@@ -270,17 +271,19 @@ void TaskScheduler::ExecuteForever(atomic<bool> *marker) {
 #ifndef DUCKDB_NO_THREADS
 	static constexpr const int64_t INITIAL_FLUSH_WAIT = 500000; // initial wait time of 0.5s (in mus) before flushing
 
-	auto &config = DBConfig::GetConfig(db);
+	const auto &block_allocator = BlockAllocator::Get(db);
+	const auto &config = DBConfig::GetConfig(db);
+
 	shared_ptr<Task> task;
 	// loop until the marker is set to false
 	while (*marker) {
-		if (!Allocator::SupportsFlush()) {
+		if (!block_allocator.SupportsFlush()) {
 			// allocator can't flush, just start an untimed wait
 			queue->semaphore.wait();
 		} else if (!queue->semaphore.wait(INITIAL_FLUSH_WAIT)) {
 			// allocator can flush, we flush this threads outstanding allocations after it was idle for 0.5s
-			Allocator::ThreadFlush(allocator_background_threads, allocator_flush_threshold,
-			                       NumericCast<idx_t>(requested_thread_count.load()));
+			block_allocator.ThreadFlush(allocator_background_threads, allocator_flush_threshold,
+			                            NumericCast<idx_t>(requested_thread_count.load()));
 			auto decay_delay = Allocator::DecayDelay();
 			if (!decay_delay.IsValid()) {
 				// no decay delay specified - just wait
@@ -322,8 +325,8 @@ void TaskScheduler::ExecuteForever(atomic<bool> *marker) {
 		}
 	}
 	// this thread will exit, flush all of its outstanding allocations
-	if (Allocator::SupportsFlush()) {
-		Allocator::ThreadFlush(allocator_background_threads, 0, NumericCast<idx_t>(requested_thread_count.load()));
+	if (block_allocator.SupportsFlush()) {
+		block_allocator.ThreadFlush(allocator_background_threads, 0, NumericCast<idx_t>(requested_thread_count.load()));
 		Allocator::ThreadIdle();
 	}
 #else
@@ -563,9 +566,7 @@ void TaskScheduler::RelaunchThreadsInternal(int32_t n) {
 		}
 	}
 	current_thread_count = NumericCast<int32_t>(threads.size() + config.options.external_threads);
-	if (Allocator::SupportsFlush()) {
-		Allocator::FlushAll();
-	}
+	BlockAllocator::Get(db).FlushAll();
 #endif
 }
 
diff --git a/src/parser/transform/expression/transform_cast.cpp b/src/parser/transform/expression/transform_cast.cpp
index a4b1dde59b..0412a3c969 100644
--- a/src/parser/transform/expression/transform_cast.cpp
+++ b/src/parser/transform/expression/transform_cast.cpp
@@ -21,7 +21,9 @@ unique_ptr<ParsedExpression> Transformer::TransformTypeCast(duckdb_libpgquery::P
 				parameters.query_location = NumericCast<idx_t>(root.location);
 			}
 			auto blob_data = Blob::ToBlob(string(c->val.val.str), parameters);
-			return make_uniq<ConstantExpression>(Value::BLOB_RAW(blob_data));
+			auto result = make_uniq<ConstantExpression>(Value::BLOB_RAW(blob_data));
+			SetQueryLocation(*result, root.location);
+			return std::move(result);
 		}
 	}
 	// transform the expression node
diff --git a/src/planner/binder/query_node/plan_select_node.cpp b/src/planner/binder/query_node/plan_select_node.cpp
index 10b206f245..335ccdb1f7 100644
--- a/src/planner/binder/query_node/plan_select_node.cpp
+++ b/src/planner/binder/query_node/plan_select_node.cpp
@@ -28,7 +28,7 @@ unique_ptr<LogicalOperator> Binder::CreatePlan(BoundSelectNode &statement) {
 		root = PlanFilter(std::move(statement.where_clause), std::move(root));
 	}
 
-	if (!statement.aggregates.empty() || !statement.groups.group_expressions.empty()) {
+	if (!statement.aggregates.empty() || !statement.groups.group_expressions.empty() || statement.having) {
 		if (!statement.groups.group_expressions.empty()) {
 			// visit the groups
 			for (auto &group : statement.groups.group_expressions) {
diff --git a/src/planner/binder/statement/bind_merge_into.cpp b/src/planner/binder/statement/bind_merge_into.cpp
index 1280d492b2..ab5ad702ac 100644
--- a/src/planner/binder/statement/bind_merge_into.cpp
+++ b/src/planner/binder/statement/bind_merge_into.cpp
@@ -40,10 +40,20 @@ unique_ptr<BoundMergeIntoAction> Binder::BindMergeAction(LogicalMergeInto &merge
 	auto result = make_uniq<BoundMergeIntoAction>();
 	result->action_type = action.action_type;
 	if (action.condition) {
-		ProjectionBinder proj_binder(*this, context, proj_index, expressions, "WHERE clause");
-		proj_binder.target_type = LogicalType::BOOLEAN;
-		auto cond = proj_binder.Bind(action.condition);
-		result->condition = std::move(cond);
+		if (action.condition->HasSubquery()) {
+			// if we have a subquery we need to execute the condition outside of the MERGE INTO statement
+			WhereBinder where_binder(*this, context);
+			auto cond = where_binder.Bind(action.condition);
+			PlanSubqueries(cond, root);
+			result->condition =
+			    make_uniq<BoundColumnRefExpression>(cond->return_type, ColumnBinding(proj_index, expressions.size()));
+			expressions.push_back(std::move(cond));
+		} else {
+			ProjectionBinder proj_binder(*this, context, proj_index, expressions, "WHERE clause");
+			proj_binder.target_type = LogicalType::BOOLEAN;
+			auto cond = proj_binder.Bind(action.condition);
+			result->condition = std::move(cond);
+		}
 	}
 	switch (action.action_type) {
 	case MergeActionType::MERGE_UPDATE: {
diff --git a/src/storage/CMakeLists.txt b/src/storage/CMakeLists.txt
index ee5382489b..1764bd4637 100644
--- a/src/storage/CMakeLists.txt
+++ b/src/storage/CMakeLists.txt
@@ -15,6 +15,7 @@ add_library_unity(
   checkpoint_manager.cpp
   temporary_memory_manager.cpp
   block.cpp
+  block_allocator.cpp
   data_pointer.cpp
   data_table.cpp
   external_file_cache.cpp
diff --git a/src/storage/block.cpp b/src/storage/block.cpp
index 7262277e5a..d45c2584e8 100644
--- a/src/storage/block.cpp
+++ b/src/storage/block.cpp
@@ -4,16 +4,16 @@
 
 namespace duckdb {
 
-Block::Block(Allocator &allocator, const block_id_t id, const idx_t block_size, const idx_t block_header_size)
+Block::Block(BlockAllocator &allocator, const block_id_t id, const idx_t block_size, const idx_t block_header_size)
     : FileBuffer(allocator, FileBufferType::BLOCK, block_size, block_header_size), id(id) {
 }
 
-Block::Block(Allocator &allocator, block_id_t id, uint32_t internal_size, idx_t block_header_size)
+Block::Block(BlockAllocator &allocator, block_id_t id, uint32_t internal_size, idx_t block_header_size)
     : FileBuffer(allocator, FileBufferType::BLOCK, internal_size, block_header_size), id(id) {
 	D_ASSERT((AllocSize() & (Storage::SECTOR_SIZE - 1)) == 0);
 }
 
-Block::Block(Allocator &allocator, block_id_t id, BlockManager &block_manager)
+Block::Block(BlockAllocator &allocator, block_id_t id, BlockManager &block_manager)
     : FileBuffer(allocator, FileBufferType::BLOCK, block_manager), id(id) {
 	D_ASSERT((AllocSize() & (Storage::SECTOR_SIZE - 1)) == 0);
 }
diff --git a/src/storage/block_allocator.cpp b/src/storage/block_allocator.cpp
new file mode 100644
index 0000000000..80975cc21d
--- /dev/null
+++ b/src/storage/block_allocator.cpp
@@ -0,0 +1,391 @@
+#include "duckdb/storage/block_allocator.hpp"
+
+#include "duckdb/common/allocator.hpp"
+#include "duckdb/main/attached_database.hpp"
+#include "duckdb/main/database.hpp"
+#include "duckdb/parallel/concurrentqueue.hpp"
+#include "duckdb/common/types/uuid.hpp"
+
+#if defined(_WIN32)
+#include "duckdb/common/windows.hpp"
+#else
+#include <sys/mman.h>
+#endif
+
+namespace duckdb {
+
+//===--------------------------------------------------------------------===//
+// Memory Helpers
+//===--------------------------------------------------------------------===//
+static data_ptr_t AllocateVirtualMemory(const idx_t size) {
+#if INTPTR_MAX == INT32_MAX
+	// Disable on 32-bit
+	return nullptr;
+#endif
+
+#if defined(_WIN32)
+	// This returns nullptr on failure
+	return data_ptr_t(VirtualAlloc(nullptr, size, MEM_RESERVE, PAGE_NOACCESS));
+#else
+	const auto ptr = mmap(nullptr, size, PROT_READ | PROT_WRITE, MAP_PRIVATE | MAP_ANONYMOUS, -1, 0);
+	return ptr == MAP_FAILED ? nullptr : data_ptr_cast(ptr);
+#endif
+}
+
+static void FreeVirtualMemory(const data_ptr_t pointer, const idx_t size) {
+	bool success;
+#if defined(_WIN32)
+	success = VirtualFree(pointer, 0, MEM_RELEASE);
+#else
+	success = munmap(pointer, size) == 0;
+#endif
+	if (!success) {
+		throw InternalException("FreeVirtualMemory failed");
+	}
+}
+
+static void OnFirstAllocation(const data_ptr_t pointer, const idx_t size) {
+	bool success = true;
+#if defined(_WIN32)
+	success = VirtualAlloc(pointer, size, MEM_COMMIT, PAGE_READWRITE);
+#elif defined(__APPLE__)
+	// Nothing to do here
+#else
+	// Pre-fault the memory
+	for (idx_t i = 0; i < size; i += 4096) {
+		pointer[i] = 0;
+	}
+#endif
+	if (!success) {
+		throw InternalException("OnFirstAllocation failed");
+	}
+}
+
+static void OnDeallocation(const data_ptr_t pointer, const idx_t size) {
+	bool success;
+#if defined(_WIN32)
+	success = VirtualFree(pointer, size, MEM_DECOMMIT);
+#elif defined(__APPLE__)
+	success = madvise(pointer, size, MADV_FREE_REUSABLE) == 0;
+#else
+	success = madvise(pointer, size, MADV_DONTNEED) == 0;
+#endif
+	if (!success) {
+		throw InternalException("OnDeallocation failed");
+	}
+}
+
+//===--------------------------------------------------------------------===//
+// BlockAllocatorThreadLocalState
+//===--------------------------------------------------------------------===//
+struct BlockQueue {
+	duckdb_moodycamel::ConcurrentQueue<uint32_t> q;
+};
+
+class BlockAllocatorThreadLocalState {
+public:
+	explicit BlockAllocatorThreadLocalState(const BlockAllocator &block_allocator_p) {
+		Initialize(block_allocator_p);
+	}
+	~BlockAllocatorThreadLocalState() {
+		Clear();
+	}
+
+public:
+	void TryInitialize(const BlockAllocator &block_allocator_p) {
+		// Local state can be invalidated if DB closes but thread stays alive
+		if (cached_uuid != block_allocator_p.uuid) {
+			Initialize(block_allocator_p);
+		}
+	}
+
+	data_ptr_t Allocate() {
+		auto pointer = TryAllocateFromLocal();
+		if (pointer) {
+			return pointer;
+		}
+
+		// We have run out of local blocks
+		if (TryGetBatch(touched, *block_allocator->touched) || TryGetBatch(untouched, *block_allocator->untouched)) {
+			// We have refilled local blocks
+			pointer = TryAllocateFromLocal();
+			D_ASSERT(pointer);
+			return pointer;
+		}
+
+		// We have also run out of global blocks, use fallback allocator
+		return block_allocator->allocator.AllocateData(block_allocator->block_size);
+	}
+
+	void Free(const data_ptr_t pointer) {
+		touched.push_back(block_allocator->GetBlockID(pointer));
+		if (touched.size() < FREE_THRESHOLD) {
+			return;
+		}
+
+		// Upon reaching the threshold, we return a local batch to global
+		std::sort(touched.begin(), touched.end());
+		block_allocator->touched->q.enqueue_bulk(touched.end() - BATCH_SIZE, BATCH_SIZE);
+		touched.resize(touched.size() - BATCH_SIZE);
+	}
+
+	void Clear() {
+		// Return all local blocks back to global
+		if (!touched.empty()) {
+			block_allocator->touched->q.enqueue_bulk(touched.begin(), touched.size());
+			touched.clear();
+		}
+		if (!untouched.empty()) {
+			block_allocator->untouched->q.enqueue_bulk(untouched.begin(), untouched.size());
+			untouched.clear();
+		}
+	}
+
+private:
+	void Initialize(const BlockAllocator &block_allocator_p) {
+		cached_uuid = block_allocator_p.uuid;
+		block_allocator = block_allocator_p;
+		untouched.clear();
+		touched.clear();
+		untouched.reserve(BATCH_SIZE);
+		touched.reserve(FREE_THRESHOLD);
+	}
+
+	data_ptr_t TryAllocateFromLocal() {
+		if (!touched.empty()) {
+			const auto pointer = block_allocator->GetPointer(touched.back());
+			touched.pop_back();
+			return pointer;
+		}
+		if (!untouched.empty()) {
+			const auto pointer = block_allocator->GetPointer(untouched.back());
+			untouched.pop_back();
+			OnFirstAllocation(pointer, block_allocator->block_size);
+			return pointer;
+		}
+		return nullptr;
+	}
+
+	static bool TryGetBatch(vector<uint32_t> &local, BlockQueue &global) {
+		D_ASSERT(local.empty());
+		local.resize(BATCH_SIZE);
+		const auto size = global.q.try_dequeue_bulk(local.begin(), BATCH_SIZE);
+		local.resize(size);
+		std::sort(local.begin(), local.end());
+		return !local.empty();
+	}
+
+private:
+	hugeint_t cached_uuid;
+	optional_ptr<const BlockAllocator> block_allocator;
+
+	static constexpr idx_t BATCH_SIZE = 128;
+	static constexpr idx_t FREE_THRESHOLD = BATCH_SIZE * 2;
+
+	vector<uint32_t> untouched;
+	vector<uint32_t> touched;
+};
+
+BlockAllocatorThreadLocalState &GetBlockAllocatorThreadLocalState(const BlockAllocator &block_allocator) {
+	thread_local BlockAllocatorThreadLocalState local_state(block_allocator);
+	local_state.TryInitialize(block_allocator);
+	return local_state;
+}
+
+//===--------------------------------------------------------------------===//
+// BlockAllocator
+//===--------------------------------------------------------------------===//
+BlockAllocator::BlockAllocator(Allocator &allocator_p, const idx_t block_size_p, const idx_t virtual_memory_size_p,
+                               const idx_t physical_memory_size_p)
+    : uuid(UUID::GenerateRandomUUID()), allocator(allocator_p), block_size(block_size_p),
+      block_size_div_shift(CountZeros<idx_t>::Trailing(block_size)),
+      virtual_memory_size(AlignValue(virtual_memory_size_p, block_size)),
+      virtual_memory_space(AllocateVirtualMemory(virtual_memory_size)), physical_memory_size(0),
+      untouched(make_unsafe_uniq<BlockQueue>()), touched(make_unsafe_uniq<BlockQueue>()) {
+	D_ASSERT(IsPowerOfTwo(block_size));
+	Resize(physical_memory_size_p);
+}
+
+BlockAllocator::~BlockAllocator() {
+	GetBlockAllocatorThreadLocalState(*this).Clear();
+	if (IsActive()) {
+		FreeVirtualMemory(virtual_memory_space, virtual_memory_size);
+	}
+}
+
+BlockAllocator &BlockAllocator::Get(DatabaseInstance &db) {
+	return *db.config.block_allocator;
+}
+
+BlockAllocator &BlockAllocator::Get(AttachedDatabase &db) {
+	return Get(db.GetDatabase());
+}
+
+void BlockAllocator::Resize(const idx_t new_physical_memory_size) {
+	if (!IsActive()) {
+		return;
+	}
+
+	lock_guard<mutex> guard(physical_memory_lock);
+	if (new_physical_memory_size < physical_memory_size) {
+		throw InvalidInputException("The \"block_allocator_size\" setting cannot be reduced (current: %llu)",
+		                            physical_memory_size.load());
+	}
+	if (new_physical_memory_size > virtual_memory_size) {
+		throw InvalidInputException("The \"block_allocator_size\" setting cannot be greater than the virtual memory "
+		                            "size (virtual memory size: %llu)",
+		                            virtual_memory_size);
+	}
+
+	// Enqueue block IDs efficiently in batches
+	uint32_t block_ids[STANDARD_VECTOR_SIZE];
+	const auto start = NumericCast<uint32_t>(DivBlockSize(physical_memory_size));
+	const auto end = NumericCast<uint32_t>(DivBlockSize(new_physical_memory_size));
+	for (auto block_id = start; block_id < end; block_id += STANDARD_VECTOR_SIZE) {
+		const auto next = MinValue<idx_t>(end - block_id, STANDARD_VECTOR_SIZE);
+		for (uint32_t i = 0; i < next; i++) {
+			block_ids[i] = block_id + i;
+		}
+		untouched->q.enqueue_bulk(block_ids, next);
+	}
+
+	// Finally, update to the new size
+	physical_memory_size = new_physical_memory_size;
+}
+
+bool BlockAllocator::IsActive() const {
+	return virtual_memory_space;
+}
+
+bool BlockAllocator::IsEnabled() const {
+	return physical_memory_size.load(std::memory_order_relaxed) != 0;
+}
+
+bool BlockAllocator::IsInPool(const data_ptr_t pointer) const {
+	return pointer >= virtual_memory_space && pointer < virtual_memory_space + virtual_memory_size;
+}
+
+idx_t BlockAllocator::ModuloBlockSize(const idx_t n) const {
+	return n & (block_size - 1);
+}
+
+idx_t BlockAllocator::DivBlockSize(const idx_t n) const {
+	return n >> block_size_div_shift;
+}
+
+uint32_t BlockAllocator::GetBlockID(const data_ptr_t pointer) const {
+	D_ASSERT(IsInPool(pointer));
+	const auto offset = NumericCast<idx_t>(pointer - virtual_memory_space);
+	D_ASSERT(ModuloBlockSize(offset) == 0);
+	const auto block_id = NumericCast<uint32_t>(DivBlockSize(offset));
+	VerifyBlockID(block_id);
+	return block_id;
+}
+
+void BlockAllocator::VerifyBlockID(const uint32_t block_id) const {
+	D_ASSERT(block_id < NumericCast<uint32_t>(virtual_memory_size / block_size));
+}
+
+data_ptr_t BlockAllocator::GetPointer(const uint32_t block_id) const {
+	VerifyBlockID(block_id);
+	return virtual_memory_space + NumericCast<idx_t>(block_id) * block_size;
+}
+
+data_ptr_t BlockAllocator::AllocateData(const idx_t size) const {
+	if (!IsActive() || !IsEnabled() || size != block_size) {
+		return allocator.AllocateData(size);
+	}
+	return GetBlockAllocatorThreadLocalState(*this).Allocate();
+}
+
+void BlockAllocator::FreeData(const data_ptr_t pointer, const idx_t size) const {
+	if (!IsActive() || !IsInPool(pointer)) {
+		return allocator.FreeData(pointer, size);
+	}
+	D_ASSERT(size == block_size);
+	GetBlockAllocatorThreadLocalState(*this).Free(pointer);
+}
+
+data_ptr_t BlockAllocator::ReallocateData(const data_ptr_t pointer, const idx_t old_size, const idx_t new_size) const {
+	if (old_size == new_size) {
+		return pointer;
+	}
+
+	// If both the old and new allocation are not (or cannot be) in the pool, immediately use the fallback allocator
+	if (!IsActive() || (!IsInPool(pointer) && new_size != block_size)) {
+		return allocator.ReallocateData(pointer, old_size, new_size);
+	}
+
+	// Either old or new can be in the pool: allocate, copy, and free
+	const auto new_pointer = AllocateData(new_size);
+	memcpy(new_pointer, pointer, MinValue(old_size, new_size));
+	FreeData(pointer, old_size);
+	return new_pointer;
+}
+
+bool BlockAllocator::SupportsFlush() const {
+	return (IsActive() && IsEnabled()) || Allocator::SupportsFlush();
+}
+
+void BlockAllocator::ThreadFlush(bool allocator_background_threads, idx_t threshold, idx_t thread_count) const {
+	if (IsActive() && IsEnabled()) {
+		GetBlockAllocatorThreadLocalState(*this).Clear();
+	}
+	if (Allocator::SupportsFlush()) {
+		Allocator::ThreadFlush(allocator_background_threads, threshold, thread_count);
+	}
+}
+
+void BlockAllocator::FlushAll(const optional_idx extra_memory) const {
+	if (IsActive() && IsEnabled() && extra_memory.IsValid()) {
+		FreeInternal(extra_memory.GetIndex());
+	}
+	if (Allocator::SupportsFlush()) {
+		Allocator::FlushAll();
+	}
+}
+
+void BlockAllocator::FreeInternal(const idx_t extra_memory) const {
+	auto count = DivBlockSize(extra_memory);
+	unsafe_vector<uint32_t> to_free_buffer;
+	to_free_buffer.resize(count);
+	count = touched->q.try_dequeue_bulk(to_free_buffer.begin(), count);
+	if (count == 0) {
+		return;
+	}
+	to_free_buffer.resize(count);
+
+	// Sort so we can coalesce madvise calls
+	std::sort(to_free_buffer.begin(), to_free_buffer.end());
+
+	// Coalesce and free
+	uint32_t block_id_start = to_free_buffer[0];
+	for (idx_t i = 1; i < to_free_buffer.size(); i++) {
+		const auto &previous_block_id = to_free_buffer[i - 1];
+		const auto &current_block_id = to_free_buffer[i];
+		if (previous_block_id == current_block_id - 1) {
+			continue; // Current is contiguous with previous block
+		}
+
+		// Previous block is the last contiguous block starting from block_id_start, free them in one go
+		FreeContiguousBlocks(block_id_start, previous_block_id);
+
+		// Continue coalescing from the current
+		block_id_start = current_block_id;
+	}
+
+	// Don't forget the last one
+	FreeContiguousBlocks(block_id_start, to_free_buffer.back());
+
+	// Make freed blocks available to allocate again
+	untouched->q.enqueue_bulk(to_free_buffer.begin(), to_free_buffer.size());
+}
+
+void BlockAllocator::FreeContiguousBlocks(const uint32_t block_id_start, const uint32_t block_id_end_including) const {
+	const auto pointer = GetPointer(block_id_start);
+	const auto num_blocks = block_id_end_including - block_id_start + 1;
+	const auto size = num_blocks * block_size;
+	OnDeallocation(pointer, size);
+}
+
+} // namespace duckdb
diff --git a/src/storage/buffer/buffer_pool.cpp b/src/storage/buffer/buffer_pool.cpp
index b974dab30c..2f2183b9c6 100644
--- a/src/storage/buffer/buffer_pool.cpp
+++ b/src/storage/buffer/buffer_pool.cpp
@@ -6,6 +6,7 @@
 #include "duckdb/common/typedefs.hpp"
 #include "duckdb/parallel/concurrentqueue.hpp"
 #include "duckdb/parallel/task_scheduler.hpp"
+#include "duckdb/storage/block_allocator.hpp"
 #include "duckdb/storage/temporary_memory_manager.hpp"
 
 namespace duckdb {
@@ -229,13 +230,13 @@ void EvictionQueue::PurgeIteration(const idx_t purge_size) {
 	total_dead_nodes -= actually_dequeued - alive_nodes;
 }
 
-BufferPool::BufferPool(idx_t maximum_memory, bool track_eviction_timestamps,
+BufferPool::BufferPool(BlockAllocator &block_allocator, idx_t maximum_memory, bool track_eviction_timestamps,
                        idx_t allocator_bulk_deallocation_flush_threshold)
     : eviction_queue_sizes({BLOCK_AND_EXTERNAL_FILE_QUEUE_SIZE, MANAGED_BUFFER_QUEUE_SIZE, TINY_BUFFER_QUEUE_SIZE}),
       maximum_memory(maximum_memory),
       allocator_bulk_deallocation_flush_threshold(allocator_bulk_deallocation_flush_threshold),
       track_eviction_timestamps(track_eviction_timestamps),
-      temporary_memory_manager(make_uniq<TemporaryMemoryManager>()) {
+      temporary_memory_manager(make_uniq<TemporaryMemoryManager>()), block_allocator(block_allocator) {
 	for (idx_t queue_type_idx = 0; queue_type_idx < EVICTION_QUEUE_TYPES; queue_type_idx++) {
 		const auto types = EvictionQueueTypeIdxToFileBufferTypes(queue_type_idx);
 		const auto &type_queue_size = eviction_queue_sizes[queue_type_idx];
@@ -333,8 +334,8 @@ BufferPool::EvictionResult BufferPool::EvictBlocksInternal(EvictionQueue &queue,
 	bool found = false;
 
 	if (memory_usage.GetUsedMemory(MemoryUsageCaches::NO_FLUSH) <= memory_limit) {
-		if (Allocator::SupportsFlush() && extra_memory > allocator_bulk_deallocation_flush_threshold) {
-			Allocator::FlushAll();
+		if (extra_memory > allocator_bulk_deallocation_flush_threshold) {
+			block_allocator.FlushAll(extra_memory);
 		}
 		return {true, std::move(r)};
 	}
@@ -362,8 +363,8 @@ BufferPool::EvictionResult BufferPool::EvictBlocksInternal(EvictionQueue &queue,
 
 	if (!found) {
 		r.Resize(0);
-	} else if (Allocator::SupportsFlush() && extra_memory > allocator_bulk_deallocation_flush_threshold) {
-		Allocator::FlushAll();
+	} else if (extra_memory > allocator_bulk_deallocation_flush_threshold) {
+		block_allocator.FlushAll(extra_memory);
 	}
 
 	return {found, std::move(r)};
@@ -454,9 +455,7 @@ void BufferPool::SetLimit(idx_t limit, const char *exception_postscript) {
 		    "Failed to change memory limit to %lld: could not free up enough memory for the new limit%s", limit,
 		    exception_postscript);
 	}
-	if (Allocator::SupportsFlush()) {
-		Allocator::FlushAll();
-	}
+	block_allocator.FlushAll();
 }
 
 void BufferPool::SetAllocatorBulkDeallocationFlushThreshold(idx_t threshold) {
diff --git a/src/storage/compression/bitpacking.cpp b/src/storage/compression/bitpacking.cpp
index 628ff19a9b..1d470b01e2 100644
--- a/src/storage/compression/bitpacking.cpp
+++ b/src/storage/compression/bitpacking.cpp
@@ -71,7 +71,7 @@ static bitpacking_metadata_encoded_t EncodeMeta(bitpacking_metadata_t metadata)
 }
 static bitpacking_metadata_t DecodeMeta(bitpacking_metadata_encoded_t *metadata_encoded) {
 	bitpacking_metadata_t metadata;
-	metadata.mode = Load<BitpackingMode>(data_ptr_cast(metadata_encoded) + 3);
+	metadata.mode = static_cast<BitpackingMode>((*metadata_encoded >> 24) & 0xFF);
 	metadata.offset = *metadata_encoded & 0x00FFFFFF;
 	return metadata;
 }
@@ -383,7 +383,7 @@ public:
 	explicit BitpackingCompressionState(ColumnDataCheckpointData &checkpoint_data, const CompressionInfo &info)
 	    : CompressionState(info), checkpoint_data(checkpoint_data),
 	      function(checkpoint_data.GetCompressionFunction(CompressionType::COMPRESSION_BITPACKING)) {
-		CreateEmptySegment(checkpoint_data.GetRowGroup().start);
+		CreateEmptySegment();
 
 		state.data_ptr = reinterpret_cast<void *>(this);
 
@@ -497,12 +497,12 @@ public:
 		       info.GetBlockSize() - BitpackingPrimitives::BITPACKING_HEADER_SIZE;
 	}
 
-	void CreateEmptySegment(idx_t row_start) {
+	void CreateEmptySegment() {
 		auto &db = checkpoint_data.GetDatabase();
 		auto &type = checkpoint_data.GetType();
 
-		auto compressed_segment = ColumnSegment::CreateTransientSegment(db, function, type, row_start,
-		                                                                info.GetBlockSize(), info.GetBlockManager());
+		auto compressed_segment =
+		    ColumnSegment::CreateTransientSegment(db, function, type, info.GetBlockSize(), info.GetBlockManager());
 		current_segment = std::move(compressed_segment);
 
 		auto &buffer_manager = BufferManager::GetBufferManager(db);
@@ -524,9 +524,8 @@ public:
 
 	void FlushAndCreateSegmentIfFull(idx_t required_data_bytes, idx_t required_meta_bytes) {
 		if (!CanStore(required_data_bytes, required_meta_bytes)) {
-			idx_t row_start = current_segment->start + current_segment->count;
 			FlushSegment();
-			CreateEmptySegment(row_start);
+			CreateEmptySegment();
 		}
 	}
 
diff --git a/src/storage/compression/dict_fsst.cpp b/src/storage/compression/dict_fsst.cpp
index 18c5dac214..9e6e2f279d 100644
--- a/src/storage/compression/dict_fsst.cpp
+++ b/src/storage/compression/dict_fsst.cpp
@@ -133,7 +133,7 @@ void DictFSSTCompressionStorage::StringScanPartial(ColumnSegment &segment, Colum
 	// clear any previously locked buffers and get the primary buffer handle
 	auto &scan_state = state.scan_state->Cast<CompressedStringScanState>();
 
-	auto start = segment.GetRelativeIndex(state.row_index);
+	auto start = state.GetPositionInSegment();
 	if (!ALLOW_DICT_VECTORS || !scan_state.AllowDictionaryScan(scan_count)) {
 		scan_state.ScanToFlatVector(result, result_offset, start, scan_count);
 	} else {
@@ -165,7 +165,7 @@ void DictFSSTSelect(ColumnSegment &segment, ColumnScanState &state, idx_t vector
 	auto &scan_state = state.scan_state->Cast<CompressedStringScanState>();
 	if (scan_state.mode == DictFSSTMode::FSST_ONLY) {
 		// for FSST only
-		auto start = segment.GetRelativeIndex(state.row_index);
+		auto start = state.GetPositionInSegment();
 		scan_state.Select(result, start, sel, sel_count);
 		return;
 	}
@@ -181,7 +181,7 @@ static void DictFSSTFilter(ColumnSegment &segment, ColumnScanState &state, idx_t
                            SelectionVector &sel, idx_t &sel_count, const TableFilter &filter,
                            TableFilterState &filter_state) {
 	auto &scan_state = state.scan_state->Cast<CompressedStringScanState>();
-	auto start = segment.GetRelativeIndex(state.row_index);
+	auto start = state.GetPositionInSegment();
 	if (scan_state.AllowDictionaryScan(vector_count)) {
 		// only pushdown filters on dictionaries
 		if (!scan_state.filter_result) {
diff --git a/src/storage/compression/dict_fsst/compression.cpp b/src/storage/compression/dict_fsst/compression.cpp
index 9c2cdf85aa..fcbc4ade97 100644
--- a/src/storage/compression/dict_fsst/compression.cpp
+++ b/src/storage/compression/dict_fsst/compression.cpp
@@ -21,7 +21,7 @@ DictFSSTCompressionState::DictFSSTCompressionState(ColumnDataCheckpointData &che
           1                                                                // maximum_target_capacity_p (byte capacity)
           ),
       analyze(std::move(analyze_p)) {
-	CreateEmptySegment(checkpoint_data.GetRowGroup().start);
+	CreateEmptySegment();
 }
 
 DictFSSTCompressionState::~DictFSSTCompressionState() {
@@ -237,12 +237,12 @@ void DictFSSTCompressionState::FlushEncodingBuffer() {
 	dictionary_encoding_buffer.clear();
 }
 
-void DictFSSTCompressionState::CreateEmptySegment(idx_t row_start) {
+void DictFSSTCompressionState::CreateEmptySegment() {
 	auto &db = checkpoint_data.GetDatabase();
 	auto &type = checkpoint_data.GetType();
 
-	auto compressed_segment = ColumnSegment::CreateTransientSegment(db, function, type, row_start, info.GetBlockSize(),
-	                                                                info.GetBlockManager());
+	auto compressed_segment =
+	    ColumnSegment::CreateTransientSegment(db, function, type, info.GetBlockSize(), info.GetBlockManager());
 	current_segment = std::move(compressed_segment);
 
 	// Reset the pointers into the current segment.
@@ -277,7 +277,6 @@ void DictFSSTCompressionState::Flush(bool final) {
 
 	current_segment->count = tuple_count;
 
-	auto next_start = current_segment->start + current_segment->count;
 	auto segment_size = Finalize();
 	auto &state = checkpoint_data.GetCheckpointState();
 	state.FlushSegment(std::move(current_segment), std::move(current_handle), segment_size);
@@ -301,7 +300,7 @@ void DictFSSTCompressionState::Flush(bool final) {
 	total_tuple_count += tuple_count;
 
 	if (!final) {
-		CreateEmptySegment(next_start);
+		CreateEmptySegment();
 	}
 }
 
diff --git a/src/storage/compression/dictionary/compression.cpp b/src/storage/compression/dictionary/compression.cpp
index d1de02fa9c..9d98a1ef37 100644
--- a/src/storage/compression/dictionary/compression.cpp
+++ b/src/storage/compression/dictionary/compression.cpp
@@ -13,15 +13,15 @@ DictionaryCompressionCompressState::DictionaryCompressionCompressState(ColumnDat
           1 // maximum_target_capacity_p, 1 because we don't care about target for our use-case, as we
             // only use PrimitiveDictionary for duplicate checks, and not for writing to any target
       ) {
-	CreateEmptySegment(checkpoint_data.GetRowGroup().start);
+	CreateEmptySegment();
 }
 
-void DictionaryCompressionCompressState::CreateEmptySegment(idx_t row_start) {
+void DictionaryCompressionCompressState::CreateEmptySegment() {
 	auto &db = checkpoint_data.GetDatabase();
 	auto &type = checkpoint_data.GetType();
 
-	auto compressed_segment = ColumnSegment::CreateTransientSegment(db, function, type, row_start, info.GetBlockSize(),
-	                                                                info.GetBlockManager());
+	auto compressed_segment =
+	    ColumnSegment::CreateTransientSegment(db, function, type, info.GetBlockSize(), info.GetBlockManager());
 	current_segment = std::move(compressed_segment);
 
 	// Reset the buffers and the string map.
@@ -108,14 +108,12 @@ bool DictionaryCompressionCompressState::CalculateSpaceRequirements(bool new_str
 }
 
 void DictionaryCompressionCompressState::Flush(bool final) {
-	auto next_start = current_segment->start + current_segment->count;
-
 	auto segment_size = Finalize();
 	auto &state = checkpoint_data.GetCheckpointState();
 	state.FlushSegment(std::move(current_segment), std::move(current_handle), segment_size);
 
 	if (!final) {
-		CreateEmptySegment(next_start);
+		CreateEmptySegment();
 	}
 }
 
diff --git a/src/storage/compression/dictionary_compression.cpp b/src/storage/compression/dictionary_compression.cpp
index e3a976ccca..6f10acb60f 100644
--- a/src/storage/compression/dictionary_compression.cpp
+++ b/src/storage/compression/dictionary_compression.cpp
@@ -140,7 +140,7 @@ void DictionaryCompressionStorage::StringScanPartial(ColumnSegment &segment, Col
 	// clear any previously locked buffers and get the primary buffer handle
 	auto &scan_state = state.scan_state->Cast<CompressedStringScanState>();
 
-	auto start = segment.GetRelativeIndex(state.row_index);
+	auto start = state.GetPositionInSegment();
 	if (!ALLOW_DICT_VECTORS || scan_count != STANDARD_VECTOR_SIZE) {
 		scan_state.ScanToFlatVector(result, result_offset, start, scan_count);
 	} else {
diff --git a/src/storage/compression/fixed_size_uncompressed.cpp b/src/storage/compression/fixed_size_uncompressed.cpp
index 89c7185259..796093322c 100644
--- a/src/storage/compression/fixed_size_uncompressed.cpp
+++ b/src/storage/compression/fixed_size_uncompressed.cpp
@@ -46,7 +46,7 @@ public:
 	UncompressedCompressState(ColumnDataCheckpointData &checkpoint_data, const CompressionInfo &info);
 
 public:
-	virtual void CreateEmptySegment(idx_t row_start);
+	virtual void CreateEmptySegment();
 	void FlushSegment(idx_t segment_size);
 	void Finalize(idx_t segment_size);
 
@@ -61,15 +61,15 @@ UncompressedCompressState::UncompressedCompressState(ColumnDataCheckpointData &c
                                                      const CompressionInfo &info)
     : CompressionState(info), checkpoint_data(checkpoint_data),
       function(checkpoint_data.GetCompressionFunction(CompressionType::COMPRESSION_UNCOMPRESSED)) {
-	UncompressedCompressState::CreateEmptySegment(checkpoint_data.GetRowGroup().start);
+	UncompressedCompressState::CreateEmptySegment();
 }
 
-void UncompressedCompressState::CreateEmptySegment(idx_t row_start) {
+void UncompressedCompressState::CreateEmptySegment() {
 	auto &db = checkpoint_data.GetDatabase();
 	auto &type = checkpoint_data.GetType();
 
-	auto compressed_segment = ColumnSegment::CreateTransientSegment(db, function, type, row_start, info.GetBlockSize(),
-	                                                                info.GetBlockManager());
+	auto compressed_segment =
+	    ColumnSegment::CreateTransientSegment(db, function, type, info.GetBlockSize(), info.GetBlockManager());
 	if (type.InternalType() == PhysicalType::VARCHAR) {
 		auto &state = compressed_segment->GetSegmentState()->Cast<UncompressedStringSegmentState>();
 		auto &storage_manager = checkpoint_data.GetStorageManager();
@@ -120,12 +120,11 @@ void UncompressedFunctions::Compress(CompressionState &state_p, Vector &data, id
 			// appended everything: finished
 			return;
 		}
-		auto next_start = state.current_segment->start + state.current_segment->count;
 		// the segment is full: flush it to disk
 		state.FlushSegment(state.current_segment->FinalizeAppend(state.append_state));
 
 		// now create a new segment and continue appending
-		state.CreateEmptySegment(next_start);
+		state.CreateEmptySegment();
 		offset += appended;
 		count -= appended;
 	}
@@ -157,7 +156,7 @@ template <class T>
 void FixedSizeScanPartial(ColumnSegment &segment, ColumnScanState &state, idx_t scan_count, Vector &result,
                           idx_t result_offset) {
 	auto &scan_state = state.scan_state->Cast<FixedSizeScanState>();
-	auto start = segment.GetRelativeIndex(state.row_index);
+	auto start = state.GetPositionInSegment();
 
 	auto data = scan_state.handle.Ptr() + segment.GetBlockOffset();
 	auto source_data = data + start * sizeof(T);
@@ -170,7 +169,7 @@ void FixedSizeScanPartial(ColumnSegment &segment, ColumnScanState &state, idx_t
 template <class T>
 void FixedSizeScan(ColumnSegment &segment, ColumnScanState &state, idx_t scan_count, Vector &result) {
 	auto &scan_state = state.scan_state->template Cast<FixedSizeScanState>();
-	auto start = segment.GetRelativeIndex(state.row_index);
+	auto start = state.GetPositionInSegment();
 
 	auto data = scan_state.handle.Ptr() + segment.GetBlockOffset();
 	auto source_data = data + start * sizeof(T);
diff --git a/src/storage/compression/fsst.cpp b/src/storage/compression/fsst.cpp
index 57b769432b..cf6ec0776f 100644
--- a/src/storage/compression/fsst.cpp
+++ b/src/storage/compression/fsst.cpp
@@ -219,7 +219,7 @@ public:
 	FSSTCompressionState(ColumnDataCheckpointData &checkpoint_data, const CompressionInfo &info)
 	    : CompressionState(info), checkpoint_data(checkpoint_data),
 	      function(checkpoint_data.GetCompressionFunction(CompressionType::COMPRESSION_FSST)) {
-		CreateEmptySegment(checkpoint_data.GetRowGroup().start);
+		CreateEmptySegment();
 	}
 
 	~FSSTCompressionState() override {
@@ -241,12 +241,12 @@ public:
 		current_end_ptr = current_handle.Ptr() + current_dictionary.end;
 	}
 
-	void CreateEmptySegment(idx_t row_start) {
+	void CreateEmptySegment() {
 		auto &db = checkpoint_data.GetDatabase();
 		auto &type = checkpoint_data.GetType();
 
-		auto compressed_segment = ColumnSegment::CreateTransientSegment(db, function, type, row_start,
-		                                                                info.GetBlockSize(), info.GetBlockManager());
+		auto compressed_segment =
+		    ColumnSegment::CreateTransientSegment(db, function, type, info.GetBlockSize(), info.GetBlockManager());
 		current_segment = std::move(compressed_segment);
 		Reset();
 	}
@@ -323,14 +323,12 @@ public:
 	}
 
 	void Flush(bool final = false) {
-		auto next_start = current_segment->start + current_segment->count;
-
 		auto segment_size = Finalize();
 		auto &state = checkpoint_data.GetCheckpointState();
 		state.FlushSegment(std::move(current_segment), std::move(current_handle), segment_size);
 
 		if (!final) {
-			CreateEmptySegment(next_start);
+			CreateEmptySegment();
 		}
 	}
 
@@ -642,7 +640,7 @@ template <bool ALLOW_FSST_VECTORS>
 void FSSTStorage::StringScanPartial(ColumnSegment &segment, ColumnScanState &state, idx_t scan_count, Vector &result,
                                     idx_t result_offset) {
 	auto &scan_state = state.scan_state->Cast<FSSTScanState>();
-	auto start = segment.GetRelativeIndex(state.row_index);
+	auto start = state.GetPositionInSegment();
 
 	bool enable_fsst_vectors;
 	if (ALLOW_FSST_VECTORS) {
@@ -710,7 +708,7 @@ void FSSTStorage::StringScan(ColumnSegment &segment, ColumnScanState &state, idx
 void FSSTStorage::Select(ColumnSegment &segment, ColumnScanState &state, idx_t vector_count, Vector &result,
                          const SelectionVector &sel, idx_t sel_count) {
 	auto &scan_state = state.scan_state->Cast<FSSTScanState>();
-	auto start = segment.GetRelativeIndex(state.row_index);
+	auto start = state.GetPositionInSegment();
 
 	auto baseptr = scan_state.handle.Ptr() + segment.GetBlockOffset();
 	auto dict = GetDictionary(segment, scan_state.handle);
diff --git a/src/storage/compression/rle.cpp b/src/storage/compression/rle.cpp
index ed26d824db..e0d53e680a 100644
--- a/src/storage/compression/rle.cpp
+++ b/src/storage/compression/rle.cpp
@@ -140,18 +140,18 @@ struct RLECompressState : public CompressionState {
 	RLECompressState(ColumnDataCheckpointData &checkpoint_data_p, const CompressionInfo &info)
 	    : CompressionState(info), checkpoint_data(checkpoint_data_p),
 	      function(checkpoint_data.GetCompressionFunction(CompressionType::COMPRESSION_RLE)) {
-		CreateEmptySegment(checkpoint_data.GetRowGroup().start);
+		CreateEmptySegment();
 
 		state.dataptr = (void *)this;
 		max_rle_count = MaxRLECount();
 	}
 
-	void CreateEmptySegment(idx_t row_start) {
+	void CreateEmptySegment() {
 		auto &db = checkpoint_data.GetDatabase();
 		auto &type = checkpoint_data.GetType();
 
-		auto column_segment = ColumnSegment::CreateTransientSegment(db, function, type, row_start, info.GetBlockSize(),
-		                                                            info.GetBlockManager());
+		auto column_segment =
+		    ColumnSegment::CreateTransientSegment(db, function, type, info.GetBlockSize(), info.GetBlockManager());
 		current_segment = std::move(column_segment);
 
 		auto &buffer_manager = BufferManager::GetBufferManager(db);
@@ -183,9 +183,8 @@ struct RLECompressState : public CompressionState {
 
 		if (entry_count == max_rle_count) {
 			// we have finished writing this segment: flush it and create a new segment
-			auto row_start = current_segment->start + current_segment->count;
 			FlushSegment();
-			CreateEmptySegment(row_start);
+			CreateEmptySegment();
 			entry_count = 0;
 		}
 	}
diff --git a/src/storage/compression/roaring/common.cpp b/src/storage/compression/roaring/common.cpp
index 3d9230787e..ee4a71797b 100644
--- a/src/storage/compression/roaring/common.cpp
+++ b/src/storage/compression/roaring/common.cpp
@@ -219,7 +219,7 @@ unique_ptr<SegmentScanState> RoaringInitScan(const QueryContext &context, Column
 void RoaringScanPartial(ColumnSegment &segment, ColumnScanState &state, idx_t scan_count, Vector &result,
                         idx_t result_offset) {
 	auto &scan_state = state.scan_state->Cast<RoaringScanState>();
-	auto start = segment.GetRelativeIndex(state.row_index);
+	auto start = state.GetPositionInSegment();
 
 	scan_state.ScanPartial(start, result, result_offset, scan_count);
 }
diff --git a/src/storage/compression/roaring/compress.cpp b/src/storage/compression/roaring/compress.cpp
index fc2ba3625a..32c3627d3d 100644
--- a/src/storage/compression/roaring/compress.cpp
+++ b/src/storage/compression/roaring/compress.cpp
@@ -202,7 +202,7 @@ RoaringCompressState::RoaringCompressState(ColumnDataCheckpointData &checkpoint_
       analyze_state(owned_analyze_state->Cast<RoaringAnalyzeState>()), container_state(),
       container_metadata(analyze_state.container_metadata), checkpoint_data(checkpoint_data),
       function(checkpoint_data.GetCompressionFunction(CompressionType::COMPRESSION_ROARING)) {
-	CreateEmptySegment(checkpoint_data.GetRowGroup().start);
+	CreateEmptySegment();
 	total_count = 0;
 	InitializeContainer();
 }
@@ -257,9 +257,8 @@ void RoaringCompressState::InitializeContainer() {
 	idx_t container_size = AlignValue<idx_t, ValidityMask::BITS_PER_VALUE>(
 	    MinValue<idx_t>(analyze_state.total_count - container_state.appended_count, ROARING_CONTAINER_SIZE));
 	if (!CanStore(container_size, metadata)) {
-		idx_t row_start = current_segment->start + current_segment->count;
 		FlushSegment();
-		CreateEmptySegment(row_start);
+		CreateEmptySegment();
 	}
 
 	// Override the pointer to write directly into the block
@@ -278,12 +277,12 @@ void RoaringCompressState::InitializeContainer() {
 	metadata_collection.AddMetadata(metadata);
 }
 
-void RoaringCompressState::CreateEmptySegment(idx_t row_start) {
+void RoaringCompressState::CreateEmptySegment() {
 	auto &db = checkpoint_data.GetDatabase();
 	auto &type = checkpoint_data.GetType();
 
-	auto compressed_segment = ColumnSegment::CreateTransientSegment(db, function, type, row_start, info.GetBlockSize(),
-	                                                                info.GetBlockManager());
+	auto compressed_segment =
+	    ColumnSegment::CreateTransientSegment(db, function, type, info.GetBlockSize(), info.GetBlockManager());
 	current_segment = std::move(compressed_segment);
 
 	auto &buffer_manager = BufferManager::GetBufferManager(db);
diff --git a/src/storage/compression/string_uncompressed.cpp b/src/storage/compression/string_uncompressed.cpp
index 201e977875..23c96f0e4d 100644
--- a/src/storage/compression/string_uncompressed.cpp
+++ b/src/storage/compression/string_uncompressed.cpp
@@ -92,7 +92,7 @@ void UncompressedStringStorage::StringScanPartial(ColumnSegment &segment, Column
                                                   Vector &result, idx_t result_offset) {
 	// clear any previously locked buffers and get the primary buffer handle
 	auto &scan_state = state.scan_state->Cast<StringScanState>();
-	auto start = segment.GetRelativeIndex(state.row_index);
+	auto start = state.GetPositionInSegment();
 
 	auto baseptr = scan_state.handle.Ptr() + segment.GetBlockOffset();
 	auto dict_end = GetDictionaryEnd(segment, scan_state.handle);
@@ -123,7 +123,7 @@ void UncompressedStringStorage::Select(ColumnSegment &segment, ColumnScanState &
                                        Vector &result, const SelectionVector &sel, idx_t sel_count) {
 	// clear any previously locked buffers and get the primary buffer handle
 	auto &scan_state = state.scan_state->Cast<StringScanState>();
-	auto start = segment.GetRelativeIndex(state.row_index);
+	auto start = state.GetPositionInSegment();
 
 	auto baseptr = scan_state.handle.Ptr() + segment.GetBlockOffset();
 	auto dict_end = GetDictionaryEnd(segment, scan_state.handle);
diff --git a/src/storage/compression/validity_uncompressed.cpp b/src/storage/compression/validity_uncompressed.cpp
index e43f237a3f..66a57e5828 100644
--- a/src/storage/compression/validity_uncompressed.cpp
+++ b/src/storage/compression/validity_uncompressed.cpp
@@ -397,7 +397,7 @@ void ValidityUncompressed::AlignedScan(data_ptr_t input, idx_t input_start, Vect
 
 void ValidityScanPartial(ColumnSegment &segment, ColumnScanState &state, idx_t scan_count, Vector &result,
                          idx_t result_offset) {
-	auto start = segment.GetRelativeIndex(state.row_index);
+	auto start = state.GetPositionInSegment();
 
 	static_assert(sizeof(validity_t) == sizeof(uint64_t), "validity_t should be 64-bit");
 	auto &scan_state = state.scan_state->Cast<ValidityScanState>();
@@ -410,7 +410,7 @@ void ValidityScanPartial(ColumnSegment &segment, ColumnScanState &state, idx_t s
 void ValidityScan(ColumnSegment &segment, ColumnScanState &state, idx_t scan_count, Vector &result) {
 	result.Flatten(scan_count);
 
-	auto start = segment.GetRelativeIndex(state.row_index);
+	auto start = state.GetPositionInSegment();
 	if (start % ValidityMask::BITS_PER_VALUE == 0) {
 		auto &scan_state = state.scan_state->Cast<ValidityScanState>();
 
@@ -435,7 +435,7 @@ void ValiditySelect(ColumnSegment &segment, ColumnScanState &state, idx_t, Vecto
 	auto &result_mask = FlatVector::Validity(result);
 	auto input_data = reinterpret_cast<validity_t *>(buffer_ptr);
 
-	auto start = segment.GetRelativeIndex(state.row_index);
+	auto start = state.GetPositionInSegment();
 	ValidityMask source_mask(input_data, segment.count);
 	for (idx_t i = 0; i < sel_count; i++) {
 		auto source_idx = start + sel.get_index(i);
@@ -511,8 +511,8 @@ idx_t ValidityFinalizeAppend(ColumnSegment &segment, SegmentStatistics &stats) {
 	return ((segment.count + STANDARD_VECTOR_SIZE - 1) / STANDARD_VECTOR_SIZE) * ValidityMask::STANDARD_MASK_SIZE;
 }
 
-void ValidityRevertAppend(ColumnSegment &segment, idx_t start_row) {
-	idx_t start_bit = start_row - segment.start;
+void ValidityRevertAppend(ColumnSegment &segment, idx_t new_count) {
+	idx_t start_bit = new_count;
 
 	auto &buffer_manager = BufferManager::GetBufferManager(segment.db);
 	auto handle = buffer_manager.Pin(segment.block);
diff --git a/src/storage/compression/zstd.cpp b/src/storage/compression/zstd.cpp
index 829a76f051..5ff72610ae 100644
--- a/src/storage/compression/zstd.cpp
+++ b/src/storage/compression/zstd.cpp
@@ -312,14 +312,8 @@ public:
 			throw InternalException("We are asking for a new segment, but somehow we're still writing vector data onto "
 			                        "the initial (segment) page");
 		}
-		idx_t row_start;
-		if (segment) {
-			row_start = segment->start + segment->count;
-			FlushSegment();
-		} else {
-			row_start = checkpoint_data.GetRowGroup().start;
-		}
-		CreateEmptySegment(row_start);
+		FlushSegment();
+		CreateEmptySegment();
 
 		// Figure out how many vectors we are storing in this segment
 		idx_t vectors_in_segment;
@@ -524,11 +518,11 @@ public:
 		return res;
 	}
 
-	void CreateEmptySegment(idx_t row_start) {
+	void CreateEmptySegment() {
 		auto &db = checkpoint_data.GetDatabase();
 		auto &type = checkpoint_data.GetType();
-		auto compressed_segment = ColumnSegment::CreateTransientSegment(db, function, type, row_start,
-		                                                                info.GetBlockSize(), info.GetBlockManager());
+		auto compressed_segment =
+		    ColumnSegment::CreateTransientSegment(db, function, type, info.GetBlockSize(), info.GetBlockManager());
 		segment = std::move(compressed_segment);
 
 		auto &buffer_manager = BufferManager::GetBufferManager(checkpoint_data.GetDatabase());
@@ -536,6 +530,9 @@ public:
 	}
 
 	void FlushSegment() {
+		if (!segment) {
+			return;
+		}
 		auto &state = checkpoint_data.GetCheckpointState();
 		idx_t segment_block_size;
 
@@ -993,7 +990,7 @@ unique_ptr<SegmentScanState> ZSTDStorage::StringInitScan(const QueryContext &con
 void ZSTDStorage::StringScanPartial(ColumnSegment &segment, ColumnScanState &state, idx_t scan_count, Vector &result,
                                     idx_t result_offset) {
 	auto &scan_state = state.scan_state->template Cast<ZSTDScanState>();
-	auto start = segment.GetRelativeIndex(state.row_index);
+	auto start = state.GetPositionInSegment();
 
 	scan_state.ScanPartial(start, result, result_offset, scan_count);
 }
diff --git a/src/storage/data_table.cpp b/src/storage/data_table.cpp
index d4a246e013..5e7806ecc9 100644
--- a/src/storage/data_table.cpp
+++ b/src/storage/data_table.cpp
@@ -1059,7 +1059,7 @@ void DataTable::ScanTableSegment(DuckTransaction &transaction, idx_t row_start,
 
 	InitializeScanWithOffset(transaction, state, column_ids, row_start, row_start + count);
 	auto row_start_aligned =
-	    state.table_state.row_group->node->start + state.table_state.vector_index * STANDARD_VECTOR_SIZE;
+	    state.table_state.row_group->row_start + state.table_state.vector_index * STANDARD_VECTOR_SIZE;
 
 	idx_t current_row = row_start_aligned;
 	while (current_row < end) {
diff --git a/src/storage/index.cpp b/src/storage/index.cpp
index fc9ffcb85d..ab5c5b6ed1 100644
--- a/src/storage/index.cpp
+++ b/src/storage/index.cpp
@@ -7,9 +7,6 @@ namespace duckdb {
 Index::Index(const vector<column_t> &column_ids, TableIOManager &table_io_manager, AttachedDatabase &db)
 
     : column_ids(column_ids), table_io_manager(table_io_manager), db(db) {
-	if (!Radix::IsLittleEndian()) {
-		throw NotImplementedException("indexes are not supported on big endian architectures");
-	}
 	// create the column id set
 	column_id_set.insert(column_ids.begin(), column_ids.end());
 }
diff --git a/src/storage/single_file_block_manager.cpp b/src/storage/single_file_block_manager.cpp
index 127fa795c2..5054a16bb5 100644
--- a/src/storage/single_file_block_manager.cpp
+++ b/src/storage/single_file_block_manager.cpp
@@ -14,6 +14,7 @@
 #include "duckdb/main/database.hpp"
 #include "duckdb/main/settings.hpp"
 #include "duckdb/storage/buffer_manager.hpp"
+#include "duckdb/storage/block_allocator.hpp"
 #include "duckdb/storage/metadata/metadata_reader.hpp"
 #include "duckdb/storage/metadata/metadata_writer.hpp"
 #include "duckdb/storage/storage_info.hpp"
@@ -66,8 +67,8 @@ void DeserializeEncryptionData(ReadStream &stream, data_t *dest, idx_t size) {
 
 void GenerateDBIdentifier(uint8_t *db_identifier) {
 	memset(db_identifier, 0, MainHeader::DB_IDENTIFIER_LEN);
-	duckdb_mbedtls::MbedTlsWrapper::AESStateMBEDTLS::GenerateRandomDataStatic(db_identifier,
-	                                                                          MainHeader::DB_IDENTIFIER_LEN);
+	RandomEngine engine;
+	engine.RandomData(db_identifier, MainHeader::DB_IDENTIFIER_LEN);
 }
 
 void EncryptCanary(MainHeader &main_header, const shared_ptr<EncryptionState> &encryption_state,
@@ -247,7 +248,7 @@ DatabaseHeader DeserializeDatabaseHeader(const MainHeader &main_header, data_ptr
 SingleFileBlockManager::SingleFileBlockManager(AttachedDatabase &db_p, const string &path_p,
                                                const StorageManagerOptions &options)
     : BlockManager(BufferManager::GetBufferManager(db_p), options.block_alloc_size, options.block_header_size),
-      db(db_p), path(path_p), header_buffer(Allocator::Get(db_p), FileBufferType::MANAGED_BUFFER,
+      db(db_p), path(path_p), header_buffer(BlockAllocator::Get(db_p), FileBufferType::MANAGED_BUFFER,
                                             Storage::FILE_HEADER_SIZE - options.block_header_size.GetIndex(),
                                             options.block_header_size.GetIndex()),
       iteration_count(0), options(options) {
@@ -361,6 +362,15 @@ void SingleFileBlockManager::CheckAndAddEncryptionKey(MainHeader &main_header) {
 void SingleFileBlockManager::CreateNewDatabase(QueryContext context) {
 	auto flags = GetFileFlags(true);
 
+	auto encryption_enabled = options.encryption_options.encryption_enabled;
+	if (encryption_enabled) {
+		if (!db.GetDatabase().GetEncryptionUtil()->SupportsEncryption() && !options.read_only) {
+			throw InvalidConfigurationException(
+			    "The database was opened with encryption enabled, but DuckDB currently has a read-only crypto module "
+			    "loaded. Please re-open using READONLY, or ensure httpfs is loaded using `LOAD httpfs`.");
+		}
+	}
+
 	// open the RDBMS handle
 	auto &fs = FileSystem::Get(db);
 	handle = fs.OpenFile(path, flags);
@@ -375,7 +385,6 @@ void SingleFileBlockManager::CreateNewDatabase(QueryContext context) {
 	// Derive the encryption key and add it to the cache.
 	// Not used for plain databases.
 	data_t derived_key[MainHeader::DEFAULT_ENCRYPTION_KEY_LENGTH];
-	auto encryption_enabled = options.encryption_options.encryption_enabled;
 
 	// We need the unique database identifier, if the storage version is new enough.
 	// If encryption is enabled, we also use it as the salt.
@@ -486,6 +495,15 @@ void SingleFileBlockManager::LoadExistingDatabase(QueryContext context) {
 	if (main_header.IsEncrypted()) {
 		if (options.encryption_options.encryption_enabled) {
 			//! Encryption is set
+
+			//! Check if our encryption module can write, if not, we should throw here
+			if (!db.GetDatabase().GetEncryptionUtil()->SupportsEncryption() && !options.read_only) {
+				throw InvalidConfigurationException(
+				    "The database is encrypted, but DuckDB currently has a read-only crypto module loaded. Either "
+				    "re-open the database using `ATTACH '..' (READONLY)`, or ensure httpfs is loaded using `LOAD "
+				    "httpfs`.");
+			}
+
 			//! Check if the given key upon attach is correct
 			// Derive the encryption key and add it to cache
 			CheckAndAddEncryptionKey(main_header);
@@ -505,6 +523,19 @@ void SingleFileBlockManager::LoadExistingDatabase(QueryContext context) {
 			                       path, EncryptionTypes::CipherToString(config_cipher),
 			                       EncryptionTypes::CipherToString(stored_cipher));
 		}
+
+		// This avoids the cipher from being downgrades by an attacker FIXME: we likely want to have a propervalidation
+		// of the cipher used instead of this trick to avoid downgrades
+		if (stored_cipher != EncryptionTypes::GCM) {
+			if (config_cipher == EncryptionTypes::INVALID) {
+				throw CatalogException(
+				    "Cannot open encrypted database \"%s\" without explicitly specifying the "
+				    "encryption cipher for security reasons. Please make sure you understand the security implications "
+				    "and re-attach the database specifying the desired cipher.",
+				    path);
+			}
+		}
+
 		// this is ugly, but the storage manager does not know the cipher type before
 		db.GetStorageManager().SetCipher(stored_cipher);
 	}
@@ -619,7 +650,7 @@ void SingleFileBlockManager::ChecksumAndWrite(QueryContext context, FileBuffer &
 	if (options.encryption_options.encryption_enabled && !skip_block_header) {
 		auto key_id = options.encryption_options.derived_key_id;
 		temp_buffer_manager =
-		    make_uniq<FileBuffer>(Allocator::Get(db), block.GetBufferType(), block.Size(), GetBlockHeaderSize());
+		    make_uniq<FileBuffer>(BlockAllocator::Get(db), block.GetBufferType(), block.Size(), GetBlockHeaderSize());
 		EncryptionEngine::EncryptBlock(db, key_id, block, *temp_buffer_manager, delta);
 		temp_buffer_manager->Write(context, *handle, location);
 	} else {
@@ -891,7 +922,7 @@ unique_ptr<Block> SingleFileBlockManager::CreateBlock(block_id_t block_id, FileB
 	if (source_buffer) {
 		result = ConvertBlock(block_id, *source_buffer);
 	} else {
-		result = make_uniq<Block>(Allocator::Get(db), block_id, *this);
+		result = make_uniq<Block>(BlockAllocator::Get(db), block_id, *this);
 	}
 	result->Initialize(options.debug_initialize);
 	return result;
diff --git a/src/storage/standard_buffer_manager.cpp b/src/storage/standard_buffer_manager.cpp
index f1170a5625..a8062b67ad 100644
--- a/src/storage/standard_buffer_manager.cpp
+++ b/src/storage/standard_buffer_manager.cpp
@@ -11,6 +11,7 @@
 #include "duckdb/storage/storage_manager.hpp"
 #include "duckdb/storage/temporary_file_manager.hpp"
 #include "duckdb/storage/temporary_memory_manager.hpp"
+#include "duckdb/storage/block_allocator.hpp"
 #include "duckdb/common/encryption_functions.hpp"
 #include "duckdb/main/settings.hpp"
 
@@ -48,7 +49,7 @@ unique_ptr<FileBuffer> StandardBufferManager::ConstructManagedBuffer(idx_t size,
 		result = make_uniq<FileBuffer>(*tmp, type, block_header_size);
 	} else {
 		// non re-usable buffer: allocate a new buffer
-		result = make_uniq<FileBuffer>(Allocator::Get(db), type, size, block_header_size);
+		result = make_uniq<FileBuffer>(BlockAllocator::Get(db), type, size, block_header_size);
 	}
 	result->Initialize(DBConfig::GetConfig(db).options.debug_initialize);
 	return result;
@@ -409,15 +410,16 @@ void StandardBufferManager::AddToEvictionQueue(shared_ptr<BlockHandle> &handle)
 void StandardBufferManager::VerifyZeroReaders(BlockLock &lock, shared_ptr<BlockHandle> &handle) {
 #ifdef DUCKDB_DEBUG_DESTROY_BLOCKS
 	unique_ptr<FileBuffer> replacement_buffer;
-	auto &allocator = Allocator::Get(db);
+	auto &block_allocator = BlockAllocator::Get(db);
 	auto &buffer = handle->GetBuffer(lock);
 	auto block_header_size = buffer->GetHeaderSize();
 	auto alloc_size = buffer->AllocSize() - block_header_size;
 	if (handle->GetBufferType() == FileBufferType::BLOCK) {
 		auto block = reinterpret_cast<Block *>(buffer.get());
-		replacement_buffer = make_uniq<Block>(allocator, block->id, alloc_size, block_header_size);
+		replacement_buffer = make_uniq<Block>(block_allocator, block->id, alloc_size, block_header_size);
 	} else {
-		replacement_buffer = make_uniq<FileBuffer>(allocator, buffer->GetBufferType(), alloc_size, block_header_size);
+		replacement_buffer =
+		    make_uniq<FileBuffer>(block_allocator, buffer->GetBufferType(), alloc_size, block_header_size);
 	}
 	memcpy(replacement_buffer->buffer, buffer->buffer, buffer->size);
 	WriteGarbageIntoBuffer(lock, *handle);
diff --git a/src/storage/storage_manager.cpp b/src/storage/storage_manager.cpp
index 37b073b5b9..8641aa3472 100644
--- a/src/storage/storage_manager.cpp
+++ b/src/storage/storage_manager.cpp
@@ -151,6 +151,14 @@ bool StorageManager::InMemory() const {
 void StorageManager::Destroy() {
 }
 
+inline void ClearUserKey(shared_ptr<string> const &encryption_key) {
+	if (encryption_key && !encryption_key->empty()) {
+		duckdb_mbedtls::MbedTlsWrapper::AESStateMBEDTLS::SecureClearData(data_ptr_cast(&(*encryption_key)[0]),
+		                                                                 encryption_key->size());
+		encryption_key->clear();
+	}
+}
+
 void StorageManager::Initialize(QueryContext context) {
 	bool in_memory = InMemory();
 	if (in_memory && read_only) {
@@ -421,11 +429,6 @@ SingleFileStorageCommitState::~SingleFileStorageCommitState() {
 	try {
 		// truncate the WAL in case of a destructor
 		RevertCommit();
-	} catch (std::exception &ex) {
-		ErrorData data(ex);
-
-		DUCKDB_LOG_ERROR(wal.GetDatabase().GetDatabase(),
-		                 "SingleFileStorageCommitState::~SingleFileStorageCommitState()\t\t" + data.Message());
 	} catch (...) { // NOLINT
 	}
 }
diff --git a/src/storage/table/array_column_data.cpp b/src/storage/table/array_column_data.cpp
index dbca6b0fef..8d0105b616 100644
--- a/src/storage/table/array_column_data.cpp
+++ b/src/storage/table/array_column_data.cpp
@@ -8,20 +8,20 @@
 
 namespace duckdb {
 
-ArrayColumnData::ArrayColumnData(BlockManager &block_manager, DataTableInfo &info, idx_t column_index, idx_t start_row,
-                                 LogicalType type_p, optional_ptr<ColumnData> parent)
-    : ColumnData(block_manager, info, column_index, start_row, std::move(type_p), parent),
-      validity(block_manager, info, 0, start_row, *this) {
+ArrayColumnData::ArrayColumnData(BlockManager &block_manager, DataTableInfo &info, idx_t column_index,
+                                 LogicalType type_p, ColumnDataType data_type, optional_ptr<ColumnData> parent)
+    : ColumnData(block_manager, info, column_index, std::move(type_p), data_type, parent),
+      validity(block_manager, info, 0, *this) {
 	D_ASSERT(type.InternalType() == PhysicalType::ARRAY);
 	auto &child_type = ArrayType::GetChildType(type);
 	// the child column, with column index 1 (0 is the validity mask)
-	child_column = ColumnData::CreateColumnUnique(block_manager, info, 1, start_row, child_type, this);
+	child_column = ColumnData::CreateColumnUnique(block_manager, info, 1, child_type, data_type, this);
 }
 
-void ArrayColumnData::SetStart(idx_t new_start) {
-	this->start = new_start;
-	child_column->SetStart(new_start);
-	validity.SetStart(new_start);
+void ArrayColumnData::SetDataType(ColumnDataType data_type) {
+	ColumnData::SetDataType(data_type);
+	child_column->SetDataType(data_type);
+	validity.SetDataType(data_type);
 }
 
 FilterPropagateResult ArrayColumnData::CheckZonemap(ColumnScanState &state, TableFilter &filter) {
@@ -41,7 +41,7 @@ void ArrayColumnData::InitializeScan(ColumnScanState &state) {
 	// initialize the validity segment
 	D_ASSERT(state.child_states.size() == 2);
 
-	state.row_index = 0;
+	state.offset_in_column = 0;
 	state.current = nullptr;
 
 	validity.InitializeScan(state.child_states[0]);
@@ -59,18 +59,18 @@ void ArrayColumnData::InitializeScanWithOffset(ColumnScanState &state, idx_t row
 		return;
 	}
 
-	state.row_index = row_idx;
+	state.offset_in_column = row_idx;
 	state.current = nullptr;
 
 	// initialize the validity segment
 	validity.InitializeScanWithOffset(state.child_states[0], row_idx);
 
 	auto array_size = ArrayType::GetSize(type);
-	auto child_count = (row_idx - start) * array_size;
+	auto child_count = row_idx * array_size;
 
 	D_ASSERT(child_count <= child_column->GetMaxEntry());
 	if (child_count < child_column->GetMaxEntry()) {
-		const auto child_offset = start + child_count;
+		const auto child_offset = child_count;
 		child_column->InitializeScanWithOffset(state.child_states[1], child_offset);
 	}
 }
@@ -210,14 +210,14 @@ void ArrayColumnData::Append(BaseStatistics &stats, ColumnAppendState &state, Ve
 	this->count += count;
 }
 
-void ArrayColumnData::RevertAppend(row_t start_row) {
+void ArrayColumnData::RevertAppend(row_t new_count) {
 	// Revert validity
-	validity.RevertAppend(start_row);
+	validity.RevertAppend(new_count);
 	// Revert child column
 	auto array_size = ArrayType::GetSize(type);
-	child_column->RevertAppend(start_row * UnsafeNumericCast<row_t>(array_size));
+	child_column->RevertAppend(new_count * UnsafeNumericCast<row_t>(array_size));
 
-	this->count = UnsafeNumericCast<idx_t>(start_row) - this->start;
+	this->count = UnsafeNumericCast<idx_t>(new_count);
 }
 
 idx_t ArrayColumnData::Fetch(ColumnScanState &state, row_t row_id, Vector &result) {
@@ -225,13 +225,13 @@ idx_t ArrayColumnData::Fetch(ColumnScanState &state, row_t row_id, Vector &resul
 }
 
 void ArrayColumnData::Update(TransactionData transaction, DataTable &data_table, idx_t column_index,
-                             Vector &update_vector, row_t *row_ids, idx_t update_count) {
+                             Vector &update_vector, row_t *row_ids, idx_t update_count, idx_t row_group_start) {
 	throw NotImplementedException("Array Update is not supported.");
 }
 
 void ArrayColumnData::UpdateColumn(TransactionData transaction, DataTable &data_table,
                                    const vector<column_t> &column_path, Vector &update_vector, row_t *row_ids,
-                                   idx_t update_count, idx_t depth) {
+                                   idx_t update_count, idx_t depth, idx_t row_group_start) {
 	throw NotImplementedException("Array Update Column is not supported");
 }
 
@@ -255,14 +255,14 @@ void ArrayColumnData::FetchRow(TransactionData transaction, ColumnFetchState &st
 	auto array_size = ArrayType::GetSize(type);
 
 	// We need to fetch between [row_id * array_size, (row_id + 1) * array_size)
-	auto child_state = make_uniq<ColumnScanState>();
-	child_state->Initialize(state.context, child_type, nullptr);
+	ColumnScanState child_state(nullptr);
+	child_state.Initialize(state.context, child_type, nullptr);
 
-	const auto child_offset = start + (UnsafeNumericCast<idx_t>(row_id) - start) * array_size;
+	const auto child_offset = UnsafeNumericCast<idx_t>(row_id) * array_size;
 
-	child_column->InitializeScanWithOffset(*child_state, child_offset);
+	child_column->InitializeScanWithOffset(child_state, child_offset);
 	Vector child_scan(child_type, array_size);
-	child_column->ScanCount(*child_state, child_scan, array_size);
+	child_column->ScanCount(child_state, child_scan, array_size);
 	VectorOperations::Copy(child_scan, child_vec, array_size, 0, result_idx * array_size);
 }
 
diff --git a/src/storage/table/column_checkpoint_state.cpp b/src/storage/table/column_checkpoint_state.cpp
index 213338d97a..26f7236b4c 100644
--- a/src/storage/table/column_checkpoint_state.cpp
+++ b/src/storage/table/column_checkpoint_state.cpp
@@ -185,7 +185,7 @@ void ColumnCheckpointState::FlushSegmentInternal(unique_ptr<ColumnSegment> segme
 	DataPointer data_pointer(segment->stats.statistics.Copy());
 	data_pointer.block_pointer.block_id = block_id;
 	data_pointer.block_pointer.offset = offset_in_block;
-	data_pointer.row_start = row_group.start;
+	data_pointer.row_start = 0;
 	if (!data_pointers.empty()) {
 		auto &last_pointer = data_pointers.back();
 		data_pointer.row_start = last_pointer.row_start + last_pointer.tuple_count;
diff --git a/src/storage/table/column_data.cpp b/src/storage/table/column_data.cpp
index 64a9fce0ee..8a11a8e6c9 100644
--- a/src/storage/table/column_data.cpp
+++ b/src/storage/table/column_data.cpp
@@ -21,10 +21,10 @@
 
 namespace duckdb {
 
-ColumnData::ColumnData(BlockManager &block_manager, DataTableInfo &info, idx_t column_index, idx_t start_row,
-                       LogicalType type_p, optional_ptr<ColumnData> parent)
-    : start(start_row), count(0), block_manager(block_manager), info(info), column_index(column_index),
-      type(std::move(type_p)), allocation_size(0), parent(parent) {
+ColumnData::ColumnData(BlockManager &block_manager, DataTableInfo &info, idx_t column_index, LogicalType type_p,
+                       ColumnDataType data_type_p, optional_ptr<ColumnData> parent)
+    : count(0), block_manager(block_manager), info(info), column_index(column_index), type(std::move(type_p)),
+      allocation_size(0), data_type(data_type_p), parent(parent) {
 	if (!parent) {
 		stats = make_uniq<SegmentStatistics>(type);
 	}
@@ -33,14 +33,8 @@ ColumnData::ColumnData(BlockManager &block_manager, DataTableInfo &info, idx_t c
 ColumnData::~ColumnData() {
 }
 
-void ColumnData::SetStart(idx_t new_start) {
-	this->start = new_start;
-	idx_t offset = 0;
-	for (auto &segment : data.Segments()) {
-		segment.start = start + offset;
-		offset += segment.count;
-	}
-	data.Reinitialize();
+void ColumnData::SetDataType(ColumnDataType data_type_p) {
+	this->data_type = data_type_p;
 }
 
 DatabaseInstance &ColumnData::GetDatabase() const {
@@ -87,7 +81,7 @@ bool ColumnData::HasChanges() const {
 			return true;
 		}
 		// persistent segment; check if there were any updates or deletions in this segment
-		idx_t start_row_idx = segment.start - start;
+		idx_t start_row_idx = nodes[segment_idx]->row_start;
 		idx_t end_row_idx = start_row_idx + segment.count;
 		if (HasChanges(start_row_idx, end_row_idx)) {
 			return true;
@@ -112,17 +106,20 @@ idx_t ColumnData::GetMaxEntry() {
 void ColumnData::InitializeScan(ColumnScanState &state) {
 	state.current = data.GetRootSegment();
 	state.segment_tree = &data;
-	state.row_index = state.current ? state.current->row_start : 0;
-	state.internal_index = state.row_index;
+	state.offset_in_column = state.current ? state.current->row_start : 0;
+	state.internal_index = state.offset_in_column;
 	state.initialized = false;
 	state.scan_state.reset();
 	state.last_offset = 0;
 }
 
 void ColumnData::InitializeScanWithOffset(ColumnScanState &state, idx_t row_idx) {
+	if (row_idx > count) {
+		throw InternalException("row_idx in InitializeScanWithOffset out of range");
+	}
 	state.current = data.GetSegment(row_idx);
 	state.segment_tree = &data;
-	state.row_index = row_idx;
+	state.offset_in_column = row_idx;
 	state.internal_index = state.current->row_start;
 	state.initialized = false;
 	state.scan_state.reset();
@@ -140,7 +137,7 @@ ScanVectorType ColumnData::GetVectorScanType(ColumnScanState &state, idx_t scan_
 	}
 	// check if the current segment has enough data remaining
 	auto &current = *state.current->node;
-	idx_t remaining_in_segment = current.start + current.count - state.row_index;
+	idx_t remaining_in_segment = state.current->row_start + current.count - state.offset_in_column;
 	if (remaining_in_segment < scan_count) {
 		// there is not enough data remaining in the current segment so we need to scan across segments
 		// we need flat vectors here
@@ -158,10 +155,10 @@ void ColumnData::InitializePrefetch(PrefetchState &prefetch_state, ColumnScanSta
 		// need to prefetch for the current segment if we have not yet initialized the scan for this segment
 		current_segment->node->InitializePrefetch(prefetch_state, scan_state);
 	}
-	idx_t row_index = scan_state.row_index;
+	idx_t row_index = scan_state.offset_in_column;
 	while (remaining > 0) {
 		auto &current = *current_segment->node;
-		idx_t scan_count = MinValue<idx_t>(remaining, current.start + current.count - row_index);
+		idx_t scan_count = MinValue<idx_t>(remaining, current_segment->row_start + current.count - row_index);
 		remaining -= scan_count;
 		row_index += scan_count;
 		if (remaining > 0) {
@@ -180,12 +177,12 @@ void ColumnData::BeginScanVectorInternal(ColumnScanState &state) {
 	if (!state.initialized) {
 		auto &current = *state.current->node;
 		current.InitializeScan(state);
-		state.internal_index = current.start;
+		state.internal_index = state.current->row_start;
 		state.initialized = true;
 	}
 	D_ASSERT(data.HasSegment(*state.current));
-	D_ASSERT(state.internal_index <= state.row_index);
-	if (state.internal_index < state.row_index) {
+	D_ASSERT(state.internal_index <= state.offset_in_column);
+	if (state.internal_index < state.offset_in_column) {
 		auto &current = *state.current->node;
 		current.Skip(state);
 	}
@@ -201,21 +198,22 @@ idx_t ColumnData::ScanVector(ColumnScanState &state, Vector &result, idx_t remai
 	idx_t initial_remaining = remaining;
 	while (remaining > 0) {
 		auto &current = *state.current->node;
-		D_ASSERT(state.row_index >= current.start && state.row_index <= current.start + current.count);
-		idx_t scan_count = MinValue<idx_t>(remaining, current.start + current.count - state.row_index);
+		auto current_start = state.current->row_start;
+		D_ASSERT(state.offset_in_column >= current_start && state.offset_in_column <= current_start + current.count);
+		idx_t scan_count = MinValue<idx_t>(remaining, current_start + current.count - state.offset_in_column);
 		idx_t result_offset = base_result_offset + initial_remaining - remaining;
 		if (scan_count > 0) {
 			if (state.scan_options && state.scan_options->force_fetch_row) {
 				for (idx_t i = 0; i < scan_count; i++) {
 					ColumnFetchState fetch_state;
-					current.FetchRow(fetch_state, UnsafeNumericCast<row_t>(state.row_index + i), result,
-					                 result_offset + i);
+					current.FetchRow(fetch_state, UnsafeNumericCast<row_t>(state.offset_in_column + i - current_start),
+					                 result, result_offset + i);
 				}
 			} else {
 				current.Scan(state, scan_count, result, result_offset, scan_type);
 			}
 
-			state.row_index += scan_count;
+			state.offset_in_column += scan_count;
 			remaining -= scan_count;
 		}
 
@@ -228,11 +226,11 @@ idx_t ColumnData::ScanVector(ColumnScanState &state, Vector &result, idx_t remai
 			state.current = next;
 			state.current->node->InitializeScan(state);
 			state.segment_checked = false;
-			D_ASSERT(state.row_index >= state.current->node->start &&
-			         state.row_index <= state.current->node->start + state.current->node->count);
+			D_ASSERT(state.offset_in_column >= state.current->row_start &&
+			         state.offset_in_column <= state.current->row_start + state.current->node->count);
 		}
 	}
-	state.internal_index = state.row_index;
+	state.internal_index = state.offset_in_column;
 	return initial_remaining - remaining;
 }
 
@@ -240,32 +238,32 @@ void ColumnData::SelectVector(ColumnScanState &state, Vector &result, idx_t targ
                               idx_t sel_count) {
 	BeginScanVectorInternal(state);
 	auto &current = *state.current->node;
-	if (current.start + current.count - state.row_index < target_count) {
+	if (state.current->row_start + current.count - state.offset_in_column < target_count) {
 		throw InternalException("ColumnData::SelectVector should be able to fetch everything from one segment");
 	}
 	if (state.scan_options && state.scan_options->force_fetch_row) {
 		for (idx_t i = 0; i < sel_count; i++) {
 			auto source_idx = sel.get_index(i);
 			ColumnFetchState fetch_state;
-			current.FetchRow(fetch_state, UnsafeNumericCast<row_t>(state.row_index + source_idx), result, i);
+			current.FetchRow(fetch_state, UnsafeNumericCast<row_t>(state.offset_in_column + source_idx), result, i);
 		}
 	} else {
 		current.Select(state, target_count, result, sel, sel_count);
 	}
-	state.row_index += target_count;
-	state.internal_index = state.row_index;
+	state.offset_in_column += target_count;
+	state.internal_index = state.offset_in_column;
 }
 
 void ColumnData::FilterVector(ColumnScanState &state, Vector &result, idx_t target_count, SelectionVector &sel,
                               idx_t &sel_count, const TableFilter &filter, TableFilterState &filter_state) {
 	BeginScanVectorInternal(state);
 	auto &current = *state.current->node;
-	if (current.start + current.count - state.row_index < target_count) {
+	if (state.current->row_start + current.count - state.offset_in_column < target_count) {
 		throw InternalException("ColumnData::Filter should be able to fetch everything from one segment");
 	}
 	current.Filter(state, target_count, result, sel, sel_count, filter, filter_state);
-	state.row_index += target_count;
-	state.internal_index = state.row_index;
+	state.offset_in_column += target_count;
+	state.internal_index = state.offset_in_column;
 }
 
 unique_ptr<BaseStatistics> ColumnData::GetUpdateStatistics() {
@@ -299,12 +297,14 @@ void ColumnData::FetchUpdateRow(TransactionData transaction, row_t row_id, Vecto
 }
 
 void ColumnData::UpdateInternal(TransactionData transaction, DataTable &data_table, idx_t column_index,
-                                Vector &update_vector, row_t *row_ids, idx_t update_count, Vector &base_vector) {
+                                Vector &update_vector, row_t *row_ids, idx_t update_count, Vector &base_vector,
+                                idx_t row_group_start) {
 	lock_guard<mutex> update_guard(update_lock);
 	if (!updates) {
 		updates = make_uniq<UpdateSegment>(*this);
 	}
-	updates->Update(transaction, data_table, column_index, update_vector, row_ids, update_count, base_vector);
+	updates->Update(transaction, data_table, column_index, update_vector, row_ids, update_count, base_vector,
+	                row_group_start);
 }
 
 idx_t ColumnData::ScanVector(TransactionData transaction, idx_t vector_index, ColumnScanState &state, Vector &result,
@@ -353,8 +353,8 @@ idx_t ColumnData::GetVectorCount(idx_t vector_index) const {
 }
 
 void ColumnData::ScanCommittedRange(idx_t row_group_start, idx_t offset_in_row_group, idx_t s_count, Vector &result) {
-	ColumnScanState child_state;
-	InitializeScanWithOffset(child_state, row_group_start + offset_in_row_group);
+	ColumnScanState child_state(nullptr);
+	InitializeScanWithOffset(child_state, offset_in_row_group);
 	bool has_updates = HasUpdates();
 	auto scan_count = ScanVector(child_state, result, s_count, ScanVectorType::SCAN_FLAT_VECTOR);
 	if (has_updates) {
@@ -480,14 +480,14 @@ void ColumnData::InitializeAppend(ColumnAppendState &state) {
 	auto l = data.Lock();
 	if (data.IsEmpty(l)) {
 		// no segments yet, append an empty segment
-		AppendTransientSegment(l, start);
+		AppendTransientSegment(l, 0);
 	}
 	auto segment = data.GetLastSegment(l);
 	auto &last_segment = *segment->node;
 	if (last_segment.segment_type == ColumnSegmentType::PERSISTENT ||
 	    !last_segment.GetCompressionFunction().init_append) {
 		// we cannot append to this segment - append a new segment
-		auto total_rows = last_segment.start + last_segment.count;
+		auto total_rows = segment->row_start + last_segment.count;
 		AppendTransientSegment(l, total_rows);
 		state.current = data.GetLastSegment(l);
 	} else {
@@ -516,7 +516,7 @@ void ColumnData::AppendData(BaseStatistics &append_stats, ColumnAppendState &sta
 		// we couldn't fit everything we wanted in the current column segment, create a new one
 		{
 			auto l = data.Lock();
-			AppendTransientSegment(l, append_segment.start + append_segment.count);
+			AppendTransientSegment(l, state.current->row_start + append_segment.count);
 			state.current = data.GetLastSegment(l);
 			state.current->node->InitializeAppend(state);
 		}
@@ -525,8 +525,8 @@ void ColumnData::AppendData(BaseStatistics &append_stats, ColumnAppendState &sta
 	}
 }
 
-void ColumnData::RevertAppend(row_t start_row_p) {
-	idx_t start_row = NumericCast<idx_t>(start_row_p);
+void ColumnData::RevertAppend(row_t new_count_p) {
+	idx_t new_count = NumericCast<idx_t>(new_count_p);
 	auto l = data.Lock();
 	// check if this row is in the segment tree at all
 	auto last_segment_node = data.GetLastSegment(l);
@@ -534,15 +534,15 @@ void ColumnData::RevertAppend(row_t start_row_p) {
 		return;
 	}
 	auto &last_segment = *last_segment_node->node;
-	if (start_row >= last_segment.start + last_segment.count) {
+	if (new_count >= last_segment_node->row_start + last_segment.count) {
 		// the start row is equal to the final portion of the column data: nothing was ever appended here
-		D_ASSERT(start_row == last_segment.start + last_segment.count);
+		D_ASSERT(new_count == last_segment_node->row_start + last_segment.count);
 		return;
 	}
 	// find the segment index that the current row belongs to
-	idx_t segment_index = data.GetSegmentIndex(l, start_row);
+	idx_t segment_index = data.GetSegmentIndex(l, new_count);
 	auto segment = data.GetSegmentByIndex(l, UnsafeNumericCast<int64_t>(segment_index));
-	if (segment->node->start == start_row) {
+	if (segment->row_start == new_count) {
 		// we are truncating exactly this segment - erase it entirely
 		data.EraseSegments(l, segment_index);
 	} else {
@@ -553,54 +553,64 @@ void ColumnData::RevertAppend(row_t start_row_p) {
 		auto &transient = *segment->node;
 		D_ASSERT(transient.segment_type == ColumnSegmentType::TRANSIENT);
 		segment->next = nullptr;
-		transient.RevertAppend(start_row);
+		transient.RevertAppend(new_count - segment->row_start);
 	}
 
-	this->count = start_row - this->start;
+	this->count = new_count;
 }
 
 idx_t ColumnData::Fetch(ColumnScanState &state, row_t row_id, Vector &result) {
+	if (UnsafeNumericCast<idx_t>(row_id) > count) {
+		throw InternalException("ColumnData::Fetch - row_id out of range");
+	}
 	D_ASSERT(row_id >= 0);
-	D_ASSERT(NumericCast<idx_t>(row_id) >= start);
 	// perform the fetch within the segment
-	state.row_index =
-	    start + ((UnsafeNumericCast<idx_t>(row_id) - start) / STANDARD_VECTOR_SIZE * STANDARD_VECTOR_SIZE);
-	state.current = data.GetSegment(state.row_index);
-	state.internal_index = state.current->node->start;
+	state.offset_in_column = UnsafeNumericCast<idx_t>(row_id) / STANDARD_VECTOR_SIZE * STANDARD_VECTOR_SIZE;
+	state.current = data.GetSegment(state.offset_in_column);
+	state.internal_index = state.current->row_start;
 	return ScanVector(state, result, STANDARD_VECTOR_SIZE, ScanVectorType::SCAN_FLAT_VECTOR);
 }
 
 void ColumnData::FetchRow(TransactionData transaction, ColumnFetchState &state, row_t row_id, Vector &result,
                           idx_t result_idx) {
+	if (UnsafeNumericCast<idx_t>(row_id) > count) {
+		throw InternalException("ColumnData::FetchRow - row_id out of range");
+	}
 	auto segment = data.GetSegment(UnsafeNumericCast<idx_t>(row_id));
 
 	// now perform the fetch within the segment
-	segment->node->FetchRow(state, row_id, result, result_idx);
+	auto index_in_segment = row_id - UnsafeNumericCast<row_t>(segment->row_start);
+	segment->node->FetchRow(state, index_in_segment, result, result_idx);
 	// merge any updates made to this row
 
 	FetchUpdateRow(transaction, row_id, result, result_idx);
 }
 
-idx_t ColumnData::FetchUpdateData(ColumnScanState &state, row_t *row_ids, Vector &base_vector) {
-	auto fetch_count = ColumnData::Fetch(state, row_ids[0], base_vector);
+idx_t ColumnData::FetchUpdateData(ColumnScanState &state, row_t *row_ids, Vector &base_vector, idx_t row_group_start) {
+	if (row_ids[0] < UnsafeNumericCast<row_t>(row_group_start)) {
+		throw InternalException("ColumnData::FetchUpdateData out of range");
+	}
+	auto fetch_count = ColumnData::Fetch(state, row_ids[0] - UnsafeNumericCast<row_t>(row_group_start), base_vector);
 	base_vector.Flatten(fetch_count);
 	return fetch_count;
 }
 
 void ColumnData::Update(TransactionData transaction, DataTable &data_table, idx_t column_index, Vector &update_vector,
-                        row_t *row_ids, idx_t update_count) {
+                        row_t *row_ids, idx_t update_count, idx_t row_group_start) {
 	Vector base_vector(type);
-	ColumnScanState state;
-	FetchUpdateData(state, row_ids, base_vector);
+	ColumnScanState state(nullptr);
+	FetchUpdateData(state, row_ids, base_vector, row_group_start);
 
-	UpdateInternal(transaction, data_table, column_index, update_vector, row_ids, update_count, base_vector);
+	UpdateInternal(transaction, data_table, column_index, update_vector, row_ids, update_count, base_vector,
+	               row_group_start);
 }
 
 void ColumnData::UpdateColumn(TransactionData transaction, DataTable &data_table, const vector<column_t> &column_path,
-                              Vector &update_vector, row_t *row_ids, idx_t update_count, idx_t depth) {
+                              Vector &update_vector, row_t *row_ids, idx_t update_count, idx_t depth,
+                              idx_t row_group_start) {
 	// this method should only be called at the end of the path in the base column case
 	D_ASSERT(depth >= column_path.size());
-	ColumnData::Update(transaction, data_table, column_path[0], update_vector, row_ids, update_count);
+	ColumnData::Update(transaction, data_table, column_path[0], update_vector, row_ids, update_count, row_group_start);
 }
 
 void ColumnData::AppendTransientSegment(SegmentLock &l, idx_t start_row) {
@@ -608,7 +618,7 @@ void ColumnData::AppendTransientSegment(SegmentLock &l, idx_t start_row) {
 	const auto type_size = GetTypeIdSize(type.InternalType());
 	auto vector_segment_size = block_size;
 
-	if (start_row == NumericCast<idx_t>(MAX_ROW_ID)) {
+	if (data_type == ColumnDataType::INITIAL_TRANSACTION_LOCAL && start_row == 0) {
 #if STANDARD_VECTOR_SIZE < 1024
 		vector_segment_size = 1024 * type_size;
 #else
@@ -624,8 +634,7 @@ void ColumnData::AppendTransientSegment(SegmentLock &l, idx_t start_row) {
 	auto &config = DBConfig::GetConfig(db);
 	auto function = config.GetCompressionFunction(CompressionType::COMPRESSION_UNCOMPRESSED, type.InternalType());
 
-	auto new_segment =
-	    ColumnSegment::CreateTransientSegment(db, *function, type, start_row, segment_size, block_manager);
+	auto new_segment = ColumnSegment::CreateTransientSegment(db, *function, type, segment_size, block_manager);
 	AppendSegment(l, std::move(new_segment));
 }
 
@@ -661,12 +670,12 @@ unique_ptr<ColumnCheckpointState> ColumnData::CreateCheckpointState(RowGroup &ro
 	return make_uniq<ColumnCheckpointState>(row_group, *this, partial_block_manager);
 }
 
-void ColumnData::CheckpointScan(ColumnSegment &segment, ColumnScanState &state, idx_t row_group_start, idx_t count,
-                                Vector &scan_vector) {
+void ColumnData::CheckpointScan(ColumnSegment &segment, ColumnScanState &state, idx_t count, Vector &scan_vector) {
 	if (state.scan_options && state.scan_options->force_fetch_row) {
 		for (idx_t i = 0; i < count; i++) {
 			ColumnFetchState fetch_state;
-			segment.FetchRow(fetch_state, UnsafeNumericCast<row_t>(state.row_index + i), scan_vector, i);
+			fetch_state.row_group = state.parent->row_group;
+			segment.FetchRow(fetch_state, UnsafeNumericCast<row_t>(state.offset_in_column + i), scan_vector, i);
 		}
 	} else {
 		segment.Scan(state, count, scan_vector, 0, ScanVectorType::SCAN_FLAT_VECTOR);
@@ -674,7 +683,7 @@ void ColumnData::CheckpointScan(ColumnSegment &segment, ColumnScanState &state,
 
 	if (updates) {
 		D_ASSERT(scan_vector.GetVectorType() == VectorType::FLAT_VECTOR);
-		updates->FetchCommittedRange(state.row_index - row_group_start, count, scan_vector);
+		updates->FetchCommittedRange(state.offset_in_column, count, scan_vector);
 	}
 }
 
@@ -708,7 +717,7 @@ void ColumnData::InitializeColumn(PersistentColumnData &column_data, BaseStatist
 	this->count = 0;
 	for (auto &data_pointer : column_data.pointers) {
 		// Update the count and statistics
-		data_pointer.row_start = start + count;
+		data_pointer.row_start = count;
 		this->count += data_pointer.tuple_count;
 
 		// Merge the statistics. If this is a child column, the target_stats reference will point into the parents stats
@@ -719,8 +728,8 @@ void ColumnData::InitializeColumn(PersistentColumnData &column_data, BaseStatist
 		// create a persistent segment
 		auto segment = ColumnSegment::CreatePersistentSegment(
 		    GetDatabase(), block_manager, data_pointer.block_pointer.block_id, data_pointer.block_pointer.offset, type,
-		    data_pointer.row_start, data_pointer.tuple_count, data_pointer.compression_type,
-		    std::move(data_pointer.statistics), std::move(data_pointer.segment_state));
+		    data_pointer.tuple_count, data_pointer.compression_type, std::move(data_pointer.statistics),
+		    std::move(data_pointer.segment_state));
 
 		auto l = data.Lock();
 		AppendSegment(l, std::move(segment));
@@ -738,8 +747,10 @@ bool ColumnData::IsPersistent() {
 
 vector<DataPointer> ColumnData::GetDataPointers() {
 	vector<DataPointer> pointers;
+	idx_t row_start = 0;
 	for (auto &segment : data.Segments()) {
-		pointers.push_back(segment.GetDataPointer());
+		pointers.push_back(segment.GetDataPointer(row_start));
+		row_start += segment.count;
 	}
 	return pointers;
 }
@@ -878,25 +889,15 @@ bool PersistentCollectionData::HasUpdates() const {
 }
 
 PersistentColumnData ColumnData::Serialize() {
-	PersistentColumnData result(type.InternalType(), GetDataPointers());
+	auto result = count ? PersistentColumnData(type.InternalType(), GetDataPointers())
+	                    : PersistentColumnData(type.InternalType());
 	result.has_updates = HasUpdates();
 	return result;
 }
 
-void RealignColumnData(PersistentColumnData &column_data, idx_t new_start) {
-	idx_t current_start = new_start;
-	for (auto &pointer : column_data.pointers) {
-		pointer.row_start = current_start;
-		current_start += pointer.tuple_count;
-	}
-	for (auto &child : column_data.child_columns) {
-		RealignColumnData(child, new_start);
-	}
-}
-
 shared_ptr<ColumnData> ColumnData::Deserialize(BlockManager &block_manager, DataTableInfo &info, idx_t column_index,
-                                               idx_t start_row, ReadStream &source, const LogicalType &type) {
-	auto entry = ColumnData::CreateColumn(block_manager, info, column_index, start_row, type, nullptr);
+                                               ReadStream &source, const LogicalType &type) {
+	auto entry = ColumnData::CreateColumn(block_manager, info, column_index, type);
 
 	// deserialize the persistent column data
 	BinaryDeserializer deserializer(source);
@@ -911,9 +912,6 @@ shared_ptr<ColumnData> ColumnData::Deserialize(BlockManager &block_manager, Data
 	deserializer.Unset<DatabaseInstance>();
 	deserializer.End();
 
-	// re-align data segments, in case our start_row has changed
-	RealignColumnData(persistent_column_data, start_row);
-
 	// initialize the column
 	entry->InitializeColumn(persistent_column_data, entry->stats->statistics);
 	return entry;
@@ -935,14 +933,15 @@ void ColumnData::GetColumnSegmentInfo(const QueryContext &context, idx_t row_gro
 
 	// iterate over the segments
 	idx_t segment_idx = 0;
-	for (auto &segment : data.Segments()) {
+	for (auto &segment_node : data.SegmentNodes()) {
+		auto &segment = *segment_node.node;
 		ColumnSegmentInfo column_info;
 		column_info.row_group_index = row_group_index;
 		column_info.column_id = col_path[0];
 		column_info.column_path = col_path_str;
 		column_info.segment_idx = segment_idx;
 		column_info.segment_type = type.ToString();
-		column_info.segment_start = segment.start;
+		column_info.segment_start = segment_node.row_start;
 		column_info.segment_count = segment.count;
 		column_info.compression_type = CompressionTypeToString(segment.GetCompressionFunction().type);
 		{
@@ -984,7 +983,6 @@ void ColumnData::GetColumnSegmentInfo(const QueryContext &context, idx_t row_gro
 
 void ColumnData::Verify(RowGroup &parent) {
 #ifdef DEBUG
-	D_ASSERT(this->start == parent.start);
 	data.Verify();
 	if (type.InternalType() == PhysicalType::STRUCT || type.InternalType() == PhysicalType::ARRAY) {
 		// structs and fixed size lists don't have segments
@@ -992,7 +990,7 @@ void ColumnData::Verify(RowGroup &parent) {
 		return;
 	}
 	idx_t current_index = 0;
-	idx_t current_start = this->start;
+	idx_t current_start = 0;
 	idx_t total_count = 0;
 	for (auto &segment : data.SegmentNodes()) {
 		D_ASSERT(segment.index == current_index);
@@ -1006,32 +1004,32 @@ void ColumnData::Verify(RowGroup &parent) {
 }
 
 template <class RET, class OP>
-static RET CreateColumnInternal(BlockManager &block_manager, DataTableInfo &info, idx_t column_index, idx_t start_row,
-                                const LogicalType &type, optional_ptr<ColumnData> parent) {
+static RET CreateColumnInternal(BlockManager &block_manager, DataTableInfo &info, idx_t column_index,
+                                const LogicalType &type, ColumnDataType data_type, optional_ptr<ColumnData> parent) {
 	if (type.InternalType() == PhysicalType::STRUCT) {
-		return OP::template Create<StructColumnData>(block_manager, info, column_index, start_row, type, parent);
+		return OP::template Create<StructColumnData>(block_manager, info, column_index, type, data_type, parent);
 	} else if (type.InternalType() == PhysicalType::LIST) {
-		return OP::template Create<ListColumnData>(block_manager, info, column_index, start_row, type, parent);
+		return OP::template Create<ListColumnData>(block_manager, info, column_index, type, data_type, parent);
 	} else if (type.InternalType() == PhysicalType::ARRAY) {
-		return OP::template Create<ArrayColumnData>(block_manager, info, column_index, start_row, type, parent);
+		return OP::template Create<ArrayColumnData>(block_manager, info, column_index, type, data_type, parent);
 	} else if (type.id() == LogicalTypeId::VALIDITY) {
-		return OP::template Create<ValidityColumnData>(block_manager, info, column_index, start_row, *parent);
+		return OP::template Create<ValidityColumnData>(block_manager, info, column_index, *parent);
 	}
-	return OP::template Create<StandardColumnData>(block_manager, info, column_index, start_row, type, parent);
+	return OP::template Create<StandardColumnData>(block_manager, info, column_index, type, data_type, parent);
 }
 
 shared_ptr<ColumnData> ColumnData::CreateColumn(BlockManager &block_manager, DataTableInfo &info, idx_t column_index,
-                                                idx_t start_row, const LogicalType &type,
+                                                const LogicalType &type, ColumnDataType data_type,
                                                 optional_ptr<ColumnData> parent) {
-	return CreateColumnInternal<shared_ptr<ColumnData>, SharedConstructor>(block_manager, info, column_index, start_row,
-	                                                                       type, parent);
+	return CreateColumnInternal<shared_ptr<ColumnData>, SharedConstructor>(block_manager, info, column_index, type,
+	                                                                       data_type, parent);
 }
 
 unique_ptr<ColumnData> ColumnData::CreateColumnUnique(BlockManager &block_manager, DataTableInfo &info,
-                                                      idx_t column_index, idx_t start_row, const LogicalType &type,
-                                                      optional_ptr<ColumnData> parent) {
-	return CreateColumnInternal<unique_ptr<ColumnData>, UniqueConstructor>(block_manager, info, column_index, start_row,
-	                                                                       type, parent);
+                                                      idx_t column_index, const LogicalType &type,
+                                                      ColumnDataType data_type, optional_ptr<ColumnData> parent) {
+	return CreateColumnInternal<unique_ptr<ColumnData>, UniqueConstructor>(block_manager, info, column_index, type,
+	                                                                       data_type, parent);
 }
 
 } // namespace duckdb
diff --git a/src/storage/table/column_data_checkpointer.cpp b/src/storage/table/column_data_checkpointer.cpp
index 0ad658f2c5..15cc1f139e 100644
--- a/src/storage/table/column_data_checkpointer.cpp
+++ b/src/storage/table/column_data_checkpointer.cpp
@@ -88,7 +88,7 @@ void ColumnDataCheckpointer::ScanSegments(const std::function<void(Vector &, idx
 	for (idx_t segment_idx = 0; segment_idx < nodes.size(); segment_idx++) {
 		auto &segment_node = *nodes[segment_idx];
 		auto &segment = *segment_node.node;
-		ColumnScanState scan_state;
+		ColumnScanState scan_state(nullptr);
 		scan_state.current = segment_node;
 		segment.InitializeScan(scan_state);
 
@@ -96,9 +96,9 @@ void ColumnDataCheckpointer::ScanSegments(const std::function<void(Vector &, idx
 			scan_vector.Reference(intermediate);
 
 			idx_t count = MinValue<idx_t>(segment.count - base_row_index, STANDARD_VECTOR_SIZE);
-			scan_state.row_index = segment.start + base_row_index;
+			scan_state.offset_in_column = segment_node.row_start + base_row_index;
 
-			col_data.CheckpointScan(segment, scan_state, row_group.start, count, scan_vector);
+			col_data.CheckpointScan(segment, scan_state, count, scan_vector);
 			callback(scan_vector, count);
 		}
 	}
@@ -373,14 +373,15 @@ void ColumnDataCheckpointer::WritePersistentSegments(ColumnCheckpointState &stat
 	auto &col_data = state.column_data;
 	auto nodes = col_data.data.MoveSegments();
 
-	idx_t current_row = row_group.start;
+	idx_t current_row = 0;
 	for (idx_t segment_idx = 0; segment_idx < nodes.size(); segment_idx++) {
 		auto &segment = *nodes[segment_idx]->node;
-		if (segment.start != current_row) {
+		auto segment_start = nodes[segment_idx]->row_start;
+		if (segment_start != current_row) {
 			string extra_info;
 			for (auto &s : nodes) {
 				extra_info += "\n";
-				extra_info += StringUtil::Format("Start %d, count %d", segment.start, segment.count.load());
+				extra_info += StringUtil::Format("Start %d, count %d", segment_start, segment.count.load());
 			}
 			const_reference<ColumnData> root = col_data;
 			while (root.get().HasParent()) {
@@ -390,12 +391,11 @@ void ColumnDataCheckpointer::WritePersistentSegments(ColumnCheckpointState &stat
 			    "Failure in RowGroup::Checkpoint - column data pointer is unaligned with row group "
 			    "start\nRow group start: %d\nRow group count %d\nCurrent row: %d\nSegment start: %d\nColumn index: "
 			    "%d\nColumn type: %s\nRoot type: %s\nTable: %s.%s\nAll segments:%s",
-			    row_group.start, row_group.count.load(), current_row, segment.start, root.get().column_index,
-			    col_data.type, root.get().type, root.get().info.GetSchemaName(), root.get().info.GetTableName(),
-			    extra_info);
+			    row_group.count.load(), current_row, segment_start, root.get().column_index, col_data.type,
+			    root.get().type, root.get().info.GetSchemaName(), root.get().info.GetTableName(), extra_info);
 		}
+		auto pointer = segment.GetDataPointer(current_row);
 		current_row += segment.count;
-		auto pointer = segment.GetDataPointer();
 
 		// merge the persistent stats into the global column stats
 		state.global_stats->Merge(segment.stats.statistics);
diff --git a/src/storage/table/column_segment.cpp b/src/storage/table/column_segment.cpp
index df0c665f92..3cc96a599a 100644
--- a/src/storage/table/column_segment.cpp
+++ b/src/storage/table/column_segment.cpp
@@ -26,7 +26,7 @@ namespace duckdb {
 
 unique_ptr<ColumnSegment> ColumnSegment::CreatePersistentSegment(DatabaseInstance &db, BlockManager &block_manager,
                                                                  block_id_t block_id, idx_t offset,
-                                                                 const LogicalType &type, idx_t start, idx_t count,
+                                                                 const LogicalType &type, idx_t count,
                                                                  CompressionType compression_type,
                                                                  BaseStatistics statistics,
                                                                  unique_ptr<ColumnSegmentState> segment_state) {
@@ -40,19 +40,19 @@ unique_ptr<ColumnSegment> ColumnSegment::CreatePersistentSegment(DatabaseInstanc
 	}
 
 	auto segment_size = block_manager.GetBlockSize();
-	return make_uniq<ColumnSegment>(db, std::move(block), type, ColumnSegmentType::PERSISTENT, start, count, *function,
+	return make_uniq<ColumnSegment>(db, std::move(block), type, ColumnSegmentType::PERSISTENT, count, *function,
 	                                std::move(statistics), block_id, offset, segment_size, std::move(segment_state));
 }
 
 unique_ptr<ColumnSegment> ColumnSegment::CreateTransientSegment(DatabaseInstance &db, CompressionFunction &function,
-                                                                const LogicalType &type, const idx_t start,
-                                                                const idx_t segment_size, BlockManager &block_manager) {
+                                                                const LogicalType &type, const idx_t segment_size,
+                                                                BlockManager &block_manager) {
 	// Allocate a buffer for the uncompressed segment.
 	auto &buffer_manager = BufferManager::GetBufferManager(db);
 	D_ASSERT(&buffer_manager == &block_manager.buffer_manager);
 	auto block = buffer_manager.RegisterTransientMemory(segment_size, block_manager);
 
-	return make_uniq<ColumnSegment>(db, std::move(block), type, ColumnSegmentType::TRANSIENT, start, 0U, function,
+	return make_uniq<ColumnSegment>(db, std::move(block), type, ColumnSegmentType::TRANSIENT, 0U, function,
 	                                BaseStatistics::CreateEmpty(type), INVALID_BLOCK, 0U, segment_size);
 }
 
@@ -60,12 +60,11 @@ unique_ptr<ColumnSegment> ColumnSegment::CreateTransientSegment(DatabaseInstance
 // Construct/Destruct
 //===--------------------------------------------------------------------===//
 ColumnSegment::ColumnSegment(DatabaseInstance &db, shared_ptr<BlockHandle> block_p, const LogicalType &type,
-                             const ColumnSegmentType segment_type, const idx_t start, const idx_t count,
-                             CompressionFunction &function_p, BaseStatistics statistics, const block_id_t block_id_p,
-                             const idx_t offset, const idx_t segment_size_p,
-                             const unique_ptr<ColumnSegmentState> segment_state_p)
+                             const ColumnSegmentType segment_type, const idx_t count, CompressionFunction &function_p,
+                             BaseStatistics statistics, const block_id_t block_id_p, const idx_t offset,
+                             const idx_t segment_size_p, const unique_ptr<ColumnSegmentState> segment_state_p)
 
-    : SegmentBase<ColumnSegment>(start, count), db(db), type(type), type_size(GetTypeIdSize(type.InternalType())),
+    : SegmentBase<ColumnSegment>(count), db(db), type(type), type_size(GetTypeIdSize(type.InternalType())),
       segment_type(segment_type), stats(std::move(statistics)), block(std::move(block_p)), function(function_p),
       block_id(block_id_p), offset(offset), segment_size(segment_size_p) {
 	if (function.get().init_segment) {
@@ -76,8 +75,8 @@ ColumnSegment::ColumnSegment(DatabaseInstance &db, shared_ptr<BlockHandle> block
 	D_ASSERT(!block || segment_size <= GetBlockManager().GetBlockSize());
 }
 
-ColumnSegment::ColumnSegment(ColumnSegment &other, const idx_t start)
-    : SegmentBase<ColumnSegment>(start, other.count.load()), db(other.db), type(std::move(other.type)),
+ColumnSegment::ColumnSegment(ColumnSegment &other)
+    : SegmentBase<ColumnSegment>(other.count.load()), db(other.db), type(std::move(other.type)),
       type_size(other.type_size), segment_type(other.segment_type), stats(std::move(other.stats)),
       block(std::move(other.block)), function(other.function), block_id(other.block_id), offset(other.offset),
       segment_size(other.segment_size), segment_state(std::move(other.segment_state)) {
@@ -136,8 +135,8 @@ void ColumnSegment::Filter(ColumnScanState &state, idx_t scan_count, Vector &res
 }
 
 void ColumnSegment::Skip(ColumnScanState &state) {
-	function.get().skip(*this, state, state.row_index - state.internal_index);
-	state.internal_index = state.row_index;
+	function.get().skip(*this, state, state.offset_in_column - state.internal_index);
+	state.internal_index = state.offset_in_column;
 }
 
 void ColumnSegment::Scan(ColumnScanState &state, idx_t scan_count, Vector &result) {
@@ -152,8 +151,10 @@ void ColumnSegment::ScanPartial(ColumnScanState &state, idx_t scan_count, Vector
 // Fetch
 //===--------------------------------------------------------------------===//
 void ColumnSegment::FetchRow(ColumnFetchState &state, row_t row_id, Vector &result, idx_t result_idx) {
-	function.get().fetch_row(*this, state, UnsafeNumericCast<int64_t>(UnsafeNumericCast<idx_t>(row_id) - this->start),
-	                         result, result_idx);
+	if (UnsafeNumericCast<idx_t>(row_id) > count) {
+		throw InternalException("ColumnSegment::FetchRow - row_id out of range for segment");
+	}
+	function.get().fetch_row(*this, state, row_id, result, result_idx);
 }
 
 //===--------------------------------------------------------------------===//
@@ -205,12 +206,12 @@ idx_t ColumnSegment::FinalizeAppend(ColumnAppendState &state) {
 	return result_count;
 }
 
-void ColumnSegment::RevertAppend(idx_t start_row) {
+void ColumnSegment::RevertAppend(idx_t new_count) {
 	D_ASSERT(segment_type == ColumnSegmentType::TRANSIENT);
 	if (function.get().revert_append) {
-		function.get().revert_append(*this, start_row);
+		function.get().revert_append(*this, new_count);
 	}
-	this->count = start_row - this->start;
+	this->count = new_count;
 }
 
 //===--------------------------------------------------------------------===//
@@ -253,7 +254,7 @@ void ColumnSegment::SetBlock(shared_ptr<BlockHandle> block_p, uint32_t offset_p)
 	block = std::move(block_p);
 }
 
-DataPointer ColumnSegment::GetDataPointer() {
+DataPointer ColumnSegment::GetDataPointer(idx_t row_start) {
 	if (segment_type != ColumnSegmentType::PERSISTENT) {
 		throw InternalException("Attempting to call ColumnSegment::GetDataPointer on a transient segment");
 	}
@@ -261,7 +262,7 @@ DataPointer ColumnSegment::GetDataPointer() {
 	DataPointer pointer(stats.statistics.Copy());
 	pointer.block_pointer.block_id = GetBlockId();
 	pointer.block_pointer.offset = NumericCast<uint32_t>(GetBlockOffset());
-	pointer.row_start = start;
+	pointer.row_start = row_start;
 	pointer.tuple_count = count;
 	pointer.compression_type = function.get().type;
 	if (function.get().serialize_state) {
diff --git a/src/storage/table/list_column_data.cpp b/src/storage/table/list_column_data.cpp
index 0d9793e9c2..8685ae4f73 100644
--- a/src/storage/table/list_column_data.cpp
+++ b/src/storage/table/list_column_data.cpp
@@ -8,20 +8,20 @@
 
 namespace duckdb {
 
-ListColumnData::ListColumnData(BlockManager &block_manager, DataTableInfo &info, idx_t column_index, idx_t start_row,
-                               LogicalType type_p, optional_ptr<ColumnData> parent)
-    : ColumnData(block_manager, info, column_index, start_row, std::move(type_p), parent),
-      validity(block_manager, info, 0, start_row, *this) {
+ListColumnData::ListColumnData(BlockManager &block_manager, DataTableInfo &info, idx_t column_index, LogicalType type_p,
+                               ColumnDataType data_type, optional_ptr<ColumnData> parent)
+    : ColumnData(block_manager, info, column_index, std::move(type_p), data_type, parent),
+      validity(block_manager, info, 0, *this) {
 	D_ASSERT(type.InternalType() == PhysicalType::LIST);
 	auto &child_type = ListType::GetChildType(type);
 	// the child column, with column index 1 (0 is the validity mask)
-	child_column = ColumnData::CreateColumnUnique(block_manager, info, 1, start_row, child_type, this);
+	child_column = ColumnData::CreateColumnUnique(block_manager, info, 1, child_type, data_type, this);
 }
 
-void ListColumnData::SetStart(idx_t new_start) {
-	ColumnData::SetStart(new_start);
-	child_column->SetStart(new_start);
-	validity.SetStart(new_start);
+void ListColumnData::SetDataType(ColumnDataType data_type) {
+	ColumnData::SetDataType(data_type);
+	child_column->SetDataType(data_type);
+	validity.SetDataType(data_type);
 }
 
 FilterPropagateResult ListColumnData::CheckZonemap(ColumnScanState &state, TableFilter &filter) {
@@ -58,7 +58,8 @@ uint64_t ListColumnData::FetchListOffset(idx_t row_idx) {
 	auto segment = data.GetSegment(row_idx);
 	ColumnFetchState fetch_state;
 	Vector result(LogicalType::UBIGINT, 1);
-	segment->node->FetchRow(fetch_state, UnsafeNumericCast<row_t>(row_idx), result, 0U);
+	auto index_in_segment = UnsafeNumericCast<row_t>(row_idx - segment->row_start);
+	segment->node->FetchRow(fetch_state, index_in_segment, result, 0U);
 
 	// initialize the child scan with the required offset
 	return FlatVector::GetData<uint64_t>(result)[0];
@@ -76,10 +77,10 @@ void ListColumnData::InitializeScanWithOffset(ColumnScanState &state, idx_t row_
 	validity.InitializeScanWithOffset(state.child_states[0], row_idx);
 
 	// we need to read the list at position row_idx to get the correct row offset of the child
-	auto child_offset = row_idx == start ? 0 : FetchListOffset(row_idx - 1);
+	auto child_offset = FetchListOffset(row_idx - 1);
 	D_ASSERT(child_offset <= child_column->GetMaxEntry());
 	if (child_offset < child_column->GetMaxEntry()) {
-		child_column->InitializeScanWithOffset(state.child_states[1], start + child_offset);
+		child_column->InitializeScanWithOffset(state.child_states[1], child_offset);
 	}
 	state.last_offset = child_offset;
 }
@@ -133,7 +134,7 @@ idx_t ListColumnData::ScanCount(ColumnScanState &state, Vector &result, idx_t co
 		auto &child_entry = ListVector::GetEntry(result);
 		if (child_entry.GetType().InternalType() != PhysicalType::STRUCT &&
 		    child_entry.GetType().InternalType() != PhysicalType::ARRAY &&
-		    state.child_states[1].row_index + child_scan_count > child_column->start + child_column->GetMaxEntry()) {
+		    state.child_states[1].offset_in_column + child_scan_count > child_column->GetMaxEntry()) {
 			throw InternalException("ListColumnData::ScanCount - internal list scan offset is out of range");
 		}
 		child_column->ScanCount(state.child_states[1], child_entry, child_scan_count);
@@ -248,11 +249,11 @@ void ListColumnData::Append(BaseStatistics &stats, ColumnAppendState &state, Vec
 	validity.AppendData(stats, state.child_appends[0], vdata, count);
 }
 
-void ListColumnData::RevertAppend(row_t start_row) {
-	ColumnData::RevertAppend(start_row);
-	validity.RevertAppend(start_row);
+void ListColumnData::RevertAppend(row_t new_count) {
+	ColumnData::RevertAppend(new_count);
+	validity.RevertAppend(new_count);
 	auto column_count = GetMaxEntry();
-	if (column_count > start) {
+	if (column_count > 0) {
 		// revert append in the child column
 		auto list_offset = FetchListOffset(column_count - 1);
 		child_column->RevertAppend(UnsafeNumericCast<row_t>(list_offset));
@@ -264,13 +265,13 @@ idx_t ListColumnData::Fetch(ColumnScanState &state, row_t row_id, Vector &result
 }
 
 void ListColumnData::Update(TransactionData transaction, DataTable &data_table, idx_t column_index,
-                            Vector &update_vector, row_t *row_ids, idx_t update_count) {
+                            Vector &update_vector, row_t *row_ids, idx_t update_count, idx_t row_group_start) {
 	throw NotImplementedException("List Update is not supported.");
 }
 
 void ListColumnData::UpdateColumn(TransactionData transaction, DataTable &data_table,
                                   const vector<column_t> &column_path, Vector &update_vector, row_t *row_ids,
-                                  idx_t update_count, idx_t depth) {
+                                  idx_t update_count, idx_t depth, idx_t row_group_start) {
 	throw NotImplementedException("List Update Column is not supported");
 }
 
@@ -290,7 +291,7 @@ void ListColumnData::FetchRow(TransactionData transaction, ColumnFetchState &sta
 	}
 
 	// now perform the fetch within the segment
-	auto start_offset = idx_t(row_id) == this->start ? 0 : FetchListOffset(UnsafeNumericCast<idx_t>(row_id - 1));
+	auto start_offset = row_id == 0 ? 0 : FetchListOffset(UnsafeNumericCast<idx_t>(row_id - 1));
 	auto end_offset = FetchListOffset(UnsafeNumericCast<idx_t>(row_id));
 	validity.FetchRow(transaction, *state.child_states[0], row_id, result, result_idx);
 
@@ -309,15 +310,15 @@ void ListColumnData::FetchRow(TransactionData transaction, ColumnFetchState &sta
 	// now we need to read from the child all the elements between [offset...length]
 	auto child_scan_count = list_entry.length;
 	if (child_scan_count > 0) {
-		auto child_state = make_uniq<ColumnScanState>();
+		ColumnScanState child_state(nullptr);
 		auto &child_type = ListType::GetChildType(result.GetType());
 		Vector child_scan(child_type, child_scan_count);
 		// seek the scan towards the specified position and read [length] entries
-		child_state->Initialize(state.context, child_type, nullptr);
-		child_column->InitializeScanWithOffset(*child_state, start + start_offset);
+		child_state.Initialize(state.context, child_type, nullptr);
+		child_column->InitializeScanWithOffset(child_state, start_offset);
 		D_ASSERT(child_type.InternalType() == PhysicalType::STRUCT ||
-		         child_state->row_index + child_scan_count - this->start <= child_column->GetMaxEntry());
-		child_column->ScanCount(*child_state, child_scan, child_scan_count);
+		         child_state.offset_in_column + child_scan_count <= child_column->GetMaxEntry());
+		child_column->ScanCount(child_state, child_scan, child_scan_count);
 
 		ListVector::Append(result, child_scan, child_scan_count);
 	}
diff --git a/src/storage/table/row_group.cpp b/src/storage/table/row_group.cpp
index 659555bedf..8f6688e9bf 100644
--- a/src/storage/table/row_group.cpp
+++ b/src/storage/table/row_group.cpp
@@ -25,14 +25,14 @@
 
 namespace duckdb {
 
-RowGroup::RowGroup(RowGroupCollection &collection_p, idx_t start, idx_t count)
-    : SegmentBase<RowGroup>(start, count), collection(collection_p), version_info(nullptr), deletes_is_loaded(false),
+RowGroup::RowGroup(RowGroupCollection &collection_p, idx_t count)
+    : SegmentBase<RowGroup>(count), collection(collection_p), version_info(nullptr), deletes_is_loaded(false),
       allocation_size(0), row_id_is_loaded(false), has_changes(false) {
 	Verify();
 }
 
 RowGroup::RowGroup(RowGroupCollection &collection_p, RowGroupPointer pointer)
-    : SegmentBase<RowGroup>(pointer.row_start, pointer.tuple_count), collection(collection_p), version_info(nullptr),
+    : SegmentBase<RowGroup>(pointer.tuple_count), collection(collection_p), version_info(nullptr),
       deletes_is_loaded(false), allocation_size(0), row_id_is_loaded(false), has_changes(false) {
 	// deserialize the columns
 	if (pointer.data_pointers.size() != collection_p.GetTypes().size()) {
@@ -52,14 +52,14 @@ RowGroup::RowGroup(RowGroupCollection &collection_p, RowGroupPointer pointer)
 }
 
 RowGroup::RowGroup(RowGroupCollection &collection_p, PersistentRowGroupData &data)
-    : SegmentBase<RowGroup>(data.start, data.count), collection(collection_p), version_info(nullptr),
-      deletes_is_loaded(false), allocation_size(0), row_id_is_loaded(false), has_changes(false) {
+    : SegmentBase<RowGroup>(data.count), collection(collection_p), version_info(nullptr), deletes_is_loaded(false),
+      allocation_size(0), row_id_is_loaded(false), has_changes(false) {
 	auto &block_manager = GetBlockManager();
 	auto &info = GetTableInfo();
 	auto &types = collection.get().GetTypes();
 	columns.reserve(types.size());
 	for (idx_t c = 0; c < types.size(); c++) {
-		auto entry = ColumnData::CreateColumn(block_manager, info, c, data.start, types[c], nullptr);
+		auto entry = ColumnData::CreateColumn(block_manager, info, c, types[c]);
 		entry->InitializeColumn(data.column_data[c]);
 		columns.push_back(std::move(entry));
 	}
@@ -67,23 +67,23 @@ RowGroup::RowGroup(RowGroupCollection &collection_p, PersistentRowGroupData &dat
 	Verify();
 }
 
-void RowGroup::MoveToCollection(RowGroupCollection &collection_p, idx_t new_start) {
+void RowGroup::MoveToCollection(RowGroupCollection &collection_p) {
 	lock_guard<mutex> l(row_group_lock);
-	if (start != new_start) {
-		has_changes = true;
-	}
+	// FIXME
+	// MoveToCollection causes any_changes to be set to true because we are changing the start position of the row group
+	// the start position is ONLY written when targeting old serialization versions - as such, we don't actually
+	// need to do this when targeting newer serialization versions
+	// not doing this could allow metadata reuse in these situations, which would improve vacuuming performance
+	// especially when vacuuming from the beginning of large tables
+	has_changes = true;
 	this->collection = collection_p;
-	this->start = new_start;
 	for (idx_t c = 0; c < columns.size(); c++) {
 		if (is_loaded && !is_loaded[c]) {
 			// we only need to set the column start position if it is already loaded
 			// if it is not loaded - we will set the correct start position upon loading
 			continue;
 		}
-		columns[c]->SetStart(new_start);
-	}
-	if (row_id_is_loaded) {
-		row_id_column_data->SetStart(new_start);
+		columns[c]->SetDataType(ColumnDataType::MAIN_TABLE);
 	}
 }
 
@@ -112,7 +112,7 @@ ColumnData &RowGroup::GetRowIdColumnData() {
 	}
 	lock_guard<mutex> l(row_group_lock);
 	if (!row_id_column_data) {
-		row_id_column_data = make_uniq<RowIdColumnData>(GetBlockManager(), GetTableInfo(), start);
+		row_id_column_data = make_uniq<RowIdColumnData>(GetBlockManager(), GetTableInfo());
 		row_id_column_data->count = count.load();
 		row_id_is_loaded = true;
 	}
@@ -149,13 +149,12 @@ ColumnData &RowGroup::GetColumn(storage_t c) {
 	auto &types = GetCollection().GetTypes();
 	auto &block_pointer = column_pointers[c];
 	MetadataReader column_data_reader(metadata_manager, block_pointer);
-	this->columns[c] =
-	    ColumnData::Deserialize(GetBlockManager(), GetTableInfo(), c, start, column_data_reader, types[c]);
+	this->columns[c] = ColumnData::Deserialize(GetBlockManager(), GetTableInfo(), c, column_data_reader, types[c]);
 	is_loaded[c] = true;
 	if (this->columns[c]->count != this->count) {
-		throw InternalException("Corrupted database - loaded column with index %llu at row start %llu, count %llu did "
+		throw InternalException("Corrupted database - loaded column with index %llu, count %llu did "
 		                        "not match count of row group %llu",
-		                        c, start, this->columns[c]->count.load(), this->count.load());
+		                        c, this->columns[c]->count.load(), this->count.load());
 	}
 	return *columns[c];
 }
@@ -167,11 +166,11 @@ DataTableInfo &RowGroup::GetTableInfo() {
 	return GetCollection().GetTableInfo();
 }
 
-void RowGroup::InitializeEmpty(const vector<LogicalType> &types) {
+void RowGroup::InitializeEmpty(const vector<LogicalType> &types, ColumnDataType data_type) {
 	// set up the segment trees for the column segments
 	D_ASSERT(columns.empty());
 	for (idx_t i = 0; i < types.size(); i++) {
-		auto column_data = ColumnData::CreateColumn(GetBlockManager(), GetTableInfo(), i, start, types[i]);
+		auto column_data = ColumnData::CreateColumn(GetBlockManager(), GetTableInfo(), i, types[i], data_type);
 		columns.push_back(std::move(column_data));
 	}
 }
@@ -186,10 +185,14 @@ void ColumnScanState::Initialize(const QueryContext &context_p, const LogicalTyp
 		// validity - nothing to initialize
 		return;
 	}
+	D_ASSERT(child_states.empty());
 	if (type.InternalType() == PhysicalType::STRUCT) {
 		// validity + struct children
 		auto &struct_children = StructType::GetChildTypes(type);
-		child_states.resize(struct_children.size() + 1);
+		child_states.reserve(struct_children.size() + 1);
+		for (idx_t i = 0; i <= struct_children.size(); i++) {
+			child_states.emplace_back(parent);
+		}
 
 		if (children.empty()) {
 			// scan all struct children
@@ -211,17 +214,21 @@ void ColumnScanState::Initialize(const QueryContext &context_p, const LogicalTyp
 		child_states[0].scan_options = options;
 	} else if (type.InternalType() == PhysicalType::LIST) {
 		// validity + list child
-		child_states.resize(2);
+		for (idx_t i = 0; i < 2; i++) {
+			child_states.emplace_back(parent);
+		}
 		child_states[1].Initialize(context, ListType::GetChildType(type), options);
 		child_states[0].scan_options = options;
 	} else if (type.InternalType() == PhysicalType::ARRAY) {
 		// validity + array child
-		child_states.resize(2);
+		for (idx_t i = 0; i < 2; i++) {
+			child_states.emplace_back(parent);
+		}
 		child_states[0].scan_options = options;
 		child_states[1].Initialize(context, ArrayType::GetChildType(type), options);
 	} else {
 		// validity
-		child_states.resize(1);
+		child_states.emplace_back(parent);
 		child_states[0].scan_options = options;
 	}
 }
@@ -234,7 +241,10 @@ void ColumnScanState::Initialize(const QueryContext &context_p, const LogicalTyp
 
 void CollectionScanState::Initialize(const QueryContext &context, const vector<LogicalType> &types) {
 	auto &column_ids = GetColumnIds();
-	column_scans = make_unsafe_uniq_array<ColumnScanState>(column_ids.size());
+	column_scans.reserve(column_scans.size());
+	for (idx_t i = 0; i < column_ids.size(); i++) {
+		column_scans.emplace_back(*this);
+	}
 	for (idx_t i = 0; i < column_ids.size(); i++) {
 		if (column_ids[i].IsRowIdColumn()) {
 			continue;
@@ -256,14 +266,14 @@ bool RowGroup::InitializeScanWithOffset(CollectionScanState &state, SegmentNode<
 
 	state.row_group = node;
 	state.vector_index = vector_offset;
-	state.max_row_group_row =
-	    this->start > state.max_row ? 0 : MinValue<idx_t>(this->count, state.max_row - this->start);
-	auto row_number = start + vector_offset * STANDARD_VECTOR_SIZE;
+	auto row_start = node.row_start;
+	state.max_row_group_row = row_start > state.max_row ? 0 : MinValue<idx_t>(this->count, state.max_row - row_start);
+	auto row_number = vector_offset * STANDARD_VECTOR_SIZE;
 	if (state.max_row_group_row == 0) {
 		// exceeded row groups to scan
 		return false;
 	}
-	D_ASSERT(state.column_scans);
+	D_ASSERT(!state.column_scans.empty());
 	for (idx_t i = 0; i < column_ids.size(); i++) {
 		const auto &column = column_ids[i];
 		auto &column_data = GetColumn(column);
@@ -282,14 +292,14 @@ bool RowGroup::InitializeScan(CollectionScanState &state, SegmentNode<RowGroup>
 	if (!RefersToSameObject(*node.node, *this)) {
 		throw InternalException("RowGroup::InitializeScan segment node mismatch");
 	}
+	auto row_start = node.row_start;
 	state.row_group = node;
 	state.vector_index = 0;
-	state.max_row_group_row =
-	    this->start > state.max_row ? 0 : MinValue<idx_t>(this->count, state.max_row - this->start);
+	state.max_row_group_row = row_start > state.max_row ? 0 : MinValue<idx_t>(this->count, state.max_row - row_start);
 	if (state.max_row_group_row == 0) {
 		return false;
 	}
-	D_ASSERT(state.column_scans);
+	D_ASSERT(!state.column_scans.empty());
 	for (idx_t i = 0; i < column_ids.size(); i++) {
 		auto column = column_ids[i];
 		auto &column_data = GetColumn(column);
@@ -306,7 +316,7 @@ unique_ptr<RowGroup> RowGroup::AlterType(RowGroupCollection &new_collection, con
 	Verify();
 
 	// construct a new column data for this type
-	auto column_data = ColumnData::CreateColumn(GetBlockManager(), GetTableInfo(), changed_idx, start, target_type);
+	auto column_data = ColumnData::CreateColumn(GetBlockManager(), GetTableInfo(), changed_idx, target_type);
 
 	ColumnAppendState append_state;
 	column_data->InitializeAppend(append_state);
@@ -334,7 +344,7 @@ unique_ptr<RowGroup> RowGroup::AlterType(RowGroupCollection &new_collection, con
 	}
 
 	// set up the row_group based on this row_group
-	auto row_group = make_uniq<RowGroup>(new_collection, this->start, this->count);
+	auto row_group = make_uniq<RowGroup>(new_collection, this->count);
 	row_group->SetVersionInfo(GetOrCreateVersionInfoPtr());
 	auto &cols = GetColumns();
 	for (idx_t i = 0; i < cols.size(); i++) {
@@ -357,7 +367,7 @@ unique_ptr<RowGroup> RowGroup::AddColumn(RowGroupCollection &new_collection, Col
 
 	// construct a new column data for the new column
 	auto added_column =
-	    ColumnData::CreateColumn(GetBlockManager(), GetTableInfo(), GetColumnCount(), start, new_column.Type());
+	    ColumnData::CreateColumn(GetBlockManager(), GetTableInfo(), GetColumnCount(), new_column.Type());
 
 	idx_t rows_to_write = this->count;
 	if (rows_to_write > 0) {
@@ -374,7 +384,7 @@ unique_ptr<RowGroup> RowGroup::AddColumn(RowGroupCollection &new_collection, Col
 	}
 
 	// set up the row_group based on this row_group
-	auto row_group = make_uniq<RowGroup>(new_collection, this->start, this->count);
+	auto row_group = make_uniq<RowGroup>(new_collection, this->count);
 	row_group->SetVersionInfo(GetOrCreateVersionInfoPtr());
 	row_group->columns = GetColumns();
 	// now add the new column
@@ -389,7 +399,7 @@ unique_ptr<RowGroup> RowGroup::RemoveColumn(RowGroupCollection &new_collection,
 
 	D_ASSERT(removed_column < columns.size());
 
-	auto row_group = make_uniq<RowGroup>(new_collection, this->start, this->count);
+	auto row_group = make_uniq<RowGroup>(new_collection, this->count);
 	row_group->SetVersionInfo(GetOrCreateVersionInfoPtr());
 	// copy over all columns except for the removed one
 	auto &cols = GetColumns();
@@ -480,14 +490,14 @@ bool RowGroup::CheckZonemapSegments(CollectionScanState &state) {
 			// no segment to skip
 			continue;
 		}
-		idx_t target_row = current_segment->node->start + current_segment->node->count;
+		auto row_start = current_segment->row_start;
+		idx_t target_row = row_start + current_segment->node->count;
 		if (target_row >= state.max_row) {
 			target_row = state.max_row;
 		}
-
-		D_ASSERT(target_row >= this->start);
-		D_ASSERT(target_row <= this->start + this->count);
-		idx_t target_vector_index = (target_row - this->start) / STANDARD_VECTOR_SIZE;
+		D_ASSERT(target_row >= row_start);
+		D_ASSERT(target_row <= row_start + this->count);
+		idx_t target_vector_index = (target_row - row_start) / STANDARD_VECTOR_SIZE;
 		if (state.vector_index == target_vector_index) {
 			// we can't skip any full vectors because this segment contains less than a full vector
 			// for now we just bail-out
@@ -768,7 +778,9 @@ idx_t RowGroup::GetCommittedSelVector(transaction_t start_time, transaction_t tr
 }
 
 bool RowGroup::Fetch(TransactionData transaction, idx_t row) {
-	D_ASSERT(row < this->count);
+	if (UnsafeNumericCast<idx_t>(row) > count) {
+		throw InternalException("RowGroup::Fetch - row_id out of range for row group");
+	}
 	auto vinfo = GetVersionInfo();
 	if (!vinfo) {
 		return true;
@@ -778,6 +790,9 @@ bool RowGroup::Fetch(TransactionData transaction, idx_t row) {
 
 void RowGroup::FetchRow(TransactionData transaction, ColumnFetchState &state, const vector<StorageIndex> &column_ids,
                         row_t row_id, DataChunk &result, idx_t result_idx) {
+	if (UnsafeNumericCast<idx_t>(row_id) > count) {
+		throw InternalException("RowGroup::FetchRow - row_id out of range for row group");
+	}
 	for (idx_t col_idx = 0; col_idx < column_ids.size(); col_idx++) {
 		auto &column = column_ids[col_idx];
 		auto &result_vector = result.data[col_idx];
@@ -818,13 +833,16 @@ void RowGroup::CommitAppend(transaction_t commit_id, idx_t row_group_start, idx_
 	vinfo.CommitAppend(commit_id, row_group_start, count);
 }
 
-void RowGroup::RevertAppend(idx_t row_group_start) {
+void RowGroup::RevertAppend(idx_t new_count) {
+	if (new_count > this->count) {
+		throw InternalException("RowGroup::RevertAppend new_count out of range");
+	}
 	auto &vinfo = GetOrCreateVersionInfo();
-	vinfo.RevertAppend(row_group_start - this->start);
+	vinfo.RevertAppend(new_count);
 	for (auto &column : GetColumns()) {
-		column->RevertAppend(UnsafeNumericCast<row_t>(row_group_start));
+		column->RevertAppend(UnsafeNumericCast<row_t>(new_count));
 	}
-	SetCount(MinValue<idx_t>(row_group_start - this->start, this->count));
+	SetCount(new_count);
 	Verify();
 }
 
@@ -857,10 +875,10 @@ void RowGroup::CleanupAppend(transaction_t lowest_transaction, idx_t start, idx_
 }
 
 void RowGroup::Update(TransactionData transaction, DataTable &data_table, DataChunk &update_chunk, row_t *ids,
-                      idx_t offset, idx_t count, const vector<PhysicalIndex> &column_ids) {
+                      idx_t offset, idx_t count, const vector<PhysicalIndex> &column_ids, idx_t row_group_start) {
 #ifdef DEBUG
 	for (size_t i = offset; i < offset + count; i++) {
-		D_ASSERT(ids[i] >= row_t(this->start) && ids[i] < row_t(this->start + this->count));
+		D_ASSERT(ids[i] >= row_t(row_group_start) && ids[i] < row_t(row_group_start + this->count));
 	}
 #endif
 	for (idx_t i = 0; i < column_ids.size(); i++) {
@@ -870,28 +888,31 @@ void RowGroup::Update(TransactionData transaction, DataTable &data_table, DataCh
 		if (offset > 0) {
 			Vector sliced_vector(update_chunk.data[i], offset, offset + count);
 			sliced_vector.Flatten(count);
-			col_data.Update(transaction, data_table, column.index, sliced_vector, ids + offset, count);
+			col_data.Update(transaction, data_table, column.index, sliced_vector, ids + offset, count, row_group_start);
 		} else {
-			col_data.Update(transaction, data_table, column.index, update_chunk.data[i], ids, count);
+			col_data.Update(transaction, data_table, column.index, update_chunk.data[i], ids, count, row_group_start);
 		}
 		MergeStatistics(column.index, *col_data.GetUpdateStatistics());
 	}
 }
 
 void RowGroup::UpdateColumn(TransactionData transaction, DataTable &data_table, DataChunk &updates, Vector &row_ids,
-                            idx_t offset, idx_t count, const vector<column_t> &column_path) {
+                            idx_t offset, idx_t count, const vector<column_t> &column_path, idx_t row_group_start) {
 	D_ASSERT(updates.ColumnCount() == 1);
 	auto ids = FlatVector::GetData<row_t>(row_ids);
 
 	auto primary_column_idx = column_path[0];
 	D_ASSERT(primary_column_idx < columns.size());
 	auto &col_data = GetColumn(primary_column_idx);
+	idx_t depth = 1;
 	if (offset > 0) {
 		Vector sliced_vector(updates.data[0], offset, offset + count);
 		sliced_vector.Flatten(count);
-		col_data.UpdateColumn(transaction, data_table, column_path, sliced_vector, ids + offset, count, 1);
+		col_data.UpdateColumn(transaction, data_table, column_path, sliced_vector, ids + offset, count, depth,
+		                      row_group_start);
 	} else {
-		col_data.UpdateColumn(transaction, data_table, column_path, updates.data[0], ids, count, 1);
+		col_data.UpdateColumn(transaction, data_table, column_path, updates.data[0], ids, count, depth,
+		                      row_group_start);
 	}
 	MergeStatistics(primary_column_idx, *col_data.GetUpdateStatistics());
 }
@@ -980,9 +1001,6 @@ vector<RowGroupWriteData> RowGroup::WriteToDisk(RowGroupWriteInfo &info,
 			auto &row_group = row_groups[row_group_idx].get();
 			auto &row_group_write_data = result[row_group_idx];
 			auto &column = row_group.GetColumn(column_idx);
-			if (column.start != row_group.start) {
-				throw InternalException("RowGroup::WriteToDisk - child-column is unaligned with row group");
-			}
 			ColumnCheckpointInfo checkpoint_info(info, column_idx);
 			auto checkpoint_state = column.Checkpoint(row_group, checkpoint_info);
 			D_ASSERT(checkpoint_state);
@@ -1045,7 +1063,7 @@ vector<idx_t> RowGroup::GetOrComputeExtraMetadataBlocks(bool force_compute) {
 	// for the last column we need to deserialize the column - because we don't know where it stops
 	auto &types = GetCollection().GetTypes();
 	MetadataReader reader(metadata_manager, column_pointers[last_idx], &read_pointers);
-	ColumnData::Deserialize(GetBlockManager(), GetTableInfo(), last_idx, start, reader, types[last_idx]);
+	ColumnData::Deserialize(GetBlockManager(), GetTableInfo(), last_idx, reader, types[last_idx]);
 
 	unordered_set<idx_t> result_as_set;
 	for (auto &ptr : read_pointers) {
@@ -1088,13 +1106,22 @@ RowGroupWriteData RowGroup::WriteToDisk(RowGroupWriter &writer) {
 	return WriteToDisk(info);
 }
 
+void IncrementSegmentStart(PersistentColumnData &data, idx_t start_increment) {
+	for (auto &pointer : data.pointers) {
+		pointer.row_start += start_increment;
+	}
+	for (auto &child_column : data.child_columns) {
+		IncrementSegmentStart(child_column, start_increment);
+	}
+}
+
 RowGroupPointer RowGroup::Checkpoint(RowGroupWriteData write_data, RowGroupWriter &writer,
-                                     TableStatistics &global_stats) {
+                                     TableStatistics &global_stats, idx_t row_group_start) {
 	RowGroupPointer row_group_pointer;
 
 	auto metadata_manager = writer.GetMetadataManager();
 	// construct the row group pointer and write the column meta data to disk
-	row_group_pointer.row_start = start;
+	row_group_pointer.row_start = row_group_start;
 	row_group_pointer.tuple_count = count;
 	if (write_data.reuse_existing_metadata_blocks) {
 		// we are re-using the previous metadata
@@ -1142,6 +1169,9 @@ RowGroupPointer RowGroup::Checkpoint(RowGroupWriteData write_data, RowGroupWrite
 		// Just as above, the state can refer to many other states, so this
 		// can cascade recursively into more pointer writes.
 		auto persistent_data = state->ToPersistentData();
+		// increment the "start" in all data pointers by the row group start
+		// FIXME: this is only necessary when targeting old serialization
+		IncrementSegmentStart(persistent_data, row_group_start);
 		BinarySerializer serializer(data_writer);
 		serializer.Begin();
 		persistent_data.Serialize(serializer);
@@ -1204,13 +1234,13 @@ bool RowGroup::IsPersistent() const {
 	return true;
 }
 
-PersistentRowGroupData RowGroup::SerializeRowGroupInfo() const {
+PersistentRowGroupData RowGroup::SerializeRowGroupInfo(idx_t row_group_start) const {
 	// all columns are persistent - serialize
 	PersistentRowGroupData result;
 	for (auto &col : columns) {
 		result.column_data.push_back(col->Serialize());
 	}
-	result.start = start;
+	result.start = row_group_start;
 	result.count = count;
 	return result;
 }
@@ -1255,9 +1285,9 @@ RowGroupPointer RowGroup::Deserialize(Deserializer &deserializer) {
 //===--------------------------------------------------------------------===//
 // GetPartitionStats
 //===--------------------------------------------------------------------===//
-PartitionStatistics RowGroup::GetPartitionStats() const {
+PartitionStatistics RowGroup::GetPartitionStats(idx_t row_group_start) const {
 	PartitionStatistics result;
-	result.row_start = start;
+	result.row_start = row_group_start;
 	result.count = count;
 	if (HasUnloadedDeletes() || version_info.load().get()) {
 		// we have version info - approx count
@@ -1304,14 +1334,14 @@ public:
 	void Flush();
 };
 
-idx_t RowGroup::Delete(TransactionData transaction, DataTable &table, row_t *ids, idx_t count) {
-	VersionDeleteState del_state(*this, transaction, table, this->start);
+idx_t RowGroup::Delete(TransactionData transaction, DataTable &table, row_t *ids, idx_t count, idx_t row_group_start) {
+	VersionDeleteState del_state(*this, transaction, table, row_group_start);
 
 	// obtain a write lock
 	for (idx_t i = 0; i < count; i++) {
 		D_ASSERT(ids[i] >= 0);
-		D_ASSERT(idx_t(ids[i]) >= this->start && idx_t(ids[i]) < this->start + this->count);
-		del_state.Delete(ids[i] - UnsafeNumericCast<row_t>(this->start));
+		D_ASSERT(idx_t(ids[i]) >= row_group_start && idx_t(ids[i]) < row_group_start + this->count);
+		del_state.Delete(ids[i] - UnsafeNumericCast<row_t>(row_group_start));
 	}
 	del_state.Flush();
 	return del_state.delete_count;
diff --git a/src/storage/table/row_group_collection.cpp b/src/storage/table/row_group_collection.cpp
index ec9c3906a4..fe024e25af 100644
--- a/src/storage/table/row_group_collection.cpp
+++ b/src/storage/table/row_group_collection.cpp
@@ -24,8 +24,8 @@ namespace duckdb {
 //===--------------------------------------------------------------------===//
 // Row Group Segment Tree
 //===--------------------------------------------------------------------===//
-RowGroupSegmentTree::RowGroupSegmentTree(RowGroupCollection &collection)
-    : SegmentTree<RowGroup, true>(), collection(collection), current_row_group(0), max_row_group(0) {
+RowGroupSegmentTree::RowGroupSegmentTree(RowGroupCollection &collection, idx_t base_row_id)
+    : SegmentTree<RowGroup, true>(base_row_id), collection(collection), current_row_group(0), max_row_group(0) {
 }
 RowGroupSegmentTree::~RowGroupSegmentTree() {
 }
@@ -63,11 +63,11 @@ RowGroupCollection::RowGroupCollection(shared_ptr<DataTableInfo> info_p, TableIO
 }
 
 RowGroupCollection::RowGroupCollection(shared_ptr<DataTableInfo> info_p, BlockManager &block_manager,
-                                       vector<LogicalType> types_p, idx_t row_start_p, idx_t total_rows_p,
+                                       vector<LogicalType> types_p, idx_t row_start, idx_t total_rows_p,
                                        idx_t row_group_size_p)
     : block_manager(block_manager), row_group_size(row_group_size_p), total_rows(total_rows_p), info(std::move(info_p)),
-      types(std::move(types_p)), row_start(row_start_p), allocation_size(0), requires_new_row_group(false) {
-	row_groups = make_shared_ptr<RowGroupSegmentTree>(*this);
+      types(std::move(types_p)), allocation_size(0), requires_new_row_group(false) {
+	row_groups = make_shared_ptr<RowGroupSegmentTree>(*this, row_start);
 }
 
 idx_t RowGroupCollection::GetTotalRows() const {
@@ -90,11 +90,14 @@ MetadataManager &RowGroupCollection::GetMetadataManager() {
 	return GetBlockManager().GetMetadataManager();
 }
 
+idx_t RowGroupCollection::GetBaseRowId() const {
+	return row_groups->GetBaseRowId();
+}
 //===--------------------------------------------------------------------===//
 // Initialize
 //===--------------------------------------------------------------------===//
 void RowGroupCollection::Initialize(PersistentTableData &data) {
-	D_ASSERT(this->row_start == 0);
+	D_ASSERT(this->GetBaseRowId() == 0);
 	auto l = row_groups->Lock();
 	this->total_rows = data.total_rows;
 	row_groups->Initialize(data);
@@ -113,7 +116,7 @@ void RowGroupCollection::Initialize(PersistentCollectionData &data) {
 		auto row_group = make_uniq<RowGroup>(*this, row_group_data);
 		row_group->MergeIntoStatistics(stats);
 		total_rows += row_group->count;
-		row_groups->AppendSegment(l, std::move(row_group));
+		row_groups->AppendSegment(l, std::move(row_group), row_group_data.start);
 	}
 }
 
@@ -125,11 +128,21 @@ void RowGroupCollection::InitializeEmpty() {
 	stats.InitializeEmpty(types);
 }
 
+ColumnDataType GetColumnDataType(idx_t row_start) {
+	if (row_start == UnsafeNumericCast<idx_t>(MAX_ROW_ID)) {
+		return ColumnDataType::INITIAL_TRANSACTION_LOCAL;
+	}
+	if (row_start > UnsafeNumericCast<idx_t>(MAX_ROW_ID)) {
+		return ColumnDataType::TRANSACTION_LOCAL;
+	}
+	return ColumnDataType::MAIN_TABLE;
+}
+
 void RowGroupCollection::AppendRowGroup(SegmentLock &l, idx_t start_row) {
-	D_ASSERT(start_row >= row_start);
-	auto new_row_group = make_uniq<RowGroup>(*this, start_row, 0U);
-	new_row_group->InitializeEmpty(types);
-	row_groups->AppendSegment(l, std::move(new_row_group));
+	D_ASSERT(start_row >= GetBaseRowId());
+	auto new_row_group = make_uniq<RowGroup>(*this, 0U);
+	new_row_group->InitializeEmpty(types, GetColumnDataType(start_row));
+	row_groups->AppendSegment(l, std::move(new_row_group), start_row);
 	requires_new_row_group = false;
 }
 
@@ -145,10 +158,11 @@ void RowGroupCollection::Verify() {
 #ifdef DEBUG
 	idx_t current_total_rows = 0;
 	row_groups->Verify();
-	for (auto &row_group : row_groups->Segments()) {
+	for (auto &entry : row_groups->SegmentNodes()) {
+		auto &row_group = *entry.node;
 		row_group.Verify();
 		D_ASSERT(&row_group.GetCollection() == this);
-		D_ASSERT(row_group.start == this->row_start + current_total_rows);
+		D_ASSERT(entry.row_start == this->GetBaseRowId() + current_total_rows);
 		current_total_rows += row_group.count;
 	}
 	D_ASSERT(current_total_rows == total_rows.load());
@@ -165,7 +179,7 @@ void RowGroupCollection::InitializeScan(const QueryContext &context, CollectionS
 	auto row_group = state.GetRootSegment();
 	D_ASSERT(row_group);
 	state.row_groups = row_groups.get();
-	state.max_row = row_start + total_rows;
+	state.max_row = GetBaseRowId() + total_rows;
 	state.Initialize(context, GetTypes());
 	while (row_group && !row_group->node->InitializeScan(state, *row_group)) {
 		row_group = state.GetNextRowGroup(*row_group);
@@ -184,7 +198,7 @@ void RowGroupCollection::InitializeScanWithOffset(const QueryContext &context, C
 	state.row_groups = row_groups.get();
 	state.max_row = end_row;
 	state.Initialize(context, GetTypes());
-	idx_t start_vector = (start_row - row_group->node->start) / STANDARD_VECTOR_SIZE;
+	idx_t start_vector = (start_row - row_group->row_start) / STANDARD_VECTOR_SIZE;
 	if (!row_group->node->InitializeScanWithOffset(state, *row_group, start_vector)) {
 		throw InternalException("Failed to initialize row group scan with offset");
 	}
@@ -195,7 +209,7 @@ bool RowGroupCollection::InitializeScanInRowGroup(const QueryContext &context, C
                                                   idx_t vector_index, idx_t max_row) {
 	state.max_row = max_row;
 	state.row_groups = collection.row_groups.get();
-	if (!state.column_scans) {
+	if (state.column_scans.empty()) {
 		// initialize the scan state
 		state.Initialize(context, collection.GetTypes());
 	}
@@ -206,7 +220,7 @@ void RowGroupCollection::InitializeParallelScan(ParallelCollectionScanState &sta
 	state.collection = this;
 	state.current_row_group = state.GetRootSegment(*row_groups);
 	state.vector_index = 0;
-	state.max_row = row_start + total_rows;
+	state.max_row = GetBaseRowId() + total_rows;
 	state.batch_index = 0;
 	state.processed_rows = 0;
 }
@@ -229,13 +243,13 @@ bool RowGroupCollection::NextParallelScan(ClientContext &context, ParallelCollec
 			if (current_row_group.count == 0) {
 				break;
 			}
+			auto row_start = state.current_row_group->row_start;
 			collection = state.collection;
 			row_group = state.current_row_group;
 			if (ClientConfig::GetConfig(context).verify_parallelism) {
 				vector_index = state.vector_index;
-				max_row = current_row_group.start +
-				          MinValue<idx_t>(current_row_group.count,
-				                          STANDARD_VECTOR_SIZE * state.vector_index + STANDARD_VECTOR_SIZE);
+				max_row = row_start + MinValue<idx_t>(current_row_group.count,
+				                                      STANDARD_VECTOR_SIZE * state.vector_index + STANDARD_VECTOR_SIZE);
 				D_ASSERT(vector_index * STANDARD_VECTOR_SIZE < current_row_group.count);
 				state.vector_index++;
 				if (state.vector_index * STANDARD_VECTOR_SIZE >= current_row_group.count) {
@@ -245,7 +259,7 @@ bool RowGroupCollection::NextParallelScan(ClientContext &context, ParallelCollec
 			} else {
 				state.processed_rows += current_row_group.count;
 				vector_index = 0;
-				max_row = current_row_group.start + current_row_group.count;
+				max_row = row_start + current_row_group.count;
 				state.current_row_group = state.GetNextRowGroup(*row_groups, *row_group).get();
 			}
 			max_row = MinValue<idx_t>(max_row, state.max_row);
@@ -324,10 +338,13 @@ void RowGroupCollection::Fetch(TransactionData transaction, DataChunk &result, c
 			row_group = row_groups->GetSegmentByIndex(l, UnsafeNumericCast<int64_t>(segment_index));
 		}
 		auto &current_row_group = *row_group->node;
-		if (!current_row_group.Fetch(transaction, UnsafeNumericCast<idx_t>(row_id) - current_row_group.start)) {
+		auto offset_in_row_group = UnsafeNumericCast<idx_t>(row_id) - row_group->row_start;
+		if (!current_row_group.Fetch(transaction, offset_in_row_group)) {
 			continue;
 		}
-		current_row_group.FetchRow(transaction, state, column_ids, row_id, result, count);
+		state.row_group = row_group;
+		current_row_group.FetchRow(transaction, state, column_ids, UnsafeNumericCast<row_t>(offset_in_row_group),
+		                           result, count);
 		count++;
 	}
 	result.SetCardinality(count);
@@ -344,7 +361,8 @@ bool RowGroupCollection::CanFetch(TransactionData transaction, const row_t row_i
 		row_group = row_groups->GetSegmentByIndex(l, UnsafeNumericCast<int64_t>(segment_index));
 	}
 	auto &current_row_group = *row_group->node;
-	return current_row_group.Fetch(transaction, UnsafeNumericCast<idx_t>(row_id) - current_row_group.start);
+	auto offset_in_row_group = UnsafeNumericCast<idx_t>(row_id) - row_group->row_start;
+	return current_row_group.Fetch(transaction, offset_in_row_group);
 }
 
 //===--------------------------------------------------------------------===//
@@ -376,12 +394,13 @@ void RowGroupCollection::InitializeAppend(TransactionData transaction, TableAppe
 	auto l = row_groups->Lock();
 	if (IsEmpty(l) || requires_new_row_group) {
 		// empty row group collection: empty first row group
-		AppendRowGroup(l, row_start + total_rows);
+		AppendRowGroup(l, GetBaseRowId() + total_rows);
 	}
 	state.start_row_group = row_groups->GetLastSegment(l);
-	D_ASSERT(this->row_start + total_rows == state.start_row_group->row_start + state.start_row_group->node->count);
+	D_ASSERT(GetBaseRowId() + total_rows == state.start_row_group->row_start + state.start_row_group->node->count);
 	state.start_row_group->node->InitializeAppend(state.row_group_append_state);
 	state.transaction = transaction;
+	state.row_group_start = state.start_row_group->row_start;
 
 	// initialize thread-local stats so we have less lock contention when updating distinct statistics
 	state.stats = TableStatistics();
@@ -415,27 +434,26 @@ bool RowGroupCollection::Append(DataChunk &chunk, TableAppendState &state) {
 			current_row_group->MergeIntoStatistics(stats);
 		}
 		remaining -= append_count;
-		if (remaining > 0) {
-			// we expect max 1 iteration of this loop (i.e. a single chunk should never overflow more than one
-			// row_group)
-			D_ASSERT(chunk.size() == remaining + append_count);
-			// slice the input chunk
-			if (remaining < chunk.size()) {
-				chunk.Slice(append_count, remaining);
-			}
-			// append a new row_group
-			new_row_group = true;
-			auto next_start = current_row_group->start + state.row_group_append_state.offset_in_row_group;
-
-			auto l = row_groups->Lock();
-			AppendRowGroup(l, next_start);
-			// set up the append state for this row_group
-			auto last_row_group = row_groups->GetLastSegment(l);
-			last_row_group->node->InitializeAppend(state.row_group_append_state);
-			continue;
-		} else {
+		if (remaining == 0) {
 			break;
 		}
+		// we expect max 1 iteration of this loop (i.e. a single chunk should never overflow more than one
+		// row_group)
+		D_ASSERT(chunk.size() == remaining + append_count);
+		// slice the input chunk
+		if (remaining < chunk.size()) {
+			chunk.Slice(append_count, remaining);
+		}
+		// append a new row_group
+		new_row_group = true;
+		auto next_start = state.row_group_start + state.row_group_append_state.offset_in_row_group;
+
+		auto l = row_groups->Lock();
+		AppendRowGroup(l, next_start);
+		// set up the append state for this row_group
+		auto last_row_group = row_groups->GetLastSegment(l);
+		last_row_group->node->InitializeAppend(state.row_group_append_state);
+		state.row_group_start = next_start;
 	}
 	state.current_row += row_t(total_append_count);
 
@@ -490,7 +508,7 @@ void RowGroupCollection::CommitAppend(transaction_t commit_id, idx_t row_start,
 	idx_t remaining = count;
 	while (true) {
 		auto &current_row_group = *row_group->node;
-		idx_t start_in_row_group = current_row - current_row_group.start;
+		idx_t start_in_row_group = current_row - row_group->row_start;
 		idx_t append_count = MinValue<idx_t>(current_row_group.count - start_in_row_group, remaining);
 
 		current_row_group.CommitAppend(commit_id, start_in_row_group, append_count);
@@ -529,7 +547,7 @@ void RowGroupCollection::RevertAppendInternal(idx_t start_row) {
 		row_groups->EraseSegments(l, segment_index + 1);
 
 		segment.next = nullptr;
-		segment.node->RevertAppend(start_row);
+		segment.node->RevertAppend(start_row - segment.row_start);
 	}
 }
 
@@ -540,7 +558,7 @@ void RowGroupCollection::CleanupAppend(transaction_t lowest_transaction, idx_t s
 	idx_t remaining = count;
 	while (true) {
 		auto &current_row_group = *row_group->node;
-		idx_t start_in_row_group = current_row - current_row_group.start;
+		idx_t start_in_row_group = current_row - row_group->row_start;
 		idx_t append_count = MinValue<idx_t>(current_row_group.count - start_in_row_group, remaining);
 
 		current_row_group.CleanupAppend(lowest_transaction, start_in_row_group, append_count);
@@ -566,7 +584,7 @@ bool RowGroupCollection::IsPersistent() const {
 void RowGroupCollection::MergeStorage(RowGroupCollection &data, optional_ptr<DataTable> table,
                                       optional_ptr<StorageCommitState> commit_state) {
 	D_ASSERT(data.types == types);
-	auto start_index = row_start + total_rows.load();
+	auto start_index = GetBaseRowId() + total_rows.load();
 	auto index = start_index;
 	auto segments = data.row_groups->MoveSegments();
 
@@ -588,11 +606,11 @@ void RowGroupCollection::MergeStorage(RowGroupCollection &data, optional_ptr<Dat
 	}
 	for (auto &entry : segments) {
 		auto &row_group = entry->node;
-		row_group->MoveToCollection(*this, index);
+		row_group->MoveToCollection(*this);
 
 		if (commit_state && (index - start_index) < optimistically_written_count) {
 			// serialize the block pointers of this row group
-			auto persistent_data = row_group->SerializeRowGroupInfo();
+			auto persistent_data = row_group->SerializeRowGroupInfo(index);
 			persistent_data.types = types;
 			row_group_data->row_group_data.push_back(std::move(persistent_data));
 		}
@@ -625,16 +643,16 @@ idx_t RowGroupCollection::Delete(TransactionData transaction, DataTable &table,
 		for (pos++; pos < count; pos++) {
 			D_ASSERT(ids[pos] >= 0);
 			// check if this id still belongs to this row group
-			if (idx_t(ids[pos]) < current_row_group.start) {
+			if (idx_t(ids[pos]) < row_group->row_start) {
 				// id is before row_group start -> it does not
 				break;
 			}
-			if (idx_t(ids[pos]) >= current_row_group.start + current_row_group.count) {
+			if (idx_t(ids[pos]) >= row_group->row_start + current_row_group.count) {
 				// id is after row group end -> it does not
 				break;
 			}
 		}
-		delete_count += current_row_group.Delete(transaction, table, ids + start, pos - start);
+		delete_count += current_row_group.Delete(transaction, table, ids + start, pos - start, row_group->row_start);
 	} while (pos < count);
 
 	return delete_count;
@@ -647,11 +665,11 @@ optional_ptr<SegmentNode<RowGroup>> RowGroupCollection::NextUpdateRowGroup(row_t
 	auto row_group = row_groups->GetSegment(UnsafeNumericCast<idx_t>(ids[pos]));
 
 	auto &current_row_group = *row_group->node;
+	auto row_start = row_group->row_start;
 	row_t base_id = UnsafeNumericCast<row_t>(
-	    current_row_group.start +
-	    ((UnsafeNumericCast<idx_t>(ids[pos]) - current_row_group.start) / STANDARD_VECTOR_SIZE * STANDARD_VECTOR_SIZE));
-	auto max_id = MinValue<row_t>(base_id + STANDARD_VECTOR_SIZE,
-	                              UnsafeNumericCast<row_t>(current_row_group.start + current_row_group.count));
+	    row_start + ((UnsafeNumericCast<idx_t>(ids[pos]) - row_start) / STANDARD_VECTOR_SIZE * STANDARD_VECTOR_SIZE));
+	auto max_id =
+	    MinValue<row_t>(base_id + STANDARD_VECTOR_SIZE, UnsafeNumericCast<row_t>(row_start + current_row_group.count));
 	for (pos++; pos < count; pos++) {
 		D_ASSERT(ids[pos] >= 0);
 		// check if this id still belongs to this vector in this row group
@@ -676,7 +694,8 @@ void RowGroupCollection::Update(TransactionData transaction, DataTable &data_tab
 		auto row_group = NextUpdateRowGroup(ids, pos, updates.size());
 
 		auto &current_row_group = *row_group->node;
-		current_row_group.Update(transaction, data_table, updates, ids, start, pos - start, column_ids);
+		current_row_group.Update(transaction, data_table, updates, ids, start, pos - start, column_ids,
+		                         row_group->row_start);
 
 		auto l = stats.GetLock();
 		for (idx_t i = 0; i < column_ids.size(); i++) {
@@ -715,7 +734,7 @@ void RowGroupCollection::RemoveFromIndexes(const QueryContext &context, TableInd
 	TableScanState state;
 	auto column_ids_copy = column_ids;
 	state.Initialize(std::move(column_ids_copy));
-	state.table_state.max_row = row_start + total_rows;
+	state.table_state.max_row = GetBaseRowId() + total_rows;
 
 	DataChunk fetch_chunk;
 	fetch_chunk.Initialize(GetAllocator(), column_types);
@@ -742,8 +761,9 @@ void RowGroupCollection::RemoveFromIndexes(const QueryContext &context, TableInd
 		auto row_group = row_groups->GetSegment(UnsafeNumericCast<idx_t>(row_id));
 
 		auto &current_row_group = *row_group->node;
-		auto row_group_vector_idx = (UnsafeNumericCast<idx_t>(row_id) - current_row_group.start) / STANDARD_VECTOR_SIZE;
-		auto base_row_id = row_group_vector_idx * STANDARD_VECTOR_SIZE + current_row_group.start;
+		auto row_start = row_group->row_start;
+		auto row_group_vector_idx = (UnsafeNumericCast<idx_t>(row_id) - row_start) / STANDARD_VECTOR_SIZE;
+		auto base_row_id = row_group_vector_idx * STANDARD_VECTOR_SIZE + row_start;
 
 		// Fetch the current vector into fetch_chunk.
 		state.table_state.Initialize(context, GetTypes());
@@ -809,7 +829,8 @@ void RowGroupCollection::UpdateColumn(TransactionData transaction, DataTable &da
 		idx_t start = pos;
 		auto row_group = NextUpdateRowGroup(ids, pos, updates.size());
 		auto &current_row_group = *row_group->node;
-		current_row_group.UpdateColumn(transaction, data_table, updates, row_ids, start, pos - start, column_path);
+		current_row_group.UpdateColumn(transaction, data_table, updates, row_ids, start, pos - start, column_path,
+		                               row_group->row_start);
 
 		auto lock = stats.GetLock();
 		auto primary_column_idx = column_path[0];
@@ -900,8 +921,8 @@ public:
 		idx_t start = row_start;
 		for (idx_t target_idx = 0; target_idx < target_count; target_idx++) {
 			idx_t current_row_group_rows = MinValue<idx_t>(row_group_rows, row_group_size);
-			auto new_row_group = make_uniq<RowGroup>(collection, start, current_row_group_rows);
-			new_row_group->InitializeEmpty(types);
+			auto new_row_group = make_uniq<RowGroup>(collection, current_row_group_rows);
+			new_row_group->InitializeEmpty(types, ColumnDataType::MAIN_TABLE);
 			new_row_groups.push_back(std::move(new_row_group));
 			append_counts.push_back(0);
 
@@ -1146,10 +1167,14 @@ void RowGroupCollection::Checkpoint(TableDataWriter &writer, TableStatistics &gl
 			}
 			// schedule a checkpoint task for this row group
 			auto &row_group = *entry->node;
-			row_group.MoveToCollection(*this, vacuum_state.row_start);
+			if (vacuum_state.row_start != entry->row_start) {
+				row_group.MoveToCollection(*this);
+			} else if (!RefersToSameObject(row_group.GetCollection(), *this)) {
+				throw InternalException("RowGroup Vacuum - row group collection of row group changed");
+			}
 			if (writer.GetCheckpointType() != CheckpointType::VACUUM_ONLY) {
 				DUCKDB_LOG(checkpoint_state.writer.GetDatabase(), CheckpointLogType, GetAttached(), *info, segment_idx,
-				           row_group);
+				           row_group, vacuum_state.row_start);
 				auto checkpoint_task = GetCheckpointTask(checkpoint_state, segment_idx);
 				checkpoint_state.executor->ScheduleTask(std::move(checkpoint_task));
 			}
@@ -1224,9 +1249,10 @@ void RowGroupCollection::Checkpoint(TableDataWriter &writer, TableStatistics &gl
 		if (!row_group_writer) {
 			throw InternalException("Missing row group writer for index %llu", segment_idx);
 		}
+		idx_t row_start = new_total_rows;
 		bool metadata_reuse = checkpoint_state.write_data[segment_idx].reuse_existing_metadata_blocks;
-		auto pointer =
-		    row_group.Checkpoint(std::move(checkpoint_state.write_data[segment_idx]), *row_group_writer, global_stats);
+		auto pointer = row_group.Checkpoint(std::move(checkpoint_state.write_data[segment_idx]), *row_group_writer,
+		                                    global_stats, row_start);
 
 		auto debug_verify_blocks = DBConfig::GetSetting<DebugVerifyBlocksSetting>(GetAttached().GetDatabase()) &&
 		                           dynamic_cast<SingleFileTableDataWriter *>(&checkpoint_state.writer) != nullptr;
@@ -1278,7 +1304,7 @@ void RowGroupCollection::Checkpoint(TableDataWriter &writer, TableStatistics &gl
 			auto &metadata_manager = row_group.GetCollection().GetMetadataManager();
 			for (idx_t i = 0; i < column_start_pointers.size(); i++) {
 				MetadataReader reader(metadata_manager, column_start_pointers[i], &all_full_read_blocks);
-				ColumnData::Deserialize(GetBlockManager(), GetTableInfo(), i, row_group.start, reader, types[i]);
+				ColumnData::Deserialize(GetBlockManager(), GetTableInfo(), i, reader, types[i]);
 			}
 
 			// Derive sets of blocks to compare
@@ -1372,8 +1398,9 @@ void RowGroupCollection::CommitDropTable() {
 //===--------------------------------------------------------------------===//
 vector<PartitionStatistics> RowGroupCollection::GetPartitionStats() const {
 	vector<PartitionStatistics> result;
-	for (auto &row_group : row_groups->Segments()) {
-		result.push_back(row_group.GetPartitionStats());
+	for (auto &entry : row_groups->SegmentNodes()) {
+		auto &row_group = *entry.node;
+		result.push_back(row_group.GetPartitionStats(entry.row_start));
 	}
 	return result;
 }
@@ -1399,7 +1426,7 @@ shared_ptr<RowGroupCollection> RowGroupCollection::AddColumn(ClientContext &cont
 	idx_t new_column_idx = types.size();
 	auto new_types = types;
 	new_types.push_back(new_column.GetType());
-	auto result = make_shared_ptr<RowGroupCollection>(info, block_manager, std::move(new_types), row_start,
+	auto result = make_shared_ptr<RowGroupCollection>(info, block_manager, std::move(new_types), GetBaseRowId(),
 	                                                  total_rows.load(), row_group_size);
 
 	DataChunk dummy_chunk;
@@ -1427,7 +1454,7 @@ shared_ptr<RowGroupCollection> RowGroupCollection::RemoveColumn(idx_t col_idx) {
 	auto new_types = types;
 	new_types.erase_at(col_idx);
 
-	auto result = make_shared_ptr<RowGroupCollection>(info, block_manager, std::move(new_types), row_start,
+	auto result = make_shared_ptr<RowGroupCollection>(info, block_manager, std::move(new_types), GetBaseRowId(),
 	                                                  total_rows.load(), row_group_size);
 	result->stats.InitializeRemoveColumn(stats, col_idx);
 
@@ -1449,7 +1476,7 @@ shared_ptr<RowGroupCollection> RowGroupCollection::AlterType(ClientContext &cont
 	auto new_types = types;
 	new_types[changed_idx] = target_type;
 
-	auto result = make_shared_ptr<RowGroupCollection>(info, block_manager, std::move(new_types), row_start,
+	auto result = make_shared_ptr<RowGroupCollection>(info, block_manager, std::move(new_types), GetBaseRowId(),
 	                                                  total_rows.load(), row_group_size);
 	result->stats.InitializeAlterType(stats, changed_idx, target_type);
 
@@ -1469,7 +1496,7 @@ shared_ptr<RowGroupCollection> RowGroupCollection::AlterType(ClientContext &cont
 
 	TableScanState scan_state;
 	scan_state.Initialize(bound_columns);
-	scan_state.table_state.max_row = row_start + total_rows;
+	scan_state.table_state.max_row = GetBaseRowId() + total_rows;
 
 	// now alter the type of the column within all of the row_groups individually
 	auto lock = result->stats.GetLock();
diff --git a/src/storage/table/row_id_column_data.cpp b/src/storage/table/row_id_column_data.cpp
index 4bc3c4148d..a1c7d2eae6 100644
--- a/src/storage/table/row_id_column_data.cpp
+++ b/src/storage/table/row_id_column_data.cpp
@@ -4,28 +4,31 @@
 
 namespace duckdb {
 
-RowIdColumnData::RowIdColumnData(BlockManager &block_manager, DataTableInfo &info, idx_t start_row)
-    : ColumnData(block_manager, info, COLUMN_IDENTIFIER_ROW_ID, start_row, LogicalType(LogicalTypeId::BIGINT),
-                 nullptr) {
+RowIdColumnData::RowIdColumnData(BlockManager &block_manager, DataTableInfo &info)
+    : ColumnData(block_manager, info, COLUMN_IDENTIFIER_ROW_ID, LogicalType(LogicalTypeId::BIGINT),
+                 ColumnDataType::MAIN_TABLE, nullptr) {
 }
 
 FilterPropagateResult RowIdColumnData::CheckZonemap(ColumnScanState &state, TableFilter &filter) {
-	return RowGroup::CheckRowIdFilter(filter, start, start + count);
-	;
+	auto row_start = state.parent->row_group->row_start;
+	return RowGroup::CheckRowIdFilter(filter, row_start, row_start + count);
 }
 
 void RowIdColumnData::InitializePrefetch(PrefetchState &prefetch_state, ColumnScanState &scan_state, idx_t rows) {
 }
 
 void RowIdColumnData::InitializeScan(ColumnScanState &state) {
-	InitializeScanWithOffset(state, start);
+	InitializeScanWithOffset(state, 0);
 }
 
 void RowIdColumnData::InitializeScanWithOffset(ColumnScanState &state, idx_t row_idx) {
+	if (row_idx > count) {
+		throw InternalException("row_idx in InitializeScanWithOffset out of range");
+	}
 	state.current = nullptr;
 	state.segment_tree = nullptr;
-	state.row_index = row_idx;
-	state.internal_index = state.row_index;
+	state.offset_in_column = row_idx;
+	state.internal_index = state.offset_in_column;
 	state.initialized = true;
 	state.scan_state.reset();
 	state.last_offset = 0;
@@ -43,25 +46,26 @@ idx_t RowIdColumnData::ScanCommitted(idx_t vector_index, ColumnScanState &state,
 
 void RowIdColumnData::ScanCommittedRange(idx_t row_group_start, idx_t offset_in_row_group, idx_t count,
                                          Vector &result) {
-	D_ASSERT(this->start == row_group_start);
-	result.Sequence(UnsafeNumericCast<int64_t>(this->start + offset_in_row_group), 1, count);
+	result.Sequence(UnsafeNumericCast<int64_t>(row_group_start + offset_in_row_group), 1, count);
 }
 
 idx_t RowIdColumnData::ScanCount(ColumnScanState &state, Vector &result, idx_t count, idx_t result_offset) {
+	auto row_start = state.parent->row_group->row_start;
 	if (result_offset != 0) {
 		throw InternalException("RowIdColumnData result_offset must be 0");
 	}
-	ScanCommittedRange(start, state.row_index - start, count, result);
-	state.row_index += count;
+	ScanCommittedRange(row_start, state.offset_in_column, count, result);
+	state.offset_in_column += count;
 	return count;
 }
 
 void RowIdColumnData::Filter(TransactionData transaction, idx_t vector_index, ColumnScanState &state, Vector &result,
                              SelectionVector &sel, idx_t &count, const TableFilter &filter,
                              TableFilterState &filter_state) {
-	auto current_row = state.row_index;
+	auto row_start = state.parent->row_group->row_start;
+	auto current_row = row_start + state.offset_in_column;
 	auto max_count = GetVectorCount(vector_index);
-	state.row_index += max_count;
+	state.offset_in_column += max_count;
 	// We do another quick statistics scan for row ids here
 	const auto rowid_start = current_row;
 	const auto rowid_end = current_row + max_count;
@@ -100,10 +104,11 @@ void RowIdColumnData::SelectCommitted(idx_t vector_index, ColumnScanState &state
                                       idx_t count, bool allow_updates) {
 	result.SetVectorType(VectorType::FLAT_VECTOR);
 	auto result_data = FlatVector::GetData<row_t>(result);
+	auto row_start = state.parent->row_group->row_start;
 	for (size_t sel_idx = 0; sel_idx < count; sel_idx++) {
-		result_data[sel_idx] = UnsafeNumericCast<row_t>(state.row_index + sel.get_index(sel_idx));
+		result_data[sel_idx] = UnsafeNumericCast<row_t>(row_start + state.offset_in_column + sel.get_index(sel_idx));
 	}
-	state.row_index += GetVectorCount(vector_index);
+	state.offset_in_column += GetVectorCount(vector_index);
 }
 
 idx_t RowIdColumnData::Fetch(ColumnScanState &state, row_t row_id, Vector &result) {
@@ -114,11 +119,13 @@ void RowIdColumnData::FetchRow(TransactionData transaction, ColumnFetchState &st
                                idx_t result_idx) {
 	result.SetVectorType(VectorType::FLAT_VECTOR);
 	auto data = FlatVector::GetData<row_t>(result);
-	data[result_idx] = row_id;
+	auto row_start = state.row_group->row_start;
+	data[result_idx] = UnsafeNumericCast<row_t>(row_start) + row_id;
 }
+
 void RowIdColumnData::Skip(ColumnScanState &state, idx_t count) {
-	state.row_index += count;
-	state.internal_index = state.row_index;
+	state.offset_in_column += count;
+	state.internal_index = state.offset_in_column;
 }
 
 void RowIdColumnData::InitializeAppend(ColumnAppendState &state) {
@@ -134,18 +141,18 @@ void RowIdColumnData::AppendData(BaseStatistics &stats, ColumnAppendState &state
 	throw InternalException("RowIdColumnData cannot be appended to");
 }
 
-void RowIdColumnData::RevertAppend(row_t start_row) {
+void RowIdColumnData::RevertAppend(row_t new_count) {
 	throw InternalException("RowIdColumnData cannot be appended to");
 }
 
 void RowIdColumnData::Update(TransactionData transaction, DataTable &data_table, idx_t column_index,
-                             Vector &update_vector, row_t *row_ids, idx_t update_count) {
+                             Vector &update_vector, row_t *row_ids, idx_t update_count, idx_t row_group_start) {
 	throw InternalException("RowIdColumnData cannot be updated");
 }
 
 void RowIdColumnData::UpdateColumn(TransactionData transaction, DataTable &data_table,
                                    const vector<column_t> &column_path, Vector &update_vector, row_t *row_ids,
-                                   idx_t update_count, idx_t depth) {
+                                   idx_t update_count, idx_t depth, idx_t row_group_start) {
 	throw InternalException("RowIdColumnData cannot be updated");
 }
 
@@ -162,8 +169,7 @@ unique_ptr<ColumnCheckpointState> RowIdColumnData::Checkpoint(RowGroup &row_grou
 	throw InternalException("RowIdColumnData cannot be checkpointed");
 }
 
-void RowIdColumnData::CheckpointScan(ColumnSegment &segment, ColumnScanState &state, idx_t row_group_start, idx_t count,
-                                     Vector &scan_vector) {
+void RowIdColumnData::CheckpointScan(ColumnSegment &segment, ColumnScanState &state, idx_t count, Vector &scan_vector) {
 	throw InternalException("RowIdColumnData cannot be checkpointed");
 }
 
diff --git a/src/storage/table/row_version_manager.cpp b/src/storage/table/row_version_manager.cpp
index 6b5f4b9bdb..f2dfc774f7 100644
--- a/src/storage/table/row_version_manager.cpp
+++ b/src/storage/table/row_version_manager.cpp
@@ -166,9 +166,9 @@ void RowVersionManager::CleanupAppend(transaction_t lowest_active_transaction, i
 	}
 }
 
-void RowVersionManager::RevertAppend(idx_t start_row) {
+void RowVersionManager::RevertAppend(idx_t new_count) {
 	lock_guard<mutex> lock(version_lock);
-	idx_t start_vector_idx = (start_row + (STANDARD_VECTOR_SIZE - 1)) / STANDARD_VECTOR_SIZE;
+	idx_t start_vector_idx = (new_count + (STANDARD_VECTOR_SIZE - 1)) / STANDARD_VECTOR_SIZE;
 	for (idx_t vector_idx = start_vector_idx; vector_idx < vector_info.size(); vector_idx++) {
 		vector_info[vector_idx].reset();
 	}
diff --git a/src/storage/table/scan_state.cpp b/src/storage/table/scan_state.cpp
index cd6869c250..15864a6d7d 100644
--- a/src/storage/table/scan_state.cpp
+++ b/src/storage/table/scan_state.cpp
@@ -298,8 +298,8 @@ void ColumnScanState::NextInternal(idx_t count) {
 		//! There is no column segment
 		return;
 	}
-	row_index += count;
-	while (row_index >= current->node->start + current->node->count) {
+	offset_in_column += count;
+	while (offset_in_column >= current->row_start + current->node->count) {
 		current = segment_tree->GetNextSegment(*current);
 		initialized = false;
 		segment_checked = false;
@@ -308,7 +308,11 @@ void ColumnScanState::NextInternal(idx_t count) {
 		}
 	}
 	D_ASSERT(!current ||
-	         (row_index >= current->node->start && row_index < current->node->start + current->node->count));
+	         (offset_in_column >= current->row_start && offset_in_column < current->row_start + current->node->count));
+}
+
+idx_t ColumnScanState::GetPositionInSegment() const {
+	return offset_in_column - (current ? current->row_start : 0);
 }
 
 void ColumnScanState::Next(idx_t count) {
@@ -385,14 +389,14 @@ bool CollectionScanState::Scan(DuckTransaction &transaction, DataChunk &result)
 		row_group->node->Scan(transaction, *this, result);
 		if (result.size() > 0) {
 			return true;
-		} else if (max_row <= row_group->node->start + row_group->node->count) {
+		} else if (max_row <= row_group->row_start + row_group->node->count) {
 			row_group = nullptr;
 			return false;
 		} else {
 			do {
 				row_group = GetNextRowGroup(*row_group).get();
 				if (row_group) {
-					if (row_group->node->start >= max_row) {
+					if (row_group->row_start >= max_row) {
 						row_group = nullptr;
 						break;
 					}
diff --git a/src/storage/table/standard_column_data.cpp b/src/storage/table/standard_column_data.cpp
index ad8814ab47..fe4620fd99 100644
--- a/src/storage/table/standard_column_data.cpp
+++ b/src/storage/table/standard_column_data.cpp
@@ -12,14 +12,14 @@
 namespace duckdb {
 
 StandardColumnData::StandardColumnData(BlockManager &block_manager, DataTableInfo &info, idx_t column_index,
-                                       idx_t start_row, LogicalType type, optional_ptr<ColumnData> parent)
-    : ColumnData(block_manager, info, column_index, start_row, std::move(type), parent),
-      validity(block_manager, info, 0, start_row, *this) {
+                                       LogicalType type, ColumnDataType data_type, optional_ptr<ColumnData> parent)
+    : ColumnData(block_manager, info, column_index, std::move(type), data_type, parent),
+      validity(block_manager, info, 0, *this) {
 }
 
-void StandardColumnData::SetStart(idx_t new_start) {
-	ColumnData::SetStart(new_start);
-	validity.SetStart(new_start);
+void StandardColumnData::SetDataType(ColumnDataType data_type) {
+	ColumnData::SetDataType(data_type);
+	validity.SetDataType(data_type);
 }
 
 ScanVectorType StandardColumnData::GetVectorScanType(ColumnScanState &state, idx_t scan_count, Vector &result) {
@@ -57,7 +57,7 @@ void StandardColumnData::InitializeScanWithOffset(ColumnScanState &state, idx_t
 
 idx_t StandardColumnData::Scan(TransactionData transaction, idx_t vector_index, ColumnScanState &state, Vector &result,
                                idx_t target_count) {
-	D_ASSERT(state.row_index == state.child_states[0].row_index);
+	D_ASSERT(state.offset_in_column == state.child_states[0].offset_in_column);
 	auto scan_type = GetVectorScanType(state, target_count, result);
 	auto mode = ScanVectorMode::REGULAR_SCAN;
 	auto scan_count = ScanVector(transaction, vector_index, state, result, target_count, scan_type, mode);
@@ -67,7 +67,7 @@ idx_t StandardColumnData::Scan(TransactionData transaction, idx_t vector_index,
 
 idx_t StandardColumnData::ScanCommitted(idx_t vector_index, ColumnScanState &state, Vector &result, bool allow_updates,
                                         idx_t target_count) {
-	D_ASSERT(state.row_index == state.child_states[0].row_index);
+	D_ASSERT(state.offset_in_column == state.child_states[0].offset_in_column);
 	auto scan_count = ColumnData::ScanCommitted(vector_index, state, result, allow_updates, target_count);
 	validity.ScanCommitted(vector_index, state.child_states[0], result, allow_updates, target_count);
 	return scan_count;
@@ -134,16 +134,15 @@ void StandardColumnData::AppendData(BaseStatistics &stats, ColumnAppendState &st
 	validity.AppendData(stats, state.child_appends[0], vdata, count);
 }
 
-void StandardColumnData::RevertAppend(row_t start_row) {
-	ColumnData::RevertAppend(start_row);
-
-	validity.RevertAppend(start_row);
+void StandardColumnData::RevertAppend(row_t new_count) {
+	ColumnData::RevertAppend(new_count);
+	validity.RevertAppend(new_count);
 }
 
 idx_t StandardColumnData::Fetch(ColumnScanState &state, row_t row_id, Vector &result) {
 	// fetch validity mask
 	if (state.child_states.empty()) {
-		ColumnScanState child_state;
+		ColumnScanState child_state(state.parent);
 		child_state.scan_options = state.scan_options;
 		state.child_states.push_back(std::move(child_state));
 	}
@@ -153,28 +152,33 @@ idx_t StandardColumnData::Fetch(ColumnScanState &state, row_t row_id, Vector &re
 }
 
 void StandardColumnData::Update(TransactionData transaction, DataTable &data_table, idx_t column_index,
-                                Vector &update_vector, row_t *row_ids, idx_t update_count) {
-	ColumnScanState standard_state, validity_state;
+                                Vector &update_vector, row_t *row_ids, idx_t update_count, idx_t row_group_start) {
+	ColumnScanState standard_state(nullptr);
+	ColumnScanState validity_state(nullptr);
 	Vector base_vector(type);
-	auto standard_fetch = FetchUpdateData(standard_state, row_ids, base_vector);
-	auto validity_fetch = validity.FetchUpdateData(validity_state, row_ids, base_vector);
+	auto standard_fetch = FetchUpdateData(standard_state, row_ids, base_vector, row_group_start);
+	auto validity_fetch = validity.FetchUpdateData(validity_state, row_ids, base_vector, row_group_start);
 	if (standard_fetch != validity_fetch) {
 		throw InternalException("Unaligned fetch in validity and main column data for update");
 	}
 
-	UpdateInternal(transaction, data_table, column_index, update_vector, row_ids, update_count, base_vector);
-	validity.UpdateInternal(transaction, data_table, column_index, update_vector, row_ids, update_count, base_vector);
+	UpdateInternal(transaction, data_table, column_index, update_vector, row_ids, update_count, base_vector,
+	               row_group_start);
+	validity.UpdateInternal(transaction, data_table, column_index, update_vector, row_ids, update_count, base_vector,
+	                        row_group_start);
 }
 
 void StandardColumnData::UpdateColumn(TransactionData transaction, DataTable &data_table,
                                       const vector<column_t> &column_path, Vector &update_vector, row_t *row_ids,
-                                      idx_t update_count, idx_t depth) {
+                                      idx_t update_count, idx_t depth, idx_t row_group_start) {
 	if (depth >= column_path.size()) {
 		// update this column
-		ColumnData::Update(transaction, data_table, column_path[0], update_vector, row_ids, update_count);
+		ColumnData::Update(transaction, data_table, column_path[0], update_vector, row_ids, update_count,
+		                   row_group_start);
 	} else {
 		// update the child column (i.e. the validity column)
-		validity.UpdateColumn(transaction, data_table, column_path, update_vector, row_ids, update_count, depth + 1);
+		validity.UpdateColumn(transaction, data_table, column_path, update_vector, row_ids, update_count, depth + 1,
+		                      row_group_start);
 	}
 }
 
@@ -269,12 +273,12 @@ unique_ptr<ColumnCheckpointState> StandardColumnData::Checkpoint(RowGroup &row_g
 	return base_state;
 }
 
-void StandardColumnData::CheckpointScan(ColumnSegment &segment, ColumnScanState &state, idx_t row_group_start,
-                                        idx_t count, Vector &scan_vector) {
-	ColumnData::CheckpointScan(segment, state, row_group_start, count, scan_vector);
+void StandardColumnData::CheckpointScan(ColumnSegment &segment, ColumnScanState &state, idx_t count,
+                                        Vector &scan_vector) {
+	ColumnData::CheckpointScan(segment, state, count, scan_vector);
 
-	idx_t offset_in_row_group = state.row_index - row_group_start;
-	validity.ScanCommittedRange(row_group_start, offset_in_row_group, count, scan_vector);
+	idx_t offset_in_row_group = state.offset_in_column;
+	validity.ScanCommittedRange(0, offset_in_row_group, count, scan_vector);
 }
 
 bool StandardColumnData::IsPersistent() {
diff --git a/src/storage/table/struct_column_data.cpp b/src/storage/table/struct_column_data.cpp
index b1de02b2d9..27e73497a8 100644
--- a/src/storage/table/struct_column_data.cpp
+++ b/src/storage/table/struct_column_data.cpp
@@ -10,9 +10,9 @@
 namespace duckdb {
 
 StructColumnData::StructColumnData(BlockManager &block_manager, DataTableInfo &info, idx_t column_index,
-                                   idx_t start_row, LogicalType type_p, optional_ptr<ColumnData> parent)
-    : ColumnData(block_manager, info, column_index, start_row, std::move(type_p), parent),
-      validity(block_manager, info, 0, start_row, *this) {
+                                   LogicalType type_p, ColumnDataType data_type, optional_ptr<ColumnData> parent)
+    : ColumnData(block_manager, info, column_index, std::move(type_p), data_type, parent),
+      validity(block_manager, info, 0, *this) {
 	D_ASSERT(type.InternalType() == PhysicalType::STRUCT);
 	auto &child_types = StructType::GetChildTypes(type);
 	D_ASSERT(!child_types.empty());
@@ -26,17 +26,17 @@ StructColumnData::StructColumnData(BlockManager &block_manager, DataTableInfo &i
 	idx_t sub_column_index = 1;
 	for (auto &child_type : child_types) {
 		sub_columns.push_back(
-		    ColumnData::CreateColumnUnique(block_manager, info, sub_column_index, start_row, child_type.second, this));
+		    ColumnData::CreateColumnUnique(block_manager, info, sub_column_index, child_type.second, data_type, this));
 		sub_column_index++;
 	}
 }
 
-void StructColumnData::SetStart(idx_t new_start) {
-	this->start = new_start;
+void StructColumnData::SetDataType(ColumnDataType data_type) {
+	ColumnData::SetDataType(data_type);
 	for (auto &sub_column : sub_columns) {
-		sub_column->SetStart(new_start);
+		sub_column->SetDataType(data_type);
 	}
-	validity.SetStart(new_start);
+	validity.SetDataType(data_type);
 }
 
 idx_t StructColumnData::GetMaxEntry() {
@@ -55,7 +55,7 @@ void StructColumnData::InitializePrefetch(PrefetchState &prefetch_state, ColumnS
 
 void StructColumnData::InitializeScan(ColumnScanState &state) {
 	D_ASSERT(state.child_states.size() == sub_columns.size() + 1);
-	state.row_index = 0;
+	state.offset_in_column = 0;
 	state.current = nullptr;
 
 	// initialize the validity segment
@@ -72,7 +72,8 @@ void StructColumnData::InitializeScan(ColumnScanState &state) {
 
 void StructColumnData::InitializeScanWithOffset(ColumnScanState &state, idx_t row_idx) {
 	D_ASSERT(state.child_states.size() == sub_columns.size() + 1);
-	state.row_index = row_idx;
+	D_ASSERT(row_idx < count);
+	state.offset_in_column = row_idx;
 	state.current = nullptr;
 
 	// initialize the validity segment
@@ -181,12 +182,12 @@ void StructColumnData::Append(BaseStatistics &stats, ColumnAppendState &state, V
 	this->count += count;
 }
 
-void StructColumnData::RevertAppend(row_t start_row) {
-	validity.RevertAppend(start_row);
+void StructColumnData::RevertAppend(row_t new_count) {
+	validity.RevertAppend(new_count);
 	for (auto &sub_column : sub_columns) {
-		sub_column->RevertAppend(start_row);
+		sub_column->RevertAppend(new_count);
 	}
-	this->count = UnsafeNumericCast<idx_t>(start_row) - this->start;
+	this->count = UnsafeNumericCast<idx_t>(new_count);
 }
 
 idx_t StructColumnData::Fetch(ColumnScanState &state, row_t row_id, Vector &result) {
@@ -194,7 +195,7 @@ idx_t StructColumnData::Fetch(ColumnScanState &state, row_t row_id, Vector &resu
 	auto &child_entries = StructVector::GetEntries(result);
 	// insert any child states that are required
 	for (idx_t i = state.child_states.size(); i < child_entries.size() + 1; i++) {
-		ColumnScanState child_state;
+		ColumnScanState child_state(state.parent);
 		child_state.scan_options = state.scan_options;
 		state.child_states.push_back(std::move(child_state));
 	}
@@ -208,17 +209,18 @@ idx_t StructColumnData::Fetch(ColumnScanState &state, row_t row_id, Vector &resu
 }
 
 void StructColumnData::Update(TransactionData transaction, DataTable &data_table, idx_t column_index,
-                              Vector &update_vector, row_t *row_ids, idx_t update_count) {
-	validity.Update(transaction, data_table, column_index, update_vector, row_ids, update_count);
+                              Vector &update_vector, row_t *row_ids, idx_t update_count, idx_t row_group_start) {
+	validity.Update(transaction, data_table, column_index, update_vector, row_ids, update_count, row_group_start);
 	auto &child_entries = StructVector::GetEntries(update_vector);
 	for (idx_t i = 0; i < child_entries.size(); i++) {
-		sub_columns[i]->Update(transaction, data_table, column_index, *child_entries[i], row_ids, update_count);
+		sub_columns[i]->Update(transaction, data_table, column_index, *child_entries[i], row_ids, update_count,
+		                       row_group_start);
 	}
 }
 
 void StructColumnData::UpdateColumn(TransactionData transaction, DataTable &data_table,
                                     const vector<column_t> &column_path, Vector &update_vector, row_t *row_ids,
-                                    idx_t update_count, idx_t depth) {
+                                    idx_t update_count, idx_t depth, idx_t row_group_start) {
 	// we can never DIRECTLY update a struct column
 	if (depth >= column_path.size()) {
 		throw InternalException("Attempting to directly update a struct column - this should not be possible");
@@ -226,13 +228,14 @@ void StructColumnData::UpdateColumn(TransactionData transaction, DataTable &data
 	auto update_column = column_path[depth];
 	if (update_column == 0) {
 		// update the validity column
-		validity.UpdateColumn(transaction, data_table, column_path, update_vector, row_ids, update_count, depth + 1);
+		validity.UpdateColumn(transaction, data_table, column_path, update_vector, row_ids, update_count, depth + 1,
+		                      row_group_start);
 	} else {
 		if (update_column > sub_columns.size()) {
 			throw InternalException("Update column_path out of range");
 		}
 		sub_columns[update_column - 1]->UpdateColumn(transaction, data_table, column_path, update_vector, row_ids,
-		                                             update_count, depth + 1);
+		                                             update_count, depth + 1, row_group_start);
 	}
 }
 
diff --git a/src/storage/table/update_segment.cpp b/src/storage/table/update_segment.cpp
index 00a6bbbc28..e75c785c68 100644
--- a/src/storage/table/update_segment.cpp
+++ b/src/storage/table/update_segment.cpp
@@ -105,8 +105,10 @@ idx_t UpdateInfo::GetAllocSize(idx_t type_size) {
 	return AlignValue<idx_t>(sizeof(UpdateInfo) + (sizeof(sel_t) + type_size) * STANDARD_VECTOR_SIZE);
 }
 
-void UpdateInfo::Initialize(UpdateInfo &info, DataTable &data_table, transaction_t transaction_id) {
+void UpdateInfo::Initialize(UpdateInfo &info, DataTable &data_table, transaction_t transaction_id,
+                            idx_t row_group_start) {
 	info.max = STANDARD_VECTOR_SIZE;
+	info.row_group_start = row_group_start;
 	info.version_number = transaction_id;
 	info.table = &data_table;
 	info.segment = nullptr;
@@ -384,6 +386,8 @@ void UpdateSegment::FetchCommittedRange(idx_t start_row, idx_t count, Vector &re
 	if (!root) {
 		return;
 	}
+	D_ASSERT(start_row <= column_data.count);
+	D_ASSERT(start_row + count <= column_data.count);
 	D_ASSERT(result.GetVectorType() == VectorType::FLAT_VECTOR);
 
 	idx_t end_row = start_row + count;
@@ -487,13 +491,16 @@ static UpdateSegment::fetch_row_function_t GetFetchRowFunction(PhysicalType type
 }
 
 void UpdateSegment::FetchRow(TransactionData transaction, idx_t row_id, Vector &result, idx_t result_idx) {
-	idx_t vector_index = (row_id - column_data.start) / STANDARD_VECTOR_SIZE;
+	if (row_id > column_data.count) {
+		throw InternalException("UpdateSegment::FetchRow out of range");
+	}
+	idx_t vector_index = row_id / STANDARD_VECTOR_SIZE;
 	auto lock_handle = lock.GetSharedLock();
 	auto entry = GetUpdateNode(*lock_handle, vector_index);
 	if (!entry.IsSet()) {
 		return;
 	}
-	idx_t row_in_vector = (row_id - column_data.start) - vector_index * STANDARD_VECTOR_SIZE;
+	idx_t row_in_vector = row_id - vector_index * STANDARD_VECTOR_SIZE;
 	auto pin = entry.Pin();
 	fetch_row_function(transaction.start_time, transaction.transaction_id, UpdateInfo::Get(pin), row_in_vector, result,
 	                   result_idx);
@@ -816,8 +823,8 @@ struct ExtractValidityEntry {
 template <class T, class V, class OP = ExtractStandardEntry>
 static void MergeUpdateLoopInternal(UpdateInfo &base_info, V *base_table_data, UpdateInfo &update_info,
                                     const SelectionVector &update_vector_sel, const V *update_vector_data, row_t *ids,
-                                    idx_t count, const SelectionVector &sel) {
-	auto base_id = base_info.segment->column_data.start + base_info.vector_index * STANDARD_VECTOR_SIZE;
+                                    idx_t count, const SelectionVector &sel, idx_t row_group_start) {
+	auto base_id = row_group_start + base_info.vector_index * STANDARD_VECTOR_SIZE;
 #ifdef DEBUG
 	// all of these should be sorted, otherwise the below algorithm does not work
 	for (idx_t i = 1; i < count; i++) {
@@ -920,20 +927,22 @@ static void MergeUpdateLoopInternal(UpdateInfo &base_info, V *base_table_data, U
 }
 
 static void MergeValidityLoop(UpdateInfo &base_info, Vector &base_data, UpdateInfo &update_info,
-                              UnifiedVectorFormat &update, row_t *ids, idx_t count, const SelectionVector &sel) {
+                              UnifiedVectorFormat &update, row_t *ids, idx_t count, const SelectionVector &sel,
+                              idx_t row_group_start) {
 	auto &base_validity = FlatVector::Validity(base_data);
 	auto &update_validity = update.validity;
-	MergeUpdateLoopInternal<bool, ValidityMask, ExtractValidityEntry>(base_info, &base_validity, update_info,
-	                                                                  *update.sel, &update_validity, ids, count, sel);
+	MergeUpdateLoopInternal<bool, ValidityMask, ExtractValidityEntry>(
+	    base_info, &base_validity, update_info, *update.sel, &update_validity, ids, count, sel, row_group_start);
 }
 
 template <class T>
 static void MergeUpdateLoop(UpdateInfo &base_info, Vector &base_data, UpdateInfo &update_info,
-                            UnifiedVectorFormat &update, row_t *ids, idx_t count, const SelectionVector &sel) {
+                            UnifiedVectorFormat &update, row_t *ids, idx_t count, const SelectionVector &sel,
+                            idx_t row_group_start) {
 	auto base_table_data = FlatVector::GetData<T>(base_data);
 	auto update_vector_data = update.GetData<T>(update);
 	MergeUpdateLoopInternal<T, T>(base_info, base_table_data, update_info, *update.sel, update_vector_data, ids, count,
-	                              sel);
+	                              sel, row_group_start);
 }
 
 static UpdateSegment::merge_update_function_t GetMergeUpdateFunction(PhysicalType type) {
@@ -1239,10 +1248,10 @@ static idx_t SortSelectionVector(SelectionVector &sel, idx_t count, row_t *ids)
 }
 
 UpdateInfo *CreateEmptyUpdateInfo(TransactionData transaction, DataTable &data_table, idx_t type_size, idx_t count,
-                                  unsafe_unique_array<char> &data) {
+                                  unsafe_unique_array<char> &data, idx_t row_group_start) {
 	data = make_unsafe_uniq_array_uninitialized<char>(UpdateInfo::GetAllocSize(type_size));
 	auto update_info = reinterpret_cast<UpdateInfo *>(data.get());
-	UpdateInfo::Initialize(*update_info, data_table, transaction.transaction_id);
+	UpdateInfo::Initialize(*update_info, data_table, transaction.transaction_id, row_group_start);
 	return update_info;
 }
 
@@ -1261,7 +1270,7 @@ void UpdateSegment::InitializeUpdateInfo(idx_t vector_idx) {
 }
 
 void UpdateSegment::Update(TransactionData transaction, DataTable &data_table, idx_t column_index, Vector &update_p,
-                           row_t *ids, idx_t count, Vector &base_data) {
+                           row_t *ids, idx_t count, Vector &base_data, idx_t row_group_start) {
 	// obtain an exclusive lock
 	auto write_lock = lock.GetExclusiveLock();
 
@@ -1288,8 +1297,8 @@ void UpdateSegment::Update(TransactionData transaction, DataTable &data_table, i
 	// get the vector index based on the first id
 	// we assert that all updates must be part of the same vector
 	auto first_id = ids[sel.get_index(0)];
-	idx_t vector_index = (UnsafeNumericCast<idx_t>(first_id) - column_data.start) / STANDARD_VECTOR_SIZE;
-	idx_t vector_offset = column_data.start + vector_index * STANDARD_VECTOR_SIZE;
+	idx_t vector_index = (UnsafeNumericCast<idx_t>(first_id) - row_group_start) / STANDARD_VECTOR_SIZE;
+	idx_t vector_offset = row_group_start + vector_index * STANDARD_VECTOR_SIZE;
 
 	if (!root || vector_index >= root->info.size() || !root->info[vector_index].IsSet()) {
 		// get a list of effective updates - i.e. updates that actually change rows
@@ -1304,7 +1313,7 @@ void UpdateSegment::Update(TransactionData transaction, DataTable &data_table, i
 
 	InitializeUpdateInfo(vector_index);
 
-	D_ASSERT(idx_t(first_id) >= column_data.start);
+	D_ASSERT(idx_t(first_id) >= row_group_start);
 
 	if (root->info[vector_index].IsSet()) {
 		// there is already a version here, check if there are any conflicts and search for the node that belongs to
@@ -1324,10 +1333,11 @@ void UpdateSegment::Update(TransactionData transaction, DataTable &data_table, i
 			// no updates made yet by this transaction: initially the update info to empty
 			if (transaction.transaction) {
 				auto &dtransaction = transaction.transaction->Cast<DuckTransaction>();
-				node_ref = dtransaction.CreateUpdateInfo(type_size, data_table, count);
+				node_ref = dtransaction.CreateUpdateInfo(type_size, data_table, count, row_group_start);
 				node = &UpdateInfo::Get(node_ref);
 			} else {
-				node = CreateEmptyUpdateInfo(transaction, data_table, type_size, count, update_info_data);
+				node =
+				    CreateEmptyUpdateInfo(transaction, data_table, type_size, count, update_info_data, row_group_start);
 			}
 			node->segment = this;
 			node->vector_index = vector_index;
@@ -1351,7 +1361,7 @@ void UpdateSegment::Update(TransactionData transaction, DataTable &data_table, i
 		node->Verify();
 
 		// now we are going to perform the merge
-		merge_update_function(base_info, base_data, *node, update_format, ids, count, sel);
+		merge_update_function(base_info, base_data, *node, update_format, ids, count, sel, row_group_start);
 
 		base_info.Verify();
 		node->Verify();
@@ -1361,7 +1371,7 @@ void UpdateSegment::Update(TransactionData transaction, DataTable &data_table, i
 		idx_t alloc_size = UpdateInfo::GetAllocSize(type_size);
 		auto handle = root->allocator.Allocate(alloc_size);
 		auto &update_info = UpdateInfo::Get(handle);
-		UpdateInfo::Initialize(update_info, data_table, TRANSACTION_ID_START - 1);
+		UpdateInfo::Initialize(update_info, data_table, TRANSACTION_ID_START - 1, row_group_start);
 		update_info.column_index = column_index;
 
 		InitializeUpdateInfo(update_info, ids, sel, count, vector_index, vector_offset);
@@ -1371,10 +1381,11 @@ void UpdateSegment::Update(TransactionData transaction, DataTable &data_table, i
 		UndoBufferReference node_ref;
 		optional_ptr<UpdateInfo> transaction_node;
 		if (transaction.transaction) {
-			node_ref = transaction.transaction->CreateUpdateInfo(type_size, data_table, count);
+			node_ref = transaction.transaction->CreateUpdateInfo(type_size, data_table, count, row_group_start);
 			transaction_node = &UpdateInfo::Get(node_ref);
 		} else {
-			transaction_node = CreateEmptyUpdateInfo(transaction, data_table, type_size, count, update_info_data);
+			transaction_node =
+			    CreateEmptyUpdateInfo(transaction, data_table, type_size, count, update_info_data, row_group_start);
 		}
 
 		InitializeUpdateInfo(*transaction_node, ids, sel, count, vector_index, vector_offset);
diff --git a/src/storage/table/validity_column_data.cpp b/src/storage/table/validity_column_data.cpp
index fc8a9e1ea5..de839b2333 100644
--- a/src/storage/table/validity_column_data.cpp
+++ b/src/storage/table/validity_column_data.cpp
@@ -5,8 +5,9 @@
 namespace duckdb {
 
 ValidityColumnData::ValidityColumnData(BlockManager &block_manager, DataTableInfo &info, idx_t column_index,
-                                       idx_t start_row, ColumnData &parent)
-    : ColumnData(block_manager, info, column_index, start_row, LogicalType(LogicalTypeId::VALIDITY), &parent) {
+                                       ColumnData &parent)
+    : ColumnData(block_manager, info, column_index, LogicalType(LogicalTypeId::VALIDITY), parent.GetDataType(),
+                 &parent) {
 }
 
 FilterPropagateResult ValidityColumnData::CheckZonemap(ColumnScanState &state, TableFilter &filter) {
diff --git a/src/transaction/duck_transaction.cpp b/src/transaction/duck_transaction.cpp
index 3362c52d04..cff9690169 100644
--- a/src/transaction/duck_transaction.cpp
+++ b/src/transaction/duck_transaction.cpp
@@ -126,11 +126,12 @@ void DuckTransaction::PushAppend(DataTable &table, idx_t start_row, idx_t row_co
 	append_info->count = row_count;
 }
 
-UndoBufferReference DuckTransaction::CreateUpdateInfo(idx_t type_size, DataTable &data_table, idx_t entries) {
+UndoBufferReference DuckTransaction::CreateUpdateInfo(idx_t type_size, DataTable &data_table, idx_t entries,
+                                                      idx_t row_group_start) {
 	idx_t alloc_size = UpdateInfo::GetAllocSize(type_size);
 	auto undo_entry = undo_buffer.CreateEntry(UndoFlags::UPDATE_TUPLE, alloc_size);
 	auto &update_info = UpdateInfo::Get(undo_entry);
-	UpdateInfo::Initialize(update_info, data_table, transaction_id);
+	UpdateInfo::Initialize(update_info, data_table, transaction_id, row_group_start);
 	return undo_entry;
 }
 
diff --git a/src/transaction/transaction_context.cpp b/src/transaction/transaction_context.cpp
index 51a201de01..f6958b899e 100644
--- a/src/transaction/transaction_context.cpp
+++ b/src/transaction/transaction_context.cpp
@@ -17,9 +17,6 @@ TransactionContext::~TransactionContext() {
 	if (current_transaction) {
 		try {
 			Rollback(nullptr);
-		} catch (std::exception &ex) {
-			ErrorData data(ex);
-			DUCKDB_LOG_ERROR(context, "TransactionContext::~TransactionContext()\t\t" + data.Message());
 		} catch (...) { // NOLINT
 		}
 	}
diff --git a/src/transaction/wal_write_state.cpp b/src/transaction/wal_write_state.cpp
index 0036ad0c69..c0671eec06 100644
--- a/src/transaction/wal_write_state.cpp
+++ b/src/transaction/wal_write_state.cpp
@@ -217,7 +217,7 @@ void WALWriteState::WriteUpdate(UpdateInfo &info) {
 
 	// write the row ids into the chunk
 	auto row_ids = FlatVector::GetData<row_t>(update_chunk->data[1]);
-	idx_t start = column_data.start + info.vector_index * STANDARD_VECTOR_SIZE;
+	idx_t start = info.row_group_start + info.vector_index * STANDARD_VECTOR_SIZE;
 	auto tuples = info.GetTuples();
 	for (idx_t i = 0; i < info.N; i++) {
 		row_ids[tuples[i]] = UnsafeNumericCast<int64_t>(start + tuples[i]);
diff --git a/test/api/adbc/test_adbc.cpp b/test/api/adbc/test_adbc.cpp
index 41cc5de0b8..7e5a3432c1 100644
--- a/test/api/adbc/test_adbc.cpp
+++ b/test/api/adbc/test_adbc.cpp
@@ -780,6 +780,84 @@ TEST_CASE("Test ADBC Statement Bind", "[adbc]") {
 	adbc_error.release(&adbc_error);
 }
 
+TEST_CASE("Test ADBC Statement with Zero Parameters", "[adbc]") {
+	if (!duckdb_lib) {
+		return;
+	}
+
+	AdbcDatabase adbc_database;
+	AdbcConnection adbc_connection;
+	AdbcError adbc_error;
+	InitializeADBCError(&adbc_error);
+
+	// Create connection
+	REQUIRE(SUCCESS(AdbcDatabaseNew(&adbc_database, &adbc_error)));
+	REQUIRE(SUCCESS(AdbcDatabaseSetOption(&adbc_database, "driver", duckdb_lib, &adbc_error)));
+	REQUIRE(SUCCESS(AdbcDatabaseSetOption(&adbc_database, "entrypoint", "duckdb_adbc_init", &adbc_error)));
+	REQUIRE(SUCCESS(AdbcDatabaseSetOption(&adbc_database, "path", ":memory:", &adbc_error)));
+	REQUIRE(SUCCESS(AdbcDatabaseInit(&adbc_database, &adbc_error)));
+	REQUIRE(SUCCESS(AdbcConnectionNew(&adbc_connection, &adbc_error)));
+	REQUIRE(SUCCESS(AdbcConnectionInit(&adbc_connection, &adbc_database, &adbc_error)));
+
+	// Test 1: Query with no parameters (CREATE TABLE)
+	{
+		AdbcStatement adbc_statement;
+		REQUIRE(SUCCESS(AdbcStatementNew(&adbc_connection, &adbc_statement, &adbc_error)));
+		REQUIRE(SUCCESS(
+		    AdbcStatementSetSqlQuery(&adbc_statement, "CREATE TABLE test_zero_params (id INTEGER)", &adbc_error)));
+		REQUIRE(SUCCESS(AdbcStatementPrepare(&adbc_statement, &adbc_error)));
+
+		// Get parameter schema - should have 0 children
+		ArrowSchema param_schema;
+		param_schema.release = nullptr;
+		REQUIRE(SUCCESS(AdbcStatementGetParameterSchema(&adbc_statement, &param_schema, &adbc_error)));
+		REQUIRE(param_schema.n_children == 0);
+		REQUIRE(param_schema.format != nullptr);
+		REQUIRE(strcmp(param_schema.format, "+s") == 0); // Should be a struct
+		param_schema.release(&param_schema);
+
+		// Execute the statement without binding any parameters
+		ArrowArrayStream arrow_stream;
+		arrow_stream.release = nullptr;
+		REQUIRE(SUCCESS(AdbcStatementExecuteQuery(&adbc_statement, &arrow_stream, nullptr, &adbc_error)));
+		arrow_stream.release(&arrow_stream);
+
+		REQUIRE(SUCCESS(AdbcStatementRelease(&adbc_statement, &adbc_error)));
+	}
+
+	// Test 2: Query with no parameters (SELECT constant)
+	{
+		AdbcStatement adbc_statement;
+		REQUIRE(SUCCESS(AdbcStatementNew(&adbc_connection, &adbc_statement, &adbc_error)));
+		REQUIRE(SUCCESS(AdbcStatementSetSqlQuery(&adbc_statement, "SELECT 42", &adbc_error)));
+		REQUIRE(SUCCESS(AdbcStatementPrepare(&adbc_statement, &adbc_error)));
+
+		// Get parameter schema - should have 0 children
+		ArrowSchema param_schema;
+		param_schema.release = nullptr;
+		REQUIRE(SUCCESS(AdbcStatementGetParameterSchema(&adbc_statement, &param_schema, &adbc_error)));
+		REQUIRE(param_schema.n_children == 0);
+		param_schema.release(&param_schema);
+
+		// Execute and verify result
+		ArrowArrayStream arrow_stream;
+		arrow_stream.release = nullptr;
+		REQUIRE(SUCCESS(AdbcStatementExecuteQuery(&adbc_statement, &arrow_stream, nullptr, &adbc_error)));
+
+		ArrowSchema result_schema;
+		arrow_stream.get_schema(&arrow_stream, &result_schema);
+		REQUIRE(result_schema.n_children == 1); // One column in result
+		result_schema.release(&result_schema);
+
+		arrow_stream.release(&arrow_stream);
+		REQUIRE(SUCCESS(AdbcStatementRelease(&adbc_statement, &adbc_error)));
+	}
+
+	REQUIRE(SUCCESS(AdbcConnectionRelease(&adbc_connection, &adbc_error)));
+	REQUIRE(SUCCESS(AdbcDatabaseRelease(&adbc_database, &adbc_error)));
+	adbc_error.release(&adbc_error);
+}
+
 TEST_CASE("Test ADBC Transactions", "[adbc]") {
 	if (!duckdb_lib) {
 		return;
diff --git a/test/api/capi/CMakeLists.txt b/test/api/capi/CMakeLists.txt
index 809b799f4b..2e0cf180e6 100644
--- a/test/api/capi/CMakeLists.txt
+++ b/test/api/capi/CMakeLists.txt
@@ -18,7 +18,6 @@ add_library_unity(
   test_capi_data_chunk.cpp
   test_capi_extract.cpp
   test_capi_instance_cache.cpp
-  test_capi_logging.cpp
   test_capi_pending.cpp
   test_capi_prepared.cpp
   test_capi_profiling.cpp
diff --git a/test/api/capi/test_capi_appender.cpp b/test/api/capi/test_capi_appender.cpp
index 5f837110fe..edf634fc03 100644
--- a/test/api/capi/test_capi_appender.cpp
+++ b/test/api/capi/test_capi_appender.cpp
@@ -157,7 +157,7 @@ TEST_CASE("Test NULL struct Appender", "[capi]") {
 	auto second_child_vector = duckdb_struct_vector_get_child(struct_vector, 1);
 
 	// set two values
-	auto first_child_ptr = (int64_t *)duckdb_vector_get_data(first_child_vector);
+	auto first_child_ptr = static_cast<int32_t *>(duckdb_vector_get_data(first_child_vector));
 	*first_child_ptr = 42;
 	duckdb_vector_assign_string_element(second_child_vector, 0, "hello");
 
diff --git a/test/api/capi/test_capi_logging.cpp b/test/api/capi/test_capi_logging.cpp
deleted file mode 100644
index 6626ac652c..0000000000
--- a/test/api/capi/test_capi_logging.cpp
+++ /dev/null
@@ -1,60 +0,0 @@
-#include "capi_tester.hpp"
-#include "util/logging.h"
-
-using namespace duckdb;
-using namespace std;
-
-unordered_set<string> LOG_STORE;
-
-void WriteLogEntry(duckdb_timestamp timestamp, const char *level, const char *log_type, const char *log_message) {
-	LOG_STORE.insert(log_message);
-}
-
-TEST_CASE("Test pluggable log storage in CAPI", "[capi]") {
-	CAPITester tester;
-	duckdb::unique_ptr<CAPIResult> result;
-
-	LOG_STORE.clear();
-
-	REQUIRE(tester.OpenDatabase(nullptr));
-
-	auto storage = duckdb_create_log_storage();
-	duckdb_log_storage_set_write_log_entry(storage, WriteLogEntry);
-
-	duckdb_register_log_storage(tester.database, "MyCustomStorage", storage);
-
-	REQUIRE_NO_FAIL(tester.Query("set enable_logging=true;"));
-	REQUIRE_NO_FAIL(tester.Query("set logging_storage='MyCustomStorage';"));
-
-	REQUIRE_NO_FAIL(tester.Query("select write_log('HELLO, BRO');"));
-	REQUIRE(LOG_STORE.find("HELLO, BRO") != LOG_STORE.end());
-
-	duckdb_destroy_log_storage(storage);
-}
-
-// Check that Fatal Error which is otherwise swallowed, is logged
-TEST_CASE("Test logging errors using pluggable log storage in CAPI", "[capi]") {
-	CAPITester tester;
-	duckdb::unique_ptr<CAPIResult> result;
-
-	LOG_STORE.clear();
-
-	REQUIRE(tester.OpenDatabase(nullptr));
-
-	auto storage = duckdb_create_log_storage();
-	duckdb_log_storage_set_write_log_entry(storage, WriteLogEntry);
-
-	duckdb_register_log_storage(tester.database, "MyCustomStorage", storage);
-
-	REQUIRE_NO_FAIL(tester.Query("CALL enable_logging(level = 'error');"));
-	REQUIRE_NO_FAIL(tester.Query("set logging_storage='MyCustomStorage';"));
-
-	auto path = TestCreatePath("log_storage_test.db");
-	REQUIRE_NO_FAIL(tester.Query("ATTACH IF NOT EXISTS '" + path + "' (TYPE DUCKDB)"));
-	REQUIRE_NO_FAIL(tester.Query("PRAGMA wal_autocheckpoint='1TB';"));
-	REQUIRE_NO_FAIL(tester.Query("PRAGMA debug_checkpoint_abort='before_header';"));
-	REQUIRE_NO_FAIL(tester.Query("CREATE TABLE log_storage_test.integers AS SELECT * FROM range(100) tbl(i);"));
-	REQUIRE_NO_FAIL(tester.Query("DETACH log_storage_test;"));
-
-	duckdb_destroy_log_storage(storage);
-}
diff --git a/test/api/test_config.cpp b/test/api/test_config.cpp
index f894f06ac3..fe15186158 100644
--- a/test/api/test_config.cpp
+++ b/test/api/test_config.cpp
@@ -1,5 +1,6 @@
 #include "catch.hpp"
 #include "test_helpers.hpp"
+#include "duckdb/common/local_file_system.hpp"
 
 #include <set>
 #include <map>
@@ -7,6 +8,22 @@
 using namespace duckdb;
 using namespace std;
 
+class FileCleaner {
+public:
+	FileCleaner(LocalFileSystem *fs, string file) : fs(*fs), file(std::move(file)) {
+	}
+
+	~FileCleaner() {
+		if (fs.FileExists(file)) {
+			fs.RemoveFile(file);
+		}
+	}
+
+private:
+	LocalFileSystem &fs;
+	string file;
+};
+
 TEST_CASE("Test DB config configuration", "[api]") {
 	DBConfig config;
 
@@ -115,3 +132,112 @@ TEST_CASE("Test user_agent", "[api]") {
 		REQUIRE_THAT(res->GetValue(0, 0).ToString(), Catch::Matchers::Matches("duckdb/.*(.*) go"));
 	}
 }
+
+TEST_CASE("Test secret_directory configuration", "[api]") {
+	DBConfig config;
+
+	auto options = config.GetOptions();
+
+	config.SetOptionByName("secret_directory", Value("my_secret_dir"));
+	config.SetOptionByName("extension_directory", Value("my_extension_dir"));
+
+	DuckDB db(nullptr, &config);
+	Connection con(db);
+
+	// Ensure that the extension directory is set correctly (according to the inital config)
+	auto select_extension_dir = con.Query("SELECT current_setting('extension_directory') AS extdir;");
+	REQUIRE(select_extension_dir->GetValue(0, 0).ToString() == "my_extension_dir");
+
+	auto select_secret_dir = con.Query("SELECT current_setting('secret_directory') AS secretdir;");
+	REQUIRE(select_secret_dir->GetValue(0, 0).ToString() == "my_secret_dir");
+}
+
+TEST_CASE("Test secret creation with a custom secret_directory configuration", "[api]") {
+	LocalFileSystem fs;
+	string directory_path = TestDirectoryPath();
+	string my_secret_dir = fs.JoinPath(directory_path, "my_secret_dir");
+	string my_secret_file = fs.JoinPath(my_secret_dir, "my_secret.duckdb_secret");
+	FileCleaner cleaner(&fs, my_secret_file);
+
+	DBConfig config;
+
+	auto options = config.GetOptions();
+
+	config.SetOptionByName("secret_directory", Value(my_secret_dir));
+
+	DuckDB db(nullptr, &config);
+	Connection con(db);
+
+	// Ensure that the extension directory is set correctly (according to the inital config)
+	auto select_secret_dir = con.Query("SELECT current_setting('secret_directory') AS secretdir;");
+	REQUIRE(select_secret_dir->GetValue(0, 0).ToString() == my_secret_dir);
+
+	// Ensure that creating a secret works and the secret file is created in the correct directory
+	auto create_secret = con.Query("CREATE PERSISTENT SECRET my_secret (TYPE http, BEARER_TOKEN 'token')");
+	REQUIRE(create_secret->GetValue(0, 0).GetValue<bool>());
+	REQUIRE(fs.FileExists(my_secret_file));
+}
+
+TEST_CASE("Test secret creation with a custom secret_directory configuration update", "[api]") {
+	LocalFileSystem fs;
+	string directory_path = TestDirectoryPath();
+	string my_secret_dir = fs.JoinPath(directory_path, "my_secret_dir");
+	string new_secret_dir = fs.JoinPath(directory_path, "new_secret_dir");
+	string my_other_secret_file = fs.JoinPath(new_secret_dir, "my_other_secret.duckdb_secret");
+	FileCleaner cleaner(&fs, my_other_secret_file);
+
+	DBConfig config;
+
+	auto options = config.GetOptions();
+
+	config.SetOptionByName("secret_directory", Value(my_secret_dir));
+
+	DuckDB db(nullptr, &config);
+	Connection con(db);
+
+	// Ensure that the extension directory is set correctly (according to the inital config)
+	auto select_secret_dir = con.Query("SELECT current_setting('secret_directory') AS secretdir;");
+	REQUIRE(select_secret_dir->GetValue(0, 0).ToString() == my_secret_dir);
+
+	// Do not create a secret here because it will initialize the secret manager and forbid us to update the value.
+
+	// Update the secret directory and ensure that the setting is updated
+	con.Query("SET secret_directory='" + new_secret_dir + "';");
+	auto select_new_secret_dir = con.Query("SELECT current_setting('secret_directory') AS secretdir;");
+	REQUIRE(select_new_secret_dir->GetValue(0, 0).ToString() == new_secret_dir);
+
+	// Create another secret and ensure that it is created in the new directory
+	auto new_create_secret = con.Query("CREATE PERSISTENT SECRET my_other_secret (TYPE http, BEARER_TOKEN 'token')");
+	REQUIRE(new_create_secret->GetValue(0, 0).GetValue<bool>());
+	REQUIRE(fs.FileExists(my_other_secret_file));
+}
+
+TEST_CASE("Test secret_directory configuration update after secret creation", "[api]") {
+	LocalFileSystem fs;
+	string directory_path = TestDirectoryPath();
+	string my_secret_dir = fs.JoinPath(directory_path, "my_secret_dir");
+	string my_secret_file = fs.JoinPath(my_secret_dir, "my_secret.duckdb_secret");
+	FileCleaner cleaner(&fs, my_secret_file);
+
+	DBConfig config;
+
+	auto options = config.GetOptions();
+
+	config.SetOptionByName("secret_directory", Value(my_secret_dir));
+
+	DuckDB db(nullptr, &config);
+	Connection con(db);
+
+	// Ensure that the extension directory is set correctly (according to the inital config)
+	auto select_secret_dir = con.Query("SELECT current_setting('secret_directory') AS secretdir;");
+	REQUIRE(select_secret_dir->GetValue(0, 0).ToString() == my_secret_dir);
+
+	// Create a secret here to initialize the secret manager
+	auto create_secret = con.Query("CREATE PERSISTENT SECRET my_secret (TYPE http, BEARER_TOKEN 'token')");
+	REQUIRE(create_secret->GetValue(0, 0).GetValue<bool>());
+	REQUIRE(fs.FileExists(my_secret_file));
+
+	// Try to update the secret directory and expect failure as the secret manager is already initialized
+	auto update_secret_directory = con.Query("SET secret_directory='new_secret_dir';");
+	REQUIRE_FAIL(update_secret_directory);
+}
diff --git a/test/api/test_relation_api.cpp b/test/api/test_relation_api.cpp
index d04357fc40..b19f2f007e 100644
--- a/test/api/test_relation_api.cpp
+++ b/test/api/test_relation_api.cpp
@@ -506,6 +506,16 @@ TEST_CASE("Test table creations using the relation API", "[relation_api]") {
 	result = con.Query("SELECT * FROM new_values ORDER BY k");
 	REQUIRE(CHECK_COLUMN(result, 0, {4, 5}));
 	REQUIRE(CHECK_COLUMN(result, 1, {"hello", "hello"}));
+
+	// create a table in an attached db and insert values
+	auto test_dir = TestDirectoryPath();
+	string db_path = test_dir + "/my_db.db";
+	REQUIRE_NO_FAIL(con.Query("ATTACH '" + db_path + "' AS my_db;"));
+	REQUIRE_NOTHROW(values = con.Values({{1, 10}, {2, 5}, {3, 4}}, {"i", "j"}));
+	REQUIRE_NOTHROW(values->Create(std::string("my_db"), std::string(), std::string("integers")));
+	result = con.Query("SELECT * FROM my_db.integers ORDER BY i");
+	REQUIRE(CHECK_COLUMN(result, 0, {1, 2, 3}));
+	REQUIRE(CHECK_COLUMN(result, 1, {10, 5, 4}));
 }
 
 TEST_CASE("Test table creations with on_create_conflict using the relation API", "[relation_api]") {
diff --git a/test/api/test_reset.cpp b/test/api/test_reset.cpp
index d3cb90f4b0..e57f35aecf 100644
--- a/test/api/test_reset.cpp
+++ b/test/api/test_reset.cpp
@@ -185,7 +185,8 @@ bool OptionIsExcludedFromTest(const string &name) {
 	    "enable_progress_bar_print",
 	    "progress_bar_time",
 	    "index_scan_max_count",
-	    "profiling_mode"};
+	    "profiling_mode",
+	    "block_allocator_memory"}; // cant reduce
 	return excluded_options.count(name) == 1;
 }
 
diff --git a/test/appender/test_appender.cpp b/test/appender/test_appender.cpp
index fb5a2d5116..317c8405af 100644
--- a/test/appender/test_appender.cpp
+++ b/test/appender/test_appender.cpp
@@ -859,3 +859,32 @@ TEST_CASE("Interrupted QueryAppender flow: interrupt -> clear -> close finishes"
 		FAIL("app.Close() did not finish within a second");
 	}
 }
+
+TEST_CASE("Test appender_allocator_flush_threshold", "[appender]") {
+	duckdb::unique_ptr<QueryResult> result;
+	DuckDB db(nullptr);
+	Connection con(db);
+	const size_t blob_size = 100 * 1024;
+	std::vector<uint8_t> data(blob_size, 'A');
+	auto value = duckdb::Value::BLOB(data.data(), data.size());
+	REQUIRE_NO_FAIL(con.Query("SET GLOBAL memory_limit='1GB'"));
+	REQUIRE_NO_FAIL(con.Query("CREATE TABLE my_table (b BLOB)"));
+
+	// Flush when call `EndRow`
+	Appender appender_1(con, "my_table", 16 * 1024 * 1024);
+	for (int i = 0; i < 10000; i++) {
+		appender_1.BeginRow();
+		appender_1.Append(value);
+		appender_1.EndRow();
+	}
+	appender_1.Close();
+
+	// Flush when call `FlushChunk`
+	Appender appender_2(con, "my_table", 64 * 1024);
+	for (int i = 0; i < 10000; i++) {
+		appender_2.BeginRow();
+		appender_2.Append(value);
+		appender_2.EndRow();
+	}
+	appender_2.Close();
+}
diff --git a/test/arrow/arrow_test_helper.cpp b/test/arrow/arrow_test_helper.cpp
index bfb3c0d240..bf56d764ac 100644
--- a/test/arrow/arrow_test_helper.cpp
+++ b/test/arrow/arrow_test_helper.cpp
@@ -173,7 +173,7 @@ bool ArrowTestHelper::CompareResults(Connection &con, unique_ptr<QueryResult> ar
 		for (idx_t i = 0; i < materialized_arrow.types.size(); i++) {
 			if (materialized_arrow.types[i] != duck->types[i] && duck->types[i].id() != LogicalTypeId::ENUM) {
 				mismatch_error = true;
-				error_msg << "Column " << i << "mismatch. DuckDB: '" << duck->types[i].ToString() << "'. Arrow '"
+				error_msg << "Column " << i << " mismatch. DuckDB: '" << duck->types[i].ToString() << "'. Arrow '"
 				          << materialized_arrow.types[i].ToString() << "'\n";
 			}
 		}
diff --git a/test/configs/block_allocator_100mib.json b/test/configs/block_allocator_100mib.json
new file mode 100644
index 0000000000..6486fd0c1a
--- /dev/null
+++ b/test/configs/block_allocator_100mib.json
@@ -0,0 +1,13 @@
+{
+  "description": "Run with block_allocator_memory set to 100MiB.",
+  "on_init": "SET block_allocator_memory='100MiB'",
+  "skip_compiled": "true",
+  "skip_tests": [
+    {
+      "reason": "This test changes block_allocator_memory.",
+      "paths": [
+        "test/sql/settings/block_allocator_memory.test"
+      ]
+    }
+  ]
+}
diff --git a/test/configs/encryption.json b/test/configs/encryption.json
index d1b06ec47b..83aefd880f 100644
--- a/test/configs/encryption.json
+++ b/test/configs/encryption.json
@@ -4,6 +4,10 @@
   "on_new_connection": "USE __test__config__crypto;",
   "on_load": "skip",
   "skip_compiled": "true",
+  "statically_loaded_extensions": [
+    "core_functions",
+    "httpfs"
+  ],
   "skip_tests": [
     {
       "reason": "TODO",
@@ -44,6 +48,14 @@
       "paths": [
         "test/fuzzer/sqlsmith/current_schemas_null.test"
       ]
+    },
+    {
+      "reason": "Expects httpfs to not be present",
+      "paths" : [
+          "test/sql/secrets/create_secret_hffs_autoload.test",
+          "test/sql/secrets/secret_autoloading_errors.test",
+          "test/sql/partitioning/hive_partitioning_autodetect.test"
+      ]
     }
   ]
 }
diff --git a/test/fuzzer/pedro/intersect_correlated_subquery.test b/test/fuzzer/pedro/intersect_correlated_subquery.test
index f95a374afe..d44dd022ef 100644
--- a/test/fuzzer/pedro/intersect_correlated_subquery.test
+++ b/test/fuzzer/pedro/intersect_correlated_subquery.test
@@ -12,6 +12,10 @@ query I
 SELECT (SELECT 1 INTERSECT SELECT 1 HAVING true) FROM t0;
 ----
 
+query I
+SELECT (SELECT 1 INTERSECT SELECT 1 HAVING false) FROM t0;
+----
+
 query I
 SELECT (SELECT 1 INTERSECT SELECT 1 HAVING t0.rowid) FROM t0;
 ----
diff --git a/test/sql/aggregate/having/having_without_groupby.test b/test/sql/aggregate/having/having_without_groupby.test
new file mode 100644
index 0000000000..c7f0a5e733
--- /dev/null
+++ b/test/sql/aggregate/having/having_without_groupby.test
@@ -0,0 +1,42 @@
+# name: test/sql/aggregate/having/having_without_groupby.test
+# group: [having]
+
+query I
+SELECT 1 AS one FROM (
+	values
+	(1,2),
+	(3,2)
+) t(a, b)
+HAVING 1 < 2;
+----
+1
+
+query I
+SELECT 1 AS one FROM (
+	values
+	(1,2),
+	(3,2)
+) t(a, b)
+HAVING false;
+----
+
+statement error
+select a FROM (
+	values
+	(1,2),
+	(3,2)
+) t(a, b)
+HAVING true
+----
+column "a" must appear in the GROUP BY clause
+
+query I
+select sum(a) FROM (
+	values
+	(1,2),
+	(3,2)
+) t(a, b)
+HAVING true
+----
+4
+
diff --git a/test/sql/attach/attach_encrypted_db_key_test.test b/test/sql/attach/attach_encrypted_db_key_test.test
index 951dc1f332..c6322b94c1 100644
--- a/test/sql/attach/attach_encrypted_db_key_test.test
+++ b/test/sql/attach/attach_encrypted_db_key_test.test
@@ -4,6 +4,9 @@
 # workaround - alternative verify always forces the latest storage
 require no_alternative_verify
 
+# We need httpfs to do encrypted writes
+require httpfs
+
 statement error
 ATTACH '__TEST_DIR__/encrypted.duckdb' AS encrypted (ENCRYPTION_KEY);
 ----
diff --git a/test/sql/attach/attach_encryption_block_header.test b/test/sql/attach/attach_encryption_block_header.test
index 3843b9263a..ffd55f3a63 100644
--- a/test/sql/attach/attach_encryption_block_header.test
+++ b/test/sql/attach/attach_encryption_block_header.test
@@ -4,6 +4,9 @@
 # workaround - alternative verify always forces the latest storage
 require no_alternative_verify
 
+# We need httpfs to do encrypted writes
+require httpfs
+
 statement error
 ATTACH '__TEST_DIR__/encrypted.duckdb' AS encrypted (ENCRYPTION_KEY '');
 ----
@@ -14,6 +17,9 @@ ATTACH '__TEST_DIR__/encrypted.duckdb' AS encrypted (ENCRYPTION_KEY 42);
 ----
 Binder Error: "42" is not a valid key. A key must be of type VARCHAR
 
+# We need httpfs to write encrypted database files
+require httpfs
+
 statement ok
 ATTACH '__TEST_DIR__/encrypted.duckdb' AS encrypted (ENCRYPTION_KEY 'asdf');
 
diff --git a/test/sql/attach/attach_encryption_downgrade_prevention.test b/test/sql/attach/attach_encryption_downgrade_prevention.test
new file mode 100644
index 0000000000..fd91169b64
--- /dev/null
+++ b/test/sql/attach/attach_encryption_downgrade_prevention.test
@@ -0,0 +1,22 @@
+# name: test/sql/attach/attach_encryption_downgrade_prevention.test
+# description: Ensure crypto cipher can not be downgraded to strip integrity checks
+# group: [attach]
+
+load __TEST_DIR__/tmp.db
+
+require httpfs
+
+# This is unsafe: an attacker could manipulate  
+statement error
+ATTACH 'data/attach_test/encrypted_ctr_key=abcde.db' as enc (ENCRYPTION_KEY 'abcde');
+----
+Catalog Error: Cannot open encrypted database "data/attach_test/encrypted_ctr_key=abcde.db" without explicitly specifying the encryption cipher for security reasons. Please make sure you understand the security implications and re-attach the database specifying the desired cipher.
+
+# For CTR we need to specify the cipher to ensure we don't accidentally downgrade the cipher 
+statement ok
+ATTACH 'data/attach_test/encrypted_ctr_key=abcde.db' as enc1 (ENCRYPTION_KEY 'abcde', ENCRYPTION_CIPHER 'CTR');
+
+# For GCM this is no problem
+statement ok
+ATTACH 'data/attach_test/encrypted_gcm_key=abcde.db' as enc2 (ENCRYPTION_KEY 'abcde');
+
diff --git a/test/sql/attach/attach_encryption_fallback_readonly.test b/test/sql/attach/attach_encryption_fallback_readonly.test
new file mode 100644
index 0000000000..fad7a4a961
--- /dev/null
+++ b/test/sql/attach/attach_encryption_fallback_readonly.test
@@ -0,0 +1,71 @@
+# name: test/sql/attach/attach_encryption_fallback_readonly.test
+# description: Ensure the fallback crypto implementation is read-only
+# group: [attach]
+
+require vector_size 2048
+
+load __TEST_DIR__/tmp.db
+
+require no_extension_autoloading "EXPECTED: This tests what happens when autoloading is disabled"
+
+# For the test, we disable auto-loading
+statement ok
+set autoinstall_known_extensions=false
+
+# First we try to read an encrypted 
+statement error
+ATTACH 'data/attach_test/encrypted_gcm_key=abcde.db' as enc (ENCRYPTION_KEY 'abcde', ENCRYPTION_CIPHER 'GCM');
+----
+Invalid Configuration Error: The database is encrypted, but DuckDB currently has a read-only crypto module loaded. Either re-open the database using `ATTACH '..' (READONLY)`, or ensure httpfs is loaded using `LOAD httpfs`.
+
+# It works again by setting READONLY
+statement ok
+ATTACH 'data/attach_test/encrypted_gcm_key=abcde.db' as enc (ENCRYPTION_KEY 'abcde', ENCRYPTION_CIPHER 'GCM', READ_ONLY);
+
+query I
+FROM enc.test ORDER BY value
+----
+0
+1
+2
+3
+4
+
+statement ok
+DETACH enc
+
+# Creating a new table will also fail
+statement error
+ATTACH '__TEST_DIR__/test_write_only.db' as enc (ENCRYPTION_KEY 'abcde', ENCRYPTION_CIPHER 'GCM');
+----
+Invalid Configuration Error: The database was opened with encryption enabled, but DuckDB currently has a read-only crypto module loaded. Please re-open using READONLY, or ensure httpfs is loaded using `LOAD httpfs`.
+
+# Loading httpfs will solve all problems
+
+require httpfs
+
+statement ok
+ATTACH 'data/attach_test/encrypted_gcm_key=abcde.db' as enc (ENCRYPTION_KEY 'abcde', ENCRYPTION_CIPHER 'GCM');
+
+query I
+FROM enc.test ORDER BY value
+----
+0
+1
+2
+3
+4
+
+statement ok
+DETACH enc
+
+statement ok
+ATTACH '__TEST_DIR__/test_write_only.db' as enc (ENCRYPTION_KEY 'abcde', ENCRYPTION_CIPHER 'GCM');
+
+statement ok
+CREATE TABLE enc.test AS SELECT 1 as a;
+
+query I
+FROM enc.test
+----
+1
diff --git a/test/sql/copy/encryption/different_aes_ciphers.test b/test/sql/copy/encryption/different_aes_ciphers.test
index ca147d7644..3396117440 100644
--- a/test/sql/copy/encryption/different_aes_ciphers.test
+++ b/test/sql/copy/encryption/different_aes_ciphers.test
@@ -4,6 +4,9 @@
 statement ok
 PRAGMA enable_verification
 
+# We need httpfs to do encrypted writes
+require httpfs
+
 statement error
 ATTACH '__TEST_DIR__/encrypted.duckdb' AS encrypted (ENCRYPTION_KEY '');
 ----
@@ -59,17 +62,11 @@ FROM encrypted.fuu
 statement ok
 DETACH encrypted
 
-# or open it without specifying the cipher, it will be read from file
-statement ok
+# opening the database without cipher for CTR is not possible for security reasons
+statement error
 ATTACH '__TEST_DIR__/encrypted_default_cipher.duckdb' AS encrypted (ENCRYPTION_KEY 'asdf');
-
-query I
-FROM encrypted.fuu
 ----
-42
-
-statement ok
-DETACH encrypted
+Catalog Error: Cannot open encrypted database
 
 # but it will fail if we specify the wrong one
 statement error
diff --git a/test/sql/copy/encryption/encrypted_to_unencrypted.test_slow b/test/sql/copy/encryption/encrypted_to_unencrypted.test_slow
index 751b16a068..1cfa19a5a0 100644
--- a/test/sql/copy/encryption/encrypted_to_unencrypted.test_slow
+++ b/test/sql/copy/encryption/encrypted_to_unencrypted.test_slow
@@ -5,6 +5,9 @@ require skip_reload
 
 require tpch
 
+# We need httpfs to do encrypted writes
+require httpfs
+
 statement ok
 PRAGMA enable_verification
 
diff --git a/test/sql/copy/encryption/encryption_storage_versions.test b/test/sql/copy/encryption/encryption_storage_versions.test
index 62467fd5e4..39f33ee27c 100644
--- a/test/sql/copy/encryption/encryption_storage_versions.test
+++ b/test/sql/copy/encryption/encryption_storage_versions.test
@@ -1,6 +1,9 @@
 # name: test/sql/copy/encryption/encryption_storage_versions.test
 # group: [encryption]
 
+# We need httpfs to do encrypted writes
+require httpfs
+
 statement ok
 PRAGMA enable_verification
 
@@ -140,4 +143,4 @@ SELECT SUM(i) FROM unencrypted_v_1_2_0.tbl;
 query I
 SELECT SUM(i) FROM unencrypted_new.tbl;
 ----
-45
\ No newline at end of file
+45
diff --git a/test/sql/copy/encryption/multiple_encrypted_databases.test_slow b/test/sql/copy/encryption/multiple_encrypted_databases.test_slow
index 2ada71319e..6c03124f09 100644
--- a/test/sql/copy/encryption/multiple_encrypted_databases.test_slow
+++ b/test/sql/copy/encryption/multiple_encrypted_databases.test_slow
@@ -5,6 +5,9 @@ require skip_reload
 
 require tpch
 
+# We need httpfs to do encrypted writes
+require httpfs
+
 statement ok
 PRAGMA enable_verification
 
diff --git a/test/sql/copy/encryption/reencrypt.test_slow b/test/sql/copy/encryption/reencrypt.test_slow
index 1974e3e915..44cc1afb1a 100644
--- a/test/sql/copy/encryption/reencrypt.test_slow
+++ b/test/sql/copy/encryption/reencrypt.test_slow
@@ -5,6 +5,9 @@ require skip_reload
 
 require tpch
 
+# We need httpfs to do encrypted writes
+require httpfs
+
 statement ok
 PRAGMA enable_verification
 
diff --git a/test/sql/copy/encryption/tpch_sf1_encrypted.test_slow b/test/sql/copy/encryption/tpch_sf1_encrypted.test_slow
index 47c5458026..4946541fab 100644
--- a/test/sql/copy/encryption/tpch_sf1_encrypted.test_slow
+++ b/test/sql/copy/encryption/tpch_sf1_encrypted.test_slow
@@ -4,6 +4,9 @@
 
 require tpch
 
+# We need httpfs to do encrypted writes
+require httpfs
+
 statement ok
 pragma verify_external
 
@@ -47,4 +50,4 @@ PRAGMA tpch(${i})
 ----
 <FILE>:extension/tpch/dbgen/answers/sf1/q${i}.csv
 
-endloop
\ No newline at end of file
+endloop
diff --git a/test/sql/copy/encryption/unencrypted_to_encrypted.test b/test/sql/copy/encryption/unencrypted_to_encrypted.test
index d79741decf..19526e6fc5 100644
--- a/test/sql/copy/encryption/unencrypted_to_encrypted.test
+++ b/test/sql/copy/encryption/unencrypted_to_encrypted.test
@@ -5,6 +5,9 @@ require skip_reload
 
 require tpch
 
+# We need httpfs to do encrypted writes
+require httpfs
+
 statement ok
 PRAGMA enable_verification
 
diff --git a/test/sql/copy/encryption/unencrypted_to_encrypted_direct_query.test b/test/sql/copy/encryption/unencrypted_to_encrypted_direct_query.test
index f77f8db633..08ed9ea3e2 100644
--- a/test/sql/copy/encryption/unencrypted_to_encrypted_direct_query.test
+++ b/test/sql/copy/encryption/unencrypted_to_encrypted_direct_query.test
@@ -5,6 +5,9 @@ require skip_reload
 
 require tpch
 
+# We need httpfs to do encrypted writes
+require httpfs
+
 statement ok
 PRAGMA enable_verification
 
@@ -26,4 +29,4 @@ COPY FROM DATABASE unencrypted to encrypted;
 query I
 SELECT SUM(i) FROM encrypted.tbl;
 ----
-45
\ No newline at end of file
+45
diff --git a/test/sql/copy/encryption/write_encrypted_database.test b/test/sql/copy/encryption/write_encrypted_database.test
index f7f9508fba..a2124925f0 100644
--- a/test/sql/copy/encryption/write_encrypted_database.test
+++ b/test/sql/copy/encryption/write_encrypted_database.test
@@ -3,6 +3,9 @@
 
 require skip_reload
 
+# We need httpfs to do encrypted writes
+require httpfs
+
 statement ok
 PRAGMA enable_verification
 
@@ -35,4 +38,4 @@ ATTACH '__TEST_DIR__/encrypted.duckdb' AS encrypted (ENCRYPTION_KEY 'asdf');
 query I
 SELECT SUM(i) FROM encrypted.tbl
 ----
-45
\ No newline at end of file
+45
diff --git a/test/sql/copy/parquet/parquet_encryption.test b/test/sql/copy/parquet/parquet_encryption.test
index a825520408..a1d7ff68e2 100644
--- a/test/sql/copy/parquet/parquet_encryption.test
+++ b/test/sql/copy/parquet/parquet_encryption.test
@@ -7,6 +7,9 @@ require parquet
 # parquet keys are not persisted across restarts
 require noforcestorage
 
+# writing encrypted parquet requires httpfs to be loaded
+require httpfs
+
 statement ok
 PRAGMA enable_verification
 
diff --git a/test/sql/copy/parquet/parquet_encryption_tpch.test_slow b/test/sql/copy/parquet/parquet_encryption_tpch.test_slow
index 7f2644162f..9268ee88a5 100644
--- a/test/sql/copy/parquet/parquet_encryption_tpch.test_slow
+++ b/test/sql/copy/parquet/parquet_encryption_tpch.test_slow
@@ -6,6 +6,8 @@ require parquet
 
 require tpch
 
+require httpfs
+
 statement ok
 CALL dbgen(sf=1)
 
diff --git a/test/sql/function/generic/test_sleep.test b/test/sql/function/generic/test_sleep.test
index fb3d8f97f7..c014bcb2ea 100644
--- a/test/sql/function/generic/test_sleep.test
+++ b/test/sql/function/generic/test_sleep.test
@@ -126,7 +126,7 @@ statement ok
 CREATE OR REPLACE TABLE sleep_profiling_multi AS SELECT * FROM read_json('__TEST_DIR__/sleep_profiling_multi.json')
 
 query T
-SELECT cpu_time >= 0.05 AND cpu_time < 0.10 FROM sleep_profiling_multi WHERE contains(extra_info, 'sleep_ms') OR contains(query_name, 'sleep_ms') LIMIT 1
+SELECT cpu_time >= 0.04 AND cpu_time < 0.15 FROM sleep_profiling_multi WHERE contains(extra_info, 'sleep_ms') OR contains(query_name, 'sleep_ms') LIMIT 1
 ----
 true
 
diff --git a/test/sql/json/test_json_serialize_sql.test b/test/sql/json/test_json_serialize_sql.test
index 01ff0641a0..b9459e17e7 100644
--- a/test/sql/json/test_json_serialize_sql.test
+++ b/test/sql/json/test_json_serialize_sql.test
@@ -86,6 +86,12 @@ PRAGMA json_execute_serialized_sql(
 15
 24
 
+# Test execute json serialized sql with multiple nested type tags
+query I
+select json_serialize_sql($$select '10'::blob$$);
+----
+<!REGEX>:.*query_location:.*11
+
 # TODO: We should add an option for the deserializer to allow missing properties in the JSON if they can be default constructed
 # Alternatively, make them optional for all the Deserializer's.
 statement error
diff --git a/test/sql/logging/http_log_timing.test b/test/sql/logging/http_log_timing.test
new file mode 100644
index 0000000000..a6ba4ebd83
--- /dev/null
+++ b/test/sql/logging/http_log_timing.test
@@ -0,0 +1,26 @@
+# name: test/sql/logging/http_log_timing.test
+# description: Test basic logging functionality
+# group: [logging]
+
+require noforcestorage
+
+require httpfs
+
+statement ok
+CALL enable_logging('HTTP')
+
+statement ok
+FROM 'https://github.com/duckdb/duckdb-data/releases/download/v1.0/title.principals.tsv'
+
+# Confirm that our request timing returns something reasonable
+query  III
+SELECT 
+	request.type, 
+	request.duration_ms >= 0,
+	request.duration_ms <= 1000 * 1000
+FROM 
+	duckdb_logs_parsed('HTTP')
+WHERE 
+	request.type='HEAD'
+----
+HEAD	true	true
diff --git a/test/sql/logging/logging_call_functions.test b/test/sql/logging/logging_call_functions.test
index dc0fc751cf..9db811ac80 100644
--- a/test/sql/logging/logging_call_functions.test
+++ b/test/sql/logging/logging_call_functions.test
@@ -93,7 +93,7 @@ logging_storage	memory
 ###
 
 statement ok
-CALL enable_logging(['QueryLog', 'FileSystem']);
+CALL enable_logging(['QueryLog', 'filesystem']);
 
 # Use sorted list to avoid indeterministic result
 query II
diff --git a/test/sql/logging/logging_file_bind_replace.test b/test/sql/logging/logging_file_bind_replace.test
index e33d043f57..aa27d0e170 100644
--- a/test/sql/logging/logging_file_bind_replace.test
+++ b/test/sql/logging/logging_file_bind_replace.test
@@ -30,6 +30,11 @@ SELECT message FROM duckdb_logs_parsed('QueryLog') WHERE starts_with(message, 'S
 ----
 SELECT 1 as a
 
+query I
+SELECT message FROM duckdb_logs_parsed('querylog') WHERE starts_with(message, 'SELECT 1');
+----
+SELECT 1 as a
+
 statement ok
 CALL truncate_duckdb_logs();
 
diff --git a/test/sql/logging/physical_operator_logging.test_slow b/test/sql/logging/physical_operator_logging.test_slow
index d399f90585..29373c2444 100644
--- a/test/sql/logging/physical_operator_logging.test_slow
+++ b/test/sql/logging/physical_operator_logging.test_slow
@@ -43,6 +43,11 @@ select count(*) from duckdb_logs_parsed('PhysicalOperator') where class = 'JoinH
 ----
 16
 
+query I
+select info.total_probe_matches from duckdb_logs_parsed('PhysicalOperator') where class = 'PhysicalHashJoin' and event = 'GetData'
+----
+3000000
+
 # all flushed row groups should be logged, these should be equal
 query I
 select count(*) = (
diff --git a/test/sql/merge/merge_into_subquery_action.test b/test/sql/merge/merge_into_subquery_action.test
new file mode 100644
index 0000000000..b887512d95
--- /dev/null
+++ b/test/sql/merge/merge_into_subquery_action.test
@@ -0,0 +1,44 @@
+# name: test/sql/merge/merge_into_subquery_action.test
+# description: Test MERGE INTO with subqueries in the merge action condition
+# group: [merge]
+
+statement ok
+CREATE TABLE Totals(item_id int, balance int, biggest_item BOOL);
+
+statement ok
+CREATE TABLE Buy(item_id int, volume int);
+
+statement ok
+INSERT INTO Buy values(10, 1000), (30, 300), (20, 2000);
+
+statement ok
+MERGE INTO Totals USING Buy USING (item_id)
+WHEN NOT MATCHED AND Buy.volume = (SELECT MAX(Volume) FROM Buy)
+	THEN INSERT VALUES (Buy.item_id, Buy.volume, true)
+WHEN NOT MATCHED
+	THEN INSERT VALUES (Buy.item_id, Buy.volume, false)
+
+query III
+SELECT * FROM Totals ORDER BY item_id
+----
+10	1000	false
+20	2000	true
+30	300	false
+
+# original issue
+statement ok
+CREATE TABLE dummy_edge(id INTEGER, ref_id INTEGER, "value" VARCHAR, note VARCHAR);
+
+statement ok
+CREATE TABLE dummy_user(user_id INTEGER, "name" VARCHAR, email VARCHAR, created_at DATE);
+
+statement ok
+CREATE TABLE dummy_null(id INTEGER, "value" INTEGER, optional_text VARCHAR);
+
+statement ok
+MERGE INTO main.dummy_edge as target_0
+USING dummy_user as ref_0
+ON target_0.note = ref_0.name
+WHEN NOT MATCHED AND EXISTS (
+  SELECT id FROM main.dummy_null WHERE true
+) THEN DO NOTHING
diff --git a/test/sql/pragma/profiling/test_logging_interaction.test b/test/sql/pragma/profiling/test_logging_interaction.test
index 745a3b324a..35b676b072 100644
--- a/test/sql/pragma/profiling/test_logging_interaction.test
+++ b/test/sql/pragma/profiling/test_logging_interaction.test
@@ -32,6 +32,6 @@ statement ok
 PRAGMA disable_profiling;
 
 query I
-SELECT count(*) FROM duckdb_logs() WHERE type == 'Metrics' AND message ILIKE '%CPU_TIME%';
+SELECT count(*) FROM duckdb_logs_parsed('Metrics') WHERE metric == 'CPU_TIME';
 ----
 3
diff --git a/test/sql/settings/block_allocator_memory.test b/test/sql/settings/block_allocator_memory.test
new file mode 100644
index 0000000000..8908279939
--- /dev/null
+++ b/test/sql/settings/block_allocator_memory.test
@@ -0,0 +1,55 @@
+# name: test/sql/settings/block_allocator_memory.test
+# description: Test block_allocator_memory setting
+# group: [settings]
+
+# defaults to 0, resetting to 0 is ok
+statement ok
+RESET block_allocator_memory;
+
+statement ok
+SET block_allocator_memory='100MiB';
+
+# can also be set as a percentage of the memory limit
+statement ok
+SET memory_limit='200MiB';
+
+statement error
+SET block_allocator_memory='-3%';
+----
+Unable to parse valid percentage
+
+statement error
+SET block_allocator_memory='150%';
+----
+Unable to parse valid percentage
+
+statement ok
+SET block_allocator_memory='75%';
+
+query I
+SELECT value FROM duckdb_settings() WHERE name = 'block_allocator_memory';
+----
+150.0 MiB
+
+require 64bit
+
+# cannot be reduced
+statement error
+RESET block_allocator_memory;
+----
+cannot be reduced
+
+statement error
+SET block_allocator_memory='50MiB';
+----
+cannot be reduced
+
+# can be increased
+statement ok
+SET block_allocator_memory='200MiB';
+
+# cannot be greater than VM size
+statement error
+SET block_allocator_memory='200TiB';
+----
+cannot be greater than the virtual memory size
diff --git a/test/sql/storage/compression/rle/rle_select_list.test b/test/sql/storage/compression/rle/rle_select_list.test
new file mode 100644
index 0000000000..0bb572fc0a
--- /dev/null
+++ b/test/sql/storage/compression/rle/rle_select_list.test
@@ -0,0 +1,38 @@
+# name: test/sql/storage/compression/rle/rle_select_list.test
+# description: Test selecting from RLE compression with a list comparison pushed down
+# group: [rle]
+
+load __TEST_DIR__/rle_select_bug.db
+
+statement ok
+pragma force_compression='rle';
+
+statement ok
+create table tbl as SELECT * FROM (
+    VALUES
+        (['first name', 'last name', 'username'], 60),
+        (['first name'], 0),
+        (['username'], 0),
+        (['first name', 'last name', 'username'], 0),
+        (['first name', 'last name', 'username'], 0),
+        (['username'], 0),
+        (['username'], 0)
+) AS t(attributes, minutes_duration);
+
+statement ok
+checkpoint
+
+query I
+SELECT
+   "minutes_duration"
+FROM
+   tbl
+WHERE NOT list_sort(['first name']) = tbl."attributes"
+ORDER BY ALL
+----
+0
+0
+0
+0
+0
+60
diff --git a/test/sql/storage/encryption/temp_files/encrypt_asof_join_merge.test_slow b/test/sql/storage/encryption/temp_files/encrypt_asof_join_merge.test_slow
index 1c705ae1c6..e79f371d42 100644
--- a/test/sql/storage/encryption/temp_files/encrypt_asof_join_merge.test_slow
+++ b/test/sql/storage/encryption/temp_files/encrypt_asof_join_merge.test_slow
@@ -2,6 +2,9 @@
 # description: Test merge queue and repartitioning with encrypted temporary files
 # group: [temp_files]
 
+# We need httpfs to do encrypted writes
+require httpfs
+
 foreach cipher GCM CTR
 
 statement ok
@@ -41,4 +44,4 @@ FROM probe ASOF JOIN build USING(k, t)
 
 restart
 
-endloop
\ No newline at end of file
+endloop
diff --git a/test/sql/storage/encryption/temp_files/encrypted_offloading_block_files.test_slow b/test/sql/storage/encryption/temp_files/encrypted_offloading_block_files.test_slow
index 280803a839..e032536193 100644
--- a/test/sql/storage/encryption/temp_files/encrypted_offloading_block_files.test_slow
+++ b/test/sql/storage/encryption/temp_files/encrypted_offloading_block_files.test_slow
@@ -1,8 +1,10 @@
 # name: test/sql/storage/encryption/temp_files/encrypted_offloading_block_files.test_slow
 # group: [temp_files]
 
-foreach cipher GCM CTR
+# We need httpfs to do encrypted writes
+require httpfs
 
+foreach cipher GCM CTR
 
 require block_size 262144
 
@@ -40,4 +42,4 @@ SELECT * FROM tbl ORDER BY random_value;
 
 restart
 
-endloop
\ No newline at end of file
+endloop
diff --git a/test/sql/storage/encryption/temp_files/encrypted_tmp_file_setting.test b/test/sql/storage/encryption/temp_files/encrypted_tmp_file_setting.test
index 4c402d7846..0f29ba99df 100644
--- a/test/sql/storage/encryption/temp_files/encrypted_tmp_file_setting.test
+++ b/test/sql/storage/encryption/temp_files/encrypted_tmp_file_setting.test
@@ -1,6 +1,9 @@
 # name: test/sql/storage/encryption/temp_files/encrypted_tmp_file_setting.test
 # group: [temp_files]
 
+# httpfs require to write encrypted data
+require httpfs
+
 foreach cipher GCM CTR
 
 require block_size 262144
@@ -49,4 +52,4 @@ Permission Error: Existing temporary files found: Modifying the temp_file_encryp
 
 restart
 
-endloop
\ No newline at end of file
+endloop
diff --git a/test/sql/storage/encryption/temp_files/encrypted_tpch_join.test_slow b/test/sql/storage/encryption/temp_files/encrypted_tpch_join.test_slow
index 2491d64495..96da86ca53 100644
--- a/test/sql/storage/encryption/temp_files/encrypted_tpch_join.test_slow
+++ b/test/sql/storage/encryption/temp_files/encrypted_tpch_join.test_slow
@@ -5,6 +5,8 @@ foreach cipher GCM CTR
 
 require tpch
 
+require httpfs
+
 statement ok
 SET threads = 8;
 
@@ -32,4 +34,4 @@ JOIN lineitem2 l2 USING (l_orderkey , l_linenumber);
 
 restart
 
-endloop
\ No newline at end of file
+endloop
diff --git a/test/sql/storage/encryption/temp_files/temp_directory_enable_external_access.test b/test/sql/storage/encryption/temp_files/temp_directory_enable_external_access.test
index 1a9a970174..22b6ad7c57 100644
--- a/test/sql/storage/encryption/temp_files/temp_directory_enable_external_access.test
+++ b/test/sql/storage/encryption/temp_files/temp_directory_enable_external_access.test
@@ -1,8 +1,10 @@
 # name: test/sql/storage/encryption/temp_files/temp_directory_enable_external_access.test
 # group: [temp_files]
 
-foreach cipher GCM CTR
+# httpfs require to write encrypted data
+require httpfs
 
+foreach cipher GCM CTR
 
 require block_size 262144
 
@@ -38,4 +40,4 @@ CREATE TEMPORARY TABLE tbl AS FROM range(10_000_000)
 
 restart
 
-endloop
\ No newline at end of file
+endloop
diff --git a/test/sql/storage/encryption/wal/encrypted_wal_blob_storage.test b/test/sql/storage/encryption/wal/encrypted_wal_blob_storage.test
index d92c055832..ae29847af6 100644
--- a/test/sql/storage/encryption/wal/encrypted_wal_blob_storage.test
+++ b/test/sql/storage/encryption/wal/encrypted_wal_blob_storage.test
@@ -2,8 +2,10 @@
 # description: Test BLOB with persistent storage with an encrypted WAL
 # group: [wal]
 
-foreach cipher GCM CTR
+# httpfs require to write encrypted data
+require httpfs
 
+foreach cipher GCM CTR
 
 load __TEST_DIR__/any_file.db
 
@@ -28,7 +30,7 @@ statement ok
 DETACH enc
 
 statement ok
-ATTACH '__TEST_DIR__/encrypted_blob_storage_${cipher}.db' AS enc (ENCRYPTION_KEY 'asdf');
+ATTACH '__TEST_DIR__/encrypted_blob_storage_${cipher}.db' AS enc (ENCRYPTION_KEY 'asdf', ENCRYPTION_CIPHER '${cipher}');
 
 query I
 SELECT * FROM enc.blobs
diff --git a/test/sql/storage/encryption/wal/encrypted_wal_lazy_creation.test b/test/sql/storage/encryption/wal/encrypted_wal_lazy_creation.test
index a704efb73c..cbb404785c 100644
--- a/test/sql/storage/encryption/wal/encrypted_wal_lazy_creation.test
+++ b/test/sql/storage/encryption/wal/encrypted_wal_lazy_creation.test
@@ -13,6 +13,9 @@ require noforcestorage
 
 require skip_reload
 
+# We need httpfs to do encrypted writes
+require httpfs
+
 statement ok
 ATTACH '__TEST_DIR__/attach_no_wal_${cipher}.db' AS attach_no_wal (ENCRYPTION_KEY 'asdf', ENCRYPTION_CIPHER '${cipher}');
 
@@ -42,4 +45,4 @@ SELECT COUNT(*) FROM attach_no_wal.integers;
 
 restart
 
-endloop
\ No newline at end of file
+endloop
diff --git a/test/sql/storage/encryption/wal/encrypted_wal_pragmas.test b/test/sql/storage/encryption/wal/encrypted_wal_pragmas.test
index d42b1b73d9..f6b31638f7 100644
--- a/test/sql/storage/encryption/wal/encrypted_wal_pragmas.test
+++ b/test/sql/storage/encryption/wal/encrypted_wal_pragmas.test
@@ -2,6 +2,9 @@
 # description: test encrypted wal debug PRAGMAS
 # group: [wal]
 
+# We need httpfs to do encrypted writes
+require httpfs
+
 foreach cipher GCM CTR
 
 
@@ -41,7 +44,7 @@ DETACH enc
 
 # WAL replay succeeds
 statement ok
-ATTACH '__TEST_DIR__/encrypted_wal_restart_${cipher}.db' as enc (ENCRYPTION_KEY 'asdf');
+ATTACH '__TEST_DIR__/encrypted_wal_restart_${cipher}.db' as enc (ENCRYPTION_KEY 'asdf', ENCRYPTION_CIPHER '${cipher}');
 
 statement ok
 DETACH enc
@@ -75,7 +78,7 @@ SELECT * FROM enc.test ORDER BY 1
 restart
 
 statement ok
-ATTACH '__TEST_DIR__/encrypted_wal_restart_new_${cipher}.db' as enc (ENCRYPTION_KEY 'asdf');
+ATTACH '__TEST_DIR__/encrypted_wal_restart_new_${cipher}.db' as enc (ENCRYPTION_KEY 'asdf', ENCRYPTION_CIPHER '${cipher}');
 
 query IT
 SELECT * FROM enc.test ORDER BY 1
@@ -130,4 +133,4 @@ SELECT * FROM enc.test ORDER BY 1
 
 restart
 
-endloop
\ No newline at end of file
+endloop
diff --git a/test/sql/storage/encryption/wal/encrypted_wal_restart.test b/test/sql/storage/encryption/wal/encrypted_wal_restart.test
index 197990dfa6..f9a9cf8714 100644
--- a/test/sql/storage/encryption/wal/encrypted_wal_restart.test
+++ b/test/sql/storage/encryption/wal/encrypted_wal_restart.test
@@ -2,6 +2,9 @@
 # description: test wal restart
 # group: [wal]
 
+# We need httpfs to do encrypted writes
+require httpfs
+
 foreach cipher GCM CTR
 
 load __TEST_DIR__/any_wal_db.db
@@ -73,4 +76,4 @@ SELECT * FROM enc.test ORDER BY 1
 
 endloop
 
-restart
\ No newline at end of file
+restart
diff --git a/test/sql/storage/types/struct/struct_of_empty_list.test b/test/sql/storage/types/struct/struct_of_empty_list.test
new file mode 100644
index 0000000000..392e05b07e
--- /dev/null
+++ b/test/sql/storage/types/struct/struct_of_empty_list.test
@@ -0,0 +1,10 @@
+# name: test/sql/storage/types/struct/struct_of_empty_list.test
+# group: [struct]
+
+load __TEST_DIR__/empty_list_in_struct.db
+
+statement ok
+create table tbl (col STRUCT(a VARCHAR[]))
+
+statement ok
+insert into tbl SELECT {'a': []} from range(122881)
diff --git a/test/sql/topn/top_n_hard_limit.test b/test/sql/topn/top_n_hard_limit.test
index dfd5bda61e..27255ecca1 100644
--- a/test/sql/topn/top_n_hard_limit.test
+++ b/test/sql/topn/top_n_hard_limit.test
@@ -13,7 +13,7 @@ USE db
 # 0 1 2 3 4 5 6 7 8 9	 ASC     DESC
 # [     ]              : 0       10240
 # [     ]              : 2       4097
-#   [     ]            : 4097    4097
+#   [       ]          : 4097    4097
 #         [   ]        : 8192    2049
 #               [ ]    : 10240   2048
 
@@ -24,7 +24,7 @@ statement ok
 INSERT INTO t SELECT 0 i UNION ALL SELECT i FROM repeat(3, 2047) t1(i)
 
 statement ok
-INSERT INTO t SELECT 1 i UNION ALL SELECT i FROM repeat(4, 2047) t1(i)
+INSERT INTO t SELECT 1 i UNION ALL SELECT i FROM repeat(5, 2047) t1(i)
 
 statement ok
 INSERT INTO t SELECT i FROM repeat(4, 2047) t1(i) UNION ALL SELECT 6 i
@@ -91,11 +91,6 @@ EXPLAIN ANALYZE SELECT * FROM t ORDER BY i DESC LIMIT 4097
 ----
 analyzed_plan	<REGEX>:.*TABLE_SCAN.*6,144 rows.*
 
-query II
-EXPLAIN ANALYZE SELECT * FROM t ORDER BY i DESC LIMIT 4098
-----
-analyzed_plan	<REGEX>:.*TABLE_SCAN.*10,240 rows.*
-
 statement ok
 INSERT INTO t SELECT 9 i UNION ALL SELECT i FROM repeat(NULL, 2046) t1(i) UNION ALL SELECT 10 i
 
diff --git a/test/sql/topn/topn_nulls_first.test b/test/sql/topn/topn_nulls_first.test
new file mode 100644
index 0000000000..78c3ca243c
--- /dev/null
+++ b/test/sql/topn/topn_nulls_first.test
@@ -0,0 +1,32 @@
+# name: test/sql/topn/topn_nulls_first.test
+# description: Ensure ORDER BY ... NULLS FIRST LIMIT uses TopN path
+# group: [topn]
+
+statement ok
+PRAGMA enable_verification
+
+statement ok
+CREATE TABLE topn_nulls(v INTEGER)
+
+statement ok
+INSERT INTO topn_nulls
+SELECT CASE WHEN i % 3 = 0 THEN NULL ELSE i END
+FROM range(12) tbl(i)
+
+statement ok
+PRAGMA explain_output='OPTIMIZED_ONLY'
+
+query II nosort
+EXPLAIN SELECT v FROM topn_nulls ORDER BY v NULLS FIRST LIMIT 6;
+----
+logical_opt	<REGEX>:.*TOP_N.*optional: v IS NULL.*
+
+query I
+SELECT v FROM topn_nulls ORDER BY v NULLS FIRST LIMIT 6;
+----
+NULL
+NULL
+NULL
+NULL
+1
+2
diff --git a/test/sql/types/nested/unnest_range_plan.test b/test/sql/types/nested/unnest_range_plan.test
new file mode 100644
index 0000000000..565f0d327c
--- /dev/null
+++ b/test/sql/types/nested/unnest_range_plan.test
@@ -0,0 +1,13 @@
+# name: test/sql/types/nested/unnest_range_plan.test
+# group: [nested]
+
+# Fix issue #19645
+
+query I rowsort
+SELECT *
+FROM UNNEST(ARRAY[6]) AS x
+UNION ALL
+SELECT 2 FROM generate_series(1, 1);
+----
+2
+6
\ No newline at end of file
diff --git a/test/sqlite/result_helper.cpp b/test/sqlite/result_helper.cpp
index 5f4675ab0f..75247d3df2 100644
--- a/test/sqlite/result_helper.cpp
+++ b/test/sqlite/result_helper.cpp
@@ -303,7 +303,7 @@ bool TestResultHelper::CheckStatementResult(const Statement &statement, ExecuteC
 			logger.InternalException(result);
 			return false;
 		}
-		if (expected_result == ExpectedResult::RESULT_UNKNOWN) {
+		if (expected_result == ExpectedResult::RESULT_UNKNOWN || expected_result == ExpectedResult::RESULT_DONT_CARE) {
 			error = false;
 		} else {
 			error = !error;
diff --git a/test/sqlite/sqllogic_command.hpp b/test/sqlite/sqllogic_command.hpp
index 8580bb1e54..76a1581011 100644
--- a/test/sqlite/sqllogic_command.hpp
+++ b/test/sqlite/sqllogic_command.hpp
@@ -15,7 +15,7 @@
 namespace duckdb {
 class SQLLogicTestRunner;
 
-enum class ExpectedResult : uint8_t { RESULT_SUCCESS, RESULT_ERROR, RESULT_UNKNOWN };
+enum class ExpectedResult : uint8_t { RESULT_SUCCESS, RESULT_ERROR, RESULT_UNKNOWN, RESULT_DONT_CARE };
 
 struct LoopDefinition {
 	string loop_iterator_name;
diff --git a/test/sqlite/sqllogic_parser.cpp b/test/sqlite/sqllogic_parser.cpp
index df59aedb37..638a054e50 100644
--- a/test/sqlite/sqllogic_parser.cpp
+++ b/test/sqlite/sqllogic_parser.cpp
@@ -87,16 +87,20 @@ vector<string> SQLLogicParser::ExtractExpectedResult() {
 	return result;
 }
 
-string SQLLogicParser::ExtractExpectedError(bool expect_ok, bool original_sqlite_test) {
+string SQLLogicParser::ExtractExpectedError(ExpectedResult expected_result, bool original_sqlite_test) {
+	bool expect_error_message =
+	    expected_result == ExpectedResult::RESULT_ERROR || expected_result == ExpectedResult::RESULT_UNKNOWN;
+
 	// check if there is an expected error at all
 	if (current_line >= lines.size() || lines[current_line] != "----") {
-		if (!expect_ok && !original_sqlite_test) {
-			Fail("Failed to parse statement: statement error needs to have an expected error message");
+		if (expect_error_message && !original_sqlite_test) {
+			Fail("Failed to parse statement: statement error and maybe needs to have an expected error message");
 		}
 		return string();
 	}
-	if (expect_ok) {
-		Fail("Failed to parse statement: only statement error can have an expected error message, not statement ok");
+	if (!expect_error_message) {
+		Fail("Failed to parse statement: only statement error or maybe can have an expected error message, not "
+		     "statement ok");
 	}
 	current_line++;
 	string error;
diff --git a/test/sqlite/sqllogic_parser.hpp b/test/sqlite/sqllogic_parser.hpp
index eff512ad22..3b640e24fc 100644
--- a/test/sqlite/sqllogic_parser.hpp
+++ b/test/sqlite/sqllogic_parser.hpp
@@ -11,6 +11,7 @@
 #include "duckdb.hpp"
 #include "duckdb/common/types.hpp"
 #include "duckdb/common/exception_format_value.hpp"
+#include "sqllogic_command.hpp"
 
 namespace duckdb {
 
@@ -86,7 +87,7 @@ public:
 	vector<string> ExtractExpectedResult();
 
 	//! Extract the expected error (in case of statement error)
-	string ExtractExpectedError(bool expect_ok, bool original_sqlite_test);
+	string ExtractExpectedError(ExpectedResult expected_result, bool original_sqlite_test);
 
 	//! Tokenize the current line
 	SQLLogicToken Tokenize();
diff --git a/test/sqlite/sqllogic_test_runner.cpp b/test/sqlite/sqllogic_test_runner.cpp
index f6d51bfd49..c86f7f4eda 100644
--- a/test/sqlite/sqllogic_test_runner.cpp
+++ b/test/sqlite/sqllogic_test_runner.cpp
@@ -835,6 +835,8 @@ void SQLLogicTestRunner::ExecuteFile(string script) {
 			}
 			auto command = make_uniq<Statement>(*this);
 
+			bool original_output_result_mode = output_result_mode;
+
 			// parse the first parameter
 			if (token.parameters[0] == "ok") {
 				command->expected_result = ExpectedResult::RESULT_SUCCESS;
@@ -842,6 +844,13 @@ void SQLLogicTestRunner::ExecuteFile(string script) {
 				command->expected_result = ExpectedResult::RESULT_ERROR;
 			} else if (token.parameters[0] == "maybe") {
 				command->expected_result = ExpectedResult::RESULT_UNKNOWN;
+			} else if (token.parameters[0] == "debug") {
+				command->expected_result = ExpectedResult::RESULT_DONT_CARE;
+				output_result_mode = true;
+			} else if (token.parameters[0] == "debug_skip") {
+				command->expected_result = ExpectedResult::RESULT_DONT_CARE;
+				output_result_mode = true;
+				skip_level++;
 			} else {
 				parser.Fail("statement argument should be 'ok' or 'error");
 			}
@@ -855,8 +864,7 @@ void SQLLogicTestRunner::ExecuteFile(string script) {
 			if (statement_text.empty()) {
 				parser.Fail("Unexpected empty statement text");
 			}
-			command->expected_error = parser.ExtractExpectedError(
-			    command->expected_result == ExpectedResult::RESULT_SUCCESS, original_sqlite_test);
+			command->expected_error = parser.ExtractExpectedError(command->expected_result, original_sqlite_test);
 
 			// perform any renames in the text
 			command->base_sql_query = ReplaceKeywords(std::move(statement_text));
@@ -866,6 +874,7 @@ void SQLLogicTestRunner::ExecuteFile(string script) {
 			}
 			command->conditions = std::move(conditions);
 			ExecuteCommand(std::move(command));
+			output_result_mode = original_output_result_mode;
 		} else if (token.type == SQLLogicTokenType::SQLLOGIC_QUERY) {
 			if (token.parameters.size() < 1) {
 				parser.Fail("query requires at least one parameter (query III)");
diff --git a/third_party/mbedtls/include/mbedtls_wrapper.hpp b/third_party/mbedtls/include/mbedtls_wrapper.hpp
index d9f8111d80..e6e97a7179 100644
--- a/third_party/mbedtls/include/mbedtls_wrapper.hpp
+++ b/third_party/mbedtls/include/mbedtls_wrapper.hpp
@@ -81,6 +81,7 @@ class AESStateMBEDTLS : public duckdb::EncryptionState {
 		DUCKDB_API void GenerateRandomData(duckdb::data_ptr_t data, duckdb::idx_t len) override;
 		DUCKDB_API void FinalizeGCM(duckdb::data_ptr_t tag, duckdb::idx_t tag_len);
 		DUCKDB_API const mbedtls_cipher_info_t *GetCipher(size_t key_len);
+		DUCKDB_API static void SecureClearData(duckdb::data_ptr_t data, duckdb::idx_t len);
 
 	private:
 		DUCKDB_API void InitializeInternal(duckdb::const_data_ptr_t iv, duckdb::idx_t iv_len, duckdb::const_data_ptr_t aad, duckdb::idx_t aad_len);
@@ -98,6 +99,10 @@ class AESStateMBEDTLS : public duckdb::EncryptionState {
 		}
 
 		~AESStateMBEDTLSFactory() override {} //
+
+		DUCKDB_API bool SupportsEncryption() override {
+			return false;
+		}
 	};
 };
 
diff --git a/third_party/mbedtls/mbedtls_wrapper.cpp b/third_party/mbedtls/mbedtls_wrapper.cpp
index 7dc0af7fd1..3a6ce981ed 100644
--- a/third_party/mbedtls/mbedtls_wrapper.cpp
+++ b/third_party/mbedtls/mbedtls_wrapper.cpp
@@ -271,6 +271,10 @@ const mbedtls_cipher_info_t *MbedTlsWrapper::AESStateMBEDTLS::GetCipher(size_t k
 	}
 }
 
+void MbedTlsWrapper::AESStateMBEDTLS::SecureClearData(duckdb::data_ptr_t data, duckdb::idx_t len) {
+	mbedtls_platform_zeroize(data, len);
+}
+
 MbedTlsWrapper::AESStateMBEDTLS::AESStateMBEDTLS(duckdb::EncryptionTypes::CipherType cipher_p, duckdb::idx_t key_len) : EncryptionState(cipher_p, key_len), context(duckdb::make_uniq<mbedtls_cipher_context_t>()) {
 	mbedtls_cipher_init(context.get());
 
@@ -296,20 +300,12 @@ MbedTlsWrapper::AESStateMBEDTLS::~AESStateMBEDTLS() {
 	}
 }
 
-void MbedTlsWrapper::AESStateMBEDTLS::GenerateRandomDataStatic(duckdb::data_ptr_t data, duckdb::idx_t len) {
-	duckdb::RandomEngine random_engine;
-
-	while (len) {
-		const auto random_integer = random_engine.NextRandomInteger();
-		const auto next = duckdb::MinValue<duckdb::idx_t>(len, sizeof(random_integer));
-		memcpy(data, duckdb::const_data_ptr_cast(&random_integer), next);
-		data += next;
-		len -= next;
-	}
+static void ThrowInsecureRNG() {
+	throw duckdb::InvalidConfigurationException("DuckDB requires a secure random engine to be loaded to enable secure crypto. Normally, this will be handled automatically by DuckDB by autoloading the `httpfs` Extension, but that seems to have failed. Please ensure the httpfs extension is loaded manually using `LOAD httpfs`.");
 }
 
 void MbedTlsWrapper::AESStateMBEDTLS::GenerateRandomData(duckdb::data_ptr_t data, duckdb::idx_t len) {
-	GenerateRandomDataStatic(data, len);
+	ThrowInsecureRNG();
 }
 
 void MbedTlsWrapper::AESStateMBEDTLS::InitializeInternal(duckdb::const_data_ptr_t iv, duckdb::idx_t iv_len, duckdb::const_data_ptr_t aad, duckdb::idx_t aad_len){
@@ -325,16 +321,7 @@ void MbedTlsWrapper::AESStateMBEDTLS::InitializeInternal(duckdb::const_data_ptr_
 }
 
 void MbedTlsWrapper::AESStateMBEDTLS::InitializeEncryption(duckdb::const_data_ptr_t iv, duckdb::idx_t iv_len, duckdb::const_data_ptr_t key, duckdb::idx_t key_len_p, duckdb::const_data_ptr_t aad, duckdb::idx_t aad_len) {
-	mode = duckdb::EncryptionTypes::ENCRYPT;
-
-	if (key_len_p != key_len) {
-		throw duckdb::InternalException("Invalid encryption key length, expected %llu, got %llu", key_len, key_len_p);
-	}
-	if (mbedtls_cipher_setkey(context.get(), key, key_len * 8, MBEDTLS_ENCRYPT)) {
-		throw runtime_error("Failed to set AES key for encryption");
-	}
-
-	InitializeInternal(iv, iv_len, aad, aad_len);
+	ThrowInsecureRNG();
 }
 
 void MbedTlsWrapper::AESStateMBEDTLS::InitializeDecryption(duckdb::const_data_ptr_t iv, duckdb::idx_t iv_len, duckdb::const_data_ptr_t key, duckdb::idx_t key_len_p, duckdb::const_data_ptr_t aad, duckdb::idx_t aad_len) {
diff --git a/tools/shell/CMakeLists.txt b/tools/shell/CMakeLists.txt
index cba07db715..cadc033168 100644
--- a/tools/shell/CMakeLists.txt
+++ b/tools/shell/CMakeLists.txt
@@ -25,6 +25,7 @@ set(SHELL_SOURCES
     shell_renderer.cpp
     shell_highlight.cpp
     shell_progress_bar.cpp
+    shell_render_table_metadata.cpp
     shell_windows.cpp)
 
 option(STATIC_LIBCPP "Statically link CLI to libc++" FALSE)
diff --git a/tools/shell/include/shell_highlight.hpp b/tools/shell/include/shell_highlight.hpp
index 31f53952b6..355e36501e 100644
--- a/tools/shell/include/shell_highlight.hpp
+++ b/tools/shell/include/shell_highlight.hpp
@@ -22,6 +22,9 @@ enum class HighlightElementType : uint32_t {
 	NUMERIC_CONSTANT,
 	STRING_CONSTANT,
 	LINE_INDICATOR,
+	DATABASE_NAME,
+	SCHEMA_NAME,
+	TABLE_NAME,
 	COLUMN_NAME,
 	COLUMN_TYPE,
 	NUMERIC_VALUE,
@@ -44,6 +47,9 @@ enum class HighlightElementType : uint32_t {
 	SUGGESTION_DIRECTORY_NAME,
 	SUGGESTION_FUNCTION_NAME,
 	SUGGESTION_SETTING_NAME,
+	TABLE_LAYOUT,
+	VIEW_LAYOUT,
+	PRIMARY_KEY_COLUMN,
 	NONE
 };
 
diff --git a/tools/shell/include/shell_state.hpp b/tools/shell/include/shell_state.hpp
index 2bdfadc529..d7c9f4e3c2 100644
--- a/tools/shell/include/shell_state.hpp
+++ b/tools/shell/include/shell_state.hpp
@@ -46,6 +46,7 @@ using duckdb::to_string;
 struct Prompt;
 struct ShellProgressBar;
 struct PagerState;
+struct ShellTableInfo;
 
 using idx_t = uint64_t;
 
@@ -60,6 +61,7 @@ enum class RenderMode : uint32_t {
 	TCL,       /* Generate ANSI-C or TCL quoted elements */
 	CSV,       /* Quote strings, numbers are plain */
 	EXPLAIN,   /* Like RenderMode::Column, but do not truncate data */
+	DESCRIBE,  /* Special DESCRIBE Renderer */
 	ASCII,     /* Use ASCII unit and record separators (0x1F/0x1E) */
 	PRETTY,    /* Pretty-print schemas */
 	EQP,       /* Converts EXPLAIN QUERY PLAN output into a graph */
@@ -128,6 +130,24 @@ struct MetadataCommand {
 	const char *extra_description;
 };
 
+struct ShellColumnInfo {
+	string column_name;
+	string column_type;
+	bool is_primary_key = false;
+	bool is_not_null = false;
+	bool is_unique = false;
+	string default_value;
+};
+
+struct ShellTableInfo {
+	string database_name;
+	string schema_name;
+	string table_name;
+	optional_idx estimated_size;
+	bool is_view = false;
+	vector<ShellColumnInfo> columns;
+};
+
 /*
 ** State information about the database connection is contained in an
 ** instance of the following structure.
@@ -276,11 +296,12 @@ public:
 	bool ReadFromFile(const string &file);
 	bool DisplaySchemas(const vector<string> &args);
 	MetadataResult DisplayEntries(const vector<string> &args, char type);
+	MetadataResult DisplayTables(const vector<string> &args);
 	void ShowConfiguration();
 
-	idx_t RenderLength(const char *z);
-	idx_t RenderLength(const string &str);
-	bool IsCharacter(char c);
+	static idx_t RenderLength(const char *z);
+	static idx_t RenderLength(const string &str);
+	static bool IsCharacter(char c);
 	void SetBinaryMode();
 	void SetTextMode();
 	static idx_t StringLength(const char *z);
@@ -307,6 +328,9 @@ public:
 	vector<string> TableColumnList(const char *zTab);
 	SuccessState ExecuteStatement(unique_ptr<duckdb::SQLStatement> statement);
 	SuccessState RenderDuckBoxResult(duckdb::QueryResult &res);
+	SuccessState RenderDescribe(duckdb::QueryResult &res);
+	static bool UseDescribeRenderMode(const duckdb::SQLStatement &stmt, string &describe_table_name);
+	void RenderTableMetadata(vector<ShellTableInfo> &result);
 
 	void PrintDatabaseError(const string &zErr);
 	int RunInitialCommand(const char *sql, bool bail);
@@ -333,6 +357,8 @@ public:
 	}
 	void ResetOutput();
 	bool ShouldUsePager(duckdb::QueryResult &result);
+	bool ShouldUsePager();
+	bool ShouldUsePager(idx_t line_count);
 	string GetSystemPager();
 	unique_ptr<PagerState> SetupPager();
 	static void StartPagerDisplay();
@@ -400,6 +426,23 @@ public:
 private:
 	ShellState();
 	~ShellState();
+
+private:
+	string describe_table_name;
+};
+
+struct PagerState {
+	explicit PagerState(ShellState &state) : state(state) {
+	}
+	~PagerState() {
+		if (state) {
+			state->ResetOutput();
+			ShellState::FinishPagerDisplay();
+			state = nullptr;
+		}
+	}
+
+	optional_ptr<ShellState> state;
 };
 
 } // namespace duckdb_shell
diff --git a/tools/shell/linenoise/linenoise.cpp b/tools/shell/linenoise/linenoise.cpp
index 074bfa718a..8d6ed8156f 100644
--- a/tools/shell/linenoise/linenoise.cpp
+++ b/tools/shell/linenoise/linenoise.cpp
@@ -131,6 +131,7 @@ bool Linenoise::CompleteLine(KeyPress &next_key) {
 	render_completion_suggestion = false;
 	if (completions.empty()) {
 		Terminal::Beep();
+		next_key.action = KEY_NULL;
 	} else {
 		bool stop = false;
 		bool accept_completion = false;
diff --git a/tools/shell/shell.cpp b/tools/shell/shell.cpp
index 850ab09449..4087a6cab3 100644
--- a/tools/shell/shell.cpp
+++ b/tools/shell/shell.cpp
@@ -742,10 +742,53 @@ int64_t ShellState::StringToInt(const string &arg) {
 }
 
 string ShellState::ModeToString(RenderMode mode) {
-	static const char *modeDescr[] = {"line",     "column", "list",    "semi",  "html",        "insert",    "quote",
-	                                  "tcl",      "csv",    "explain", "ascii", "prettyprint", "eqp",       "json",
-	                                  "markdown", "table",  "box",     "latex", "trash",       "jsonlines", "duckbox"};
-	return modeDescr[int(mode)];
+	switch (mode) {
+	case RenderMode::LINE:
+		return "line";
+	case RenderMode::COLUMN:
+		return "column";
+	case RenderMode::LIST:
+		return "list";
+	case RenderMode::SEMI:
+		return "semi";
+	case RenderMode::HTML:
+		return "html";
+	case RenderMode::INSERT:
+		return "insert";
+	case RenderMode::QUOTE:
+		return "quote";
+	case RenderMode::TCL:
+		return "tcl";
+	case RenderMode::CSV:
+		return "csv";
+	case RenderMode::EXPLAIN:
+		return "explain";
+	case RenderMode::DESCRIBE:
+		return "describe";
+	case RenderMode::ASCII:
+		return "ascii";
+	case RenderMode::PRETTY:
+		return "prettyprint";
+	case RenderMode::EQP:
+		return "eqp";
+	case RenderMode::JSON:
+		return "json";
+	case RenderMode::MARKDOWN:
+		return "markdown";
+	case RenderMode::TABLE:
+		return "table";
+	case RenderMode::BOX:
+		return "box";
+	case RenderMode::LATEX:
+		return "latex";
+	case RenderMode::TRASH:
+		return "trash";
+	case RenderMode::JSONLINES:
+		return "jsonlines";
+	case RenderMode::DUCKBOX:
+		return "duckbox";
+	}
+	return "invalid";
 }
 
 /*
@@ -1420,24 +1463,6 @@ ShellState &ShellState::Get() {
 	return state;
 }
 
-namespace duckdb_shell {
-
-struct PagerState {
-	explicit PagerState(ShellState &state) : state(state) {
-	}
-	~PagerState() {
-		if (state) {
-			state->ResetOutput();
-			ShellState::FinishPagerDisplay();
-			state = nullptr;
-		}
-	}
-
-	optional_ptr<ShellState> state;
-};
-
-} // namespace duckdb_shell
-
 SuccessState ShellState::ExecuteStatement(unique_ptr<duckdb::SQLStatement> statement) {
 	if (!statement->named_param_map.empty()) {
 		PrintDatabaseError("Prepared statement parameters cannot be used directly\nTo use prepared "
@@ -1446,7 +1471,8 @@ SuccessState ShellState::ExecuteStatement(unique_ptr<duckdb::SQLStatement> state
 	}
 	auto &con = *conn;
 	unique_ptr<duckdb::QueryResult> result;
-	if (ShellRenderer::IsColumnar(cMode) && cMode != RenderMode::TRASH && cMode != RenderMode::DUCKBOX) {
+	if (ShellRenderer::IsColumnar(cMode) && cMode != RenderMode::TRASH && cMode != RenderMode::DUCKBOX &&
+	    cMode != RenderMode::DESCRIBE) {
 		// for row-wise rendering we can use streaming results
 		result = con.SendQuery(std::move(statement));
 	} else {
@@ -1492,6 +1518,10 @@ SuccessState ShellState::ExecuteStatement(unique_ptr<duckdb::SQLStatement> state
 		RenderColumnarResult(res);
 		return SuccessState::SUCCESS;
 	}
+	if (cMode == RenderMode::DESCRIBE) {
+		RenderDescribe(res);
+		return SuccessState::SUCCESS;
+	}
 	if (cMode == RenderMode::DUCKBOX) {
 		return RenderDuckBoxResult(res);
 	}
@@ -1536,6 +1566,7 @@ SuccessState ShellState::RenderDuckBoxResult(duckdb::QueryResult &res) {
 		return SuccessState::FAILURE;
 	}
 }
+
 /*
 ** Execute a statement or set of statements.  Print
 ** any result rows/columns depending on the current mode
@@ -1563,6 +1594,9 @@ SuccessState ShellState::ExecuteSQL(const string &zSql) {
 			if (statement->type == duckdb::StatementType::EXPLAIN_STATEMENT) {
 				cMode = RenderMode::EXPLAIN;
 			}
+			if (UseDescribeRenderMode(*statement, describe_table_name)) {
+				cMode = RenderMode::DESCRIBE;
+			}
 
 			auto rc = ExecuteStatement(std::move(statement));
 			if (rc != SuccessState::SUCCESS) {
@@ -1894,7 +1928,7 @@ string ShellState::GetSystemPager() {
 #endif
 }
 
-bool ShellState::ShouldUsePager(duckdb::QueryResult &result) {
+bool ShellState::ShouldUsePager() {
 	if (out != stdout || !stdout_is_console || !outfile.empty()) {
 		// if we have an outfile specified we don't set up the pager
 		return false;
@@ -1911,15 +1945,46 @@ bool ShellState::ShouldUsePager(duckdb::QueryResult &result) {
 			return false;
 		}
 	}
+	return true;
+}
+
+bool ShellState::ShouldUsePager(idx_t line_count) {
+	if (!ShouldUsePager()) {
+		return false;
+	}
+	if (pager_mode == PagerMode::PAGER_AUTOMATIC) {
+		if (line_count < pager_min_rows) {
+			return false;
+		}
+	}
+	return true;
+}
+
+bool ShellState::ShouldUsePager(duckdb::QueryResult &result) {
+	if (!ShouldUsePager()) {
+		return false;
+	}
 	if (pager_mode == PagerMode::PAGER_AUTOMATIC) {
 		// in automatic mode we only use a pager when the output is large enough
-		if (mode == RenderMode::DUCKBOX) {
+		if (cMode == RenderMode::DUCKBOX) {
 			// in duckbox mode the output is automatically truncated to "max_rows"
 			// if "max_rows" is smaller than pager_min_rows in this mode, we never show the pager
 			if (max_rows < pager_min_rows) {
 				return false;
 			}
 		}
+		if (cMode == RenderMode::EXPLAIN) {
+			auto &materialized = result.Cast<MaterializedQueryResult>();
+			idx_t row_count = 0;
+			for (auto &row : materialized.Collection().Rows()) {
+				for (auto c : row.GetValue(1).GetValue<string>()) {
+					if (c == '\n') {
+						row_count++;
+					}
+				}
+			}
+			return row_count >= pager_min_rows;
+		}
 		// otherwise we check the size of the result set
 		// if it has less than X columns, or there are fewer than Y rows, we omit the pager
 		if (result.ColumnCount() < pager_min_columns && !result.MoreRowsThan(pager_min_rows)) {
@@ -2649,6 +2714,113 @@ void ShellState::ShowConfiguration() {
 	PrintF("%12.12s: %s\n", "filename", zDbFilename.c_str());
 }
 
+SuccessState ShellState::RenderDescribe(duckdb::QueryResult &res) {
+	vector<ShellTableInfo> result;
+	ShellTableInfo table;
+	table.table_name = describe_table_name;
+	for (auto &row : res) {
+		ShellColumnInfo column;
+		column.column_name = row.GetValue<string>(0);
+		column.column_type = row.GetValue<string>(1);
+		if (!row.IsNull(2)) {
+			column.is_not_null = row.GetValue<string>(2) == "NO";
+		}
+		if (!row.IsNull(3)) {
+			column.is_primary_key = row.GetValue<string>(3) == "PRI";
+			column.is_unique = row.GetValue<string>(3) == "UNI";
+		}
+		if (!row.IsNull(4)) {
+			column.default_value = row.GetValue<string>(4);
+		}
+		table.columns.push_back(std::move(column));
+	}
+	result.push_back(std::move(table));
+	RenderTableMetadata(result);
+	return SuccessState::SUCCESS;
+}
+
+MetadataResult ShellState::DisplayTables(const vector<string> &args) {
+	if (args.size() > 2) {
+		return MetadataResult::PRINT_USAGE;
+	}
+	// FIXME: copy pasted from below
+	// Parse the filter pattern to check for schema qualification
+	string filter_pattern = args.size() > 1 ? args[1] : string();
+	string schema_filter = "";
+	string table_filter = "%" + filter_pattern + "%";
+
+	// Parse the filter pattern to check for schema qualification
+	try {
+		auto components = duckdb::QualifiedName::ParseComponents(filter_pattern);
+		if (components.size() >= 2) {
+			// e.g : "schema.table" or "schema.%"
+			schema_filter = "%" + components[0] + "%";
+			table_filter = "%" + components[1] + "%";
+		}
+	} catch (const duckdb::ParserException &) {
+		// If parsing fails, treat as a simple table pattern
+	}
+	string schema_filter_str;
+	string name_filter;
+	if (!table_filter.empty()) {
+		name_filter = StringUtil::Format(" AND columns.table_name ILIKE %s", SQLString(table_filter));
+	}
+	if (!schema_filter.empty()) {
+		schema_filter_str = StringUtil::Format(" AND columns.schema_name ILIKE %s", SQLString(schema_filter));
+	}
+	auto query = StringUtil::Format(R"(
+SELECT columns.database_name, columns.schema_name, columns.table_name, list(
+	struct_pack(column_name, data_type, is_primary_key := c.column_index IS NOT NULL) order by column_index),
+	t.estimated_size AS estimated_size, t.table_oid AS table_oid
+FROM duckdb_columns() columns
+LEFT JOIN duckdb_tables() t USING (table_oid)
+LEFT JOIN (
+	SELECT table_oid, UNNEST(constraint_column_indexes)+1 column_index
+	FROM duckdb_constraints()
+	WHERE constraint_type='PRIMARY KEY') c
+USING (table_oid, column_index)
+WHERE NOT columns.internal%s%s
+GROUP BY ALL;
+)",
+	                                schema_filter_str, name_filter);
+
+	auto &con = *conn;
+	auto query_result = con.Query(query);
+	if (query_result->HasError()) {
+		PrintDatabaseError(query_result->GetError());
+		return MetadataResult::FAIL;
+	}
+	vector<ShellTableInfo> result;
+	for (auto &row : *query_result) {
+		ShellTableInfo table;
+		table.database_name = row.GetValue<string>(0);
+		table.schema_name = row.GetValue<string>(1);
+		table.table_name = row.GetValue<string>(2);
+
+		auto column_val = row.GetBaseValue(3);
+		for (auto &column_entry : duckdb::ListValue::GetChildren(column_val)) {
+			ShellColumnInfo column;
+			auto &struct_children = duckdb::StructValue::GetChildren(column_entry);
+			column.column_name = struct_children[0].GetValue<string>();
+			column.column_type = struct_children[1].GetValue<string>();
+			column.is_primary_key = struct_children[2].GetValue<bool>();
+			table.columns.push_back(std::move(column));
+		}
+
+		if (!row.IsNull(4)) {
+			table.estimated_size = row.GetValue<idx_t>(4);
+		}
+		if (row.IsNull(5)) {
+			// view
+			table.is_view = true;
+		}
+
+		result.push_back(std::move(table));
+	}
+	RenderTableMetadata(result);
+	return MetadataResult::SUCCESS;
+}
+
 MetadataResult ShellState::DisplayEntries(const vector<string> &args, char type) {
 	string s;
 
diff --git a/tools/shell/shell_highlight.cpp b/tools/shell/shell_highlight.cpp
index 2307f85a29..d6bd7aaee6 100644
--- a/tools/shell/shell_highlight.cpp
+++ b/tools/shell/shell_highlight.cpp
@@ -14,8 +14,11 @@ static HighlightElement highlight_elements[] = {
     {"numeric_constant", PrintColor::YELLOW, PrintIntensity::STANDARD},
     {"string_constant", PrintColor::YELLOW, PrintIntensity::STANDARD},
     {"line_indicator", PrintColor::STANDARD, PrintIntensity::BOLD},
+    {"database_name", PrintColor::ORANGE3, PrintIntensity::STANDARD},
+    {"schema_name", PrintColor::DEEPSKYBLUE1, PrintIntensity::STANDARD},
+    {"table_name", PrintColor::STANDARD, PrintIntensity::BOLD},
     {"column_name", PrintColor::STANDARD, PrintIntensity::STANDARD},
-    {"column_type", PrintColor::STANDARD, PrintIntensity::STANDARD},
+    {"column_type", PrintColor::GRAY, PrintIntensity::STANDARD},
     {"numeric_value", PrintColor::STANDARD, PrintIntensity::STANDARD},
     {"string_value", PrintColor::STANDARD, PrintIntensity::STANDARD},
     {"temporal_value", PrintColor::STANDARD, PrintIntensity::STANDARD},
@@ -36,6 +39,9 @@ static HighlightElement highlight_elements[] = {
     {"suggestion_directory_name", PrintColor::DARKORANGE3, PrintIntensity::STANDARD},
     {"suggestion_function_name", PrintColor::DARKORANGE3, PrintIntensity::STANDARD},
     {"suggestion_setting_name", PrintColor::STANDARD, PrintIntensity::STANDARD},
+    {"table_layout", PrintColor::GRAY, PrintIntensity::STANDARD},
+    {"view_layout", PrintColor::STANDARD, PrintIntensity::STANDARD},
+    {"primary_key_column", PrintColor::STANDARD, PrintIntensity::UNDERLINE},
     {"none", PrintColor::STANDARD, PrintIntensity::STANDARD},
     {nullptr, PrintColor::STANDARD, PrintIntensity::STANDARD}};
 
diff --git a/tools/shell/shell_metadata_command.cpp b/tools/shell/shell_metadata_command.cpp
index 3491c1c28a..59b9374905 100644
--- a/tools/shell/shell_metadata_command.cpp
+++ b/tools/shell/shell_metadata_command.cpp
@@ -444,7 +444,7 @@ MetadataResult ShowIndexes(ShellState &state, const vector<string> &args) {
 }
 
 MetadataResult ShowTables(ShellState &state, const vector<string> &args) {
-	return state.DisplayEntries(args, 't');
+	return state.DisplayTables(args);
 }
 
 MetadataResult SetUICommand(ShellState &state, const vector<string> &args) {
diff --git a/tools/shell/shell_render_table_metadata.cpp b/tools/shell/shell_render_table_metadata.cpp
new file mode 100644
index 0000000000..06fac0eeb4
--- /dev/null
+++ b/tools/shell/shell_render_table_metadata.cpp
@@ -0,0 +1,629 @@
+#include "duckdb.hpp"
+#include "shell_state.hpp"
+#include "shell_highlight.hpp"
+#include "duckdb/parser/query_node/select_node.hpp"
+#include "duckdb/parser/tableref/showref.hpp"
+#include "duckdb/parser/tableref/basetableref.hpp"
+#include "duckdb/common/box_renderer.hpp"
+#include "duckdb/common/map.hpp"
+
+namespace duckdb_shell {
+
+bool ShellState::UseDescribeRenderMode(const duckdb::SQLStatement &statement, string &describe_table_name) {
+	// identify if this is a DESCRIBE {table} or DESCRIBE {query} statement
+	// if it is we use the special DESCRIBE render mode
+	if (statement.type != duckdb::StatementType::SELECT_STATEMENT) {
+		return false;
+	}
+	auto &select = statement.Cast<duckdb::SelectStatement>();
+	if (select.node->type != duckdb::QueryNodeType::SELECT_NODE) {
+		return false;
+	}
+	auto &select_node = select.node->Cast<duckdb::SelectNode>();
+	if (select_node.select_list.size() != 1 || select_node.select_list[0]->type != duckdb::ExpressionType::STAR) {
+		return false;
+	}
+	if (select_node.from_table->type != duckdb::TableReferenceType::SHOW_REF) {
+		return false;
+	}
+	auto &showref = select_node.from_table->Cast<duckdb::ShowRef>();
+	if (showref.show_type == duckdb::ShowType::SUMMARY) {
+		return false;
+	}
+	if (showref.table_name == "\"databases\"" || showref.table_name == "\"tables\"" ||
+	    showref.table_name == "\"variables\"" || showref.table_name == "__show_tables_expanded") {
+		// ignore special cases in ShowRef
+		// TODO: this is ugly, should just be using the ShowType enum...
+		return false;
+	}
+	describe_table_name = "Describe";
+	if (!showref.table_name.empty()) {
+		describe_table_name = showref.table_name;
+	} else if (showref.query && showref.query->type == duckdb::QueryNodeType::SELECT_NODE) {
+		auto &show_select = showref.query->Cast<duckdb::SelectNode>();
+		if (show_select.from_table->type == duckdb::TableReferenceType::BASE_TABLE) {
+			auto &base_table = show_select.from_table->Cast<duckdb::BaseTableRef>();
+			describe_table_name = base_table.table_name;
+		}
+	}
+	return true;
+}
+
+struct RenderComponent {
+	RenderComponent(string text_p, HighlightElementType type) : text(std::move(text_p)), type(type) {
+		render_width = ShellState::RenderLength(text);
+	}
+
+	string text;
+	idx_t render_width;
+	HighlightElementType type;
+};
+
+struct ShellColumnRenderInfo {
+	vector<RenderComponent> components;
+};
+
+struct ColumnRenderRow {
+	vector<ShellColumnRenderInfo> columns;
+};
+
+struct ShellTableRenderInfo {
+	ShellTableRenderInfo(ShellTableInfo table, char decimal_sep);
+
+	ShellTableInfo table;
+	idx_t table_name_length;
+	idx_t render_width;
+	vector<idx_t> max_component_widths;
+	string estimated_size_text;
+	optional_idx estimated_size_length;
+	vector<ColumnRenderRow> column_renders;
+
+	idx_t LineCount() const {
+		idx_t constant_count = 4;
+		if (estimated_size_length.IsValid()) {
+			constant_count += 2;
+		}
+		return ColumnLines() + constant_count;
+	}
+	idx_t PerColumnWidth() const {
+		idx_t width = 4;
+		for (auto &component_width : max_component_widths) {
+			width += component_width;
+		}
+		width += max_component_widths.size() - 1;
+		return width;
+	}
+
+	idx_t ColumnLines() const {
+		return column_renders[0].columns.size();
+	}
+
+	void Truncate(idx_t max_render_width);
+	static void TruncateValueIfRequired(string &value, idx_t &render_length, idx_t max_render_width);
+};
+
+struct TableMetadataLine {
+	idx_t render_height = 0;
+	idx_t render_width = 0;
+	vector<idx_t> max_component_widths;
+	vector<idx_t> tables;
+
+	idx_t PerColumnWidth() const {
+		idx_t width = 4;
+		for (auto &component_width : max_component_widths) {
+			width += component_width;
+		}
+		width += max_component_widths.size() - 1;
+		return width;
+	}
+	void RenderLine(ShellHighlight &highlight, const vector<ShellTableRenderInfo> &tables, idx_t line_idx,
+	                bool last_line);
+};
+
+struct TableMetadataDisplayInfo {
+	string database_name;
+	string schema_name;
+	idx_t render_height = 0;
+	idx_t render_width = 0;
+	vector<TableMetadataLine> display_lines;
+};
+
+void TableMetadataLine::RenderLine(ShellHighlight &highlight, const vector<ShellTableRenderInfo> &table_list,
+                                   idx_t line_idx, bool last_line) {
+	// figure out the table to render
+	idx_t table_idx = 0;
+	for (; table_idx < tables.size(); table_idx++) {
+		idx_t line_count = table_list[tables[table_idx]].LineCount();
+		if (line_idx < line_count) {
+			// line belongs to this table - render it
+			break;
+		}
+		// line belongs to a subsequent table - subtract line count and move to next table
+		line_idx -= line_count;
+	}
+	if (table_idx == tables.size()) {
+		// we didn't render any table - break
+		if (!last_line) {
+			// we need to add spaces if this is not the last line
+			highlight.PrintText(string(render_width, ' '), PrintOutput::STDOUT, HighlightElementType::LAYOUT);
+		}
+		return;
+	}
+	auto &render_table = table_list[tables[table_idx]];
+	auto &table = render_table.table;
+	auto layout_type = table.is_view ? HighlightElementType::VIEW_LAYOUT : HighlightElementType::TABLE_LAYOUT;
+	duckdb::BoxRendererConfig config;
+	if (line_idx == 0) {
+		// first line - render layout
+		string top_line;
+		top_line = config.LTCORNER;
+		for (idx_t i = 0; i < render_width - 2; i++) {
+			top_line += config.HORIZONTAL;
+		}
+		top_line += config.RTCORNER;
+		highlight.PrintText(top_line, PrintOutput::STDOUT, layout_type);
+		return;
+	}
+	if (line_idx == 1) {
+		// table name
+		string table_name;
+		idx_t space_count = render_width - render_table.table_name_length - 2;
+		idx_t lspace = space_count / 2;
+		idx_t rspace = space_count - lspace;
+		string table_line = config.VERTICAL;
+		table_line += string(lspace, ' ');
+		highlight.PrintText(table_line, PrintOutput::STDOUT, layout_type);
+		highlight.PrintText(table.table_name, PrintOutput::STDOUT, HighlightElementType::TABLE_NAME);
+		table_line = string(rspace, ' ');
+		table_line += config.VERTICAL;
+		highlight.PrintText(table_line, PrintOutput::STDOUT, layout_type);
+		return;
+	}
+	if (line_idx > 2 && line_idx < render_table.ColumnLines() + 3) {
+		idx_t column_idx = line_idx - 3;
+		// column line
+
+		// if we have a long table name the table name might have stretched out the box
+		// in that case we need to add padding here
+		// compute the required padding
+		idx_t total_render_width = render_width;
+		idx_t per_column_render_width = PerColumnWidth();
+		idx_t column_render_width = render_table.column_renders.size() * per_column_render_width;
+		idx_t extra_render_width = total_render_width - column_render_width;
+		idx_t render_width_per_column = extra_render_width / render_table.column_renders.size();
+
+		for (idx_t render_idx = 0; render_idx < render_table.column_renders.size(); render_idx++) {
+			auto &column_render = render_table.column_renders[render_idx];
+			string column_line = render_idx == 0 ? config.VERTICAL : " ";
+			bool is_last = render_idx + 1 == render_table.column_renders.size();
+			if (column_idx < column_render.columns.size()) {
+				auto &col = column_render.columns[column_idx];
+				column_line += " ";
+				highlight.PrintText(column_line, PrintOutput::STDOUT, layout_type);
+				// render each of the components
+				for (idx_t component_idx = 0; component_idx < col.components.size(); component_idx++) {
+					auto &component = col.components[component_idx];
+					highlight.PrintText(component.text, PrintOutput::STDOUT, component.type);
+
+					// render space padding between components
+					column_line = string(max_component_widths[component_idx] - component.render_width + 1, ' ');
+					if (extra_render_width > 0) {
+						// if we need extra spacing add it
+						idx_t render_count = is_last ? extra_render_width : render_width_per_column;
+						column_line += string(render_count, ' ');
+						extra_render_width -= render_count;
+					}
+					highlight.PrintText(column_line, PrintOutput::STDOUT, layout_type);
+				}
+			} else {
+				// we have already rendered all columns for this line - pad with spaces
+				column_line = string(per_column_render_width - 1, ' ');
+				highlight.PrintText(column_line, PrintOutput::STDOUT, layout_type);
+			}
+			column_line = is_last ? config.VERTICAL : " ";
+			highlight.PrintText(column_line, PrintOutput::STDOUT, layout_type);
+		}
+		return;
+	}
+	if (line_idx == 2 || (table.estimated_size.IsValid() && line_idx == render_table.ColumnLines() + 3)) {
+		// blank line after table name
+		string blank_line = config.VERTICAL;
+		blank_line += string(render_width - 2, ' ');
+		blank_line += config.VERTICAL;
+		highlight.PrintText(blank_line, PrintOutput::STDOUT, layout_type);
+		return;
+	}
+	if (table.estimated_size.IsValid() && line_idx == render_table.ColumnLines() + 4) {
+		idx_t space_count = render_width - render_table.estimated_size_length.GetIndex() - 2;
+		idx_t lspace = space_count / 2;
+		idx_t rspace = space_count - lspace;
+		string estimated_size_line = config.VERTICAL;
+		estimated_size_line += string(lspace, ' ');
+		highlight.PrintText(estimated_size_line, PrintOutput::STDOUT, layout_type);
+		highlight.PrintText(render_table.estimated_size_text, PrintOutput::STDOUT, HighlightElementType::COMMENT);
+		estimated_size_line = string(rspace, ' ');
+		estimated_size_line += config.VERTICAL;
+		highlight.PrintText(estimated_size_line, PrintOutput::STDOUT, layout_type);
+		return;
+	}
+	// bottom line
+	string bottom_line;
+	bottom_line = config.LDCORNER;
+	for (idx_t i = 0; i < render_width - 2; i++) {
+		bottom_line += config.HORIZONTAL;
+	}
+	bottom_line += config.RDCORNER;
+	highlight.PrintText(bottom_line, PrintOutput::STDOUT, layout_type);
+}
+
+string FormatTableMetadataType(const string &type) {
+	auto result = StringUtil::Lower(type);
+	if (StringUtil::StartsWith(result, "decimal")) {
+		return "decimal";
+	}
+	return result;
+}
+
+ShellTableRenderInfo::ShellTableRenderInfo(ShellTableInfo table_p, char decimal_sep) : table(std::move(table_p)) {
+	table_name_length = ShellState::RenderLength(table.table_name);
+	ColumnRenderRow render_row;
+	// figure out if we need an extra line to render constraint info
+	bool has_constraint_component = false;
+	for (auto &col : table.columns) {
+		if (col.is_not_null || col.is_unique || !col.default_value.empty()) {
+			has_constraint_component = true;
+			break;
+		}
+	}
+
+	idx_t component_count = 2 + has_constraint_component;
+	for (auto &col_p : table.columns) {
+		ShellColumnRenderInfo col_display;
+		HighlightElementType column_name_type =
+		    col_p.is_primary_key ? HighlightElementType::PRIMARY_KEY_COLUMN : HighlightElementType::COLUMN_NAME;
+		col_display.components.emplace_back(std::move(col_p.column_name), column_name_type);
+		col_display.components.emplace_back(FormatTableMetadataType(col_p.column_type),
+		                                    HighlightElementType::COLUMN_TYPE);
+		if (has_constraint_component) {
+			string constraint_text;
+			if (col_p.is_not_null) {
+				constraint_text = "not null";
+			}
+			if (col_p.is_unique) {
+				if (!constraint_text.empty()) {
+					constraint_text += " ";
+				}
+				constraint_text += "unique";
+			}
+			if (!col_p.default_value.empty()) {
+				if (!constraint_text.empty()) {
+					constraint_text += " ";
+				}
+				constraint_text += "default " + col_p.default_value;
+			}
+
+			col_display.components.emplace_back(constraint_text, HighlightElementType::COLUMN_TYPE);
+		}
+		render_row.columns.push_back(std::move(col_display));
+	}
+	table.columns.clear();
+
+	// iterate over the components to find the max length
+	max_component_widths.resize(component_count);
+	for (auto &row : render_row.columns) {
+		if (row.components.size() != component_count) {
+			throw InternalException("Component count is misaligned");
+		}
+		for (idx_t component_idx = 0; component_idx < row.components.size(); component_idx++) {
+			max_component_widths[component_idx] = duckdb::MaxValue<idx_t>(max_component_widths[component_idx],
+			                                                              row.components[component_idx].render_width);
+		}
+	}
+	column_renders.push_back(std::move(render_row));
+
+	render_width = table_name_length + 4;
+	if (PerColumnWidth() > render_width) {
+		render_width = PerColumnWidth();
+	}
+	if (table.estimated_size.IsValid()) {
+		estimated_size_text = to_string(table.estimated_size.GetIndex());
+		auto formatted = duckdb::BoxRenderer::TryFormatLargeNumber(estimated_size_text, decimal_sep);
+		if (!formatted.empty()) {
+			estimated_size_text = std::move(formatted);
+		}
+		estimated_size_text += " rows";
+		estimated_size_length = ShellState::RenderLength(estimated_size_text);
+		if (estimated_size_length.GetIndex() + 4 > render_width) {
+			render_width = estimated_size_length.GetIndex() + 4;
+		}
+	}
+}
+
+void ShellTableRenderInfo::TruncateValueIfRequired(string &value, idx_t &render_length, idx_t max_render_width) {
+	if (render_length <= max_render_width) {
+		return;
+	}
+	duckdb::BoxRendererConfig config;
+	idx_t pos = 0;
+	idx_t value_render_width = 0;
+	value =
+	    duckdb::BoxRenderer::TruncateValue(value, max_render_width - config.DOTDOTDOT_LENGTH, pos, value_render_width) +
+	    config.DOTDOTDOT;
+	render_length = value_render_width + config.DOTDOTDOT_LENGTH;
+}
+
+void ShellTableRenderInfo::Truncate(idx_t max_render_width) {
+	if (render_width <= max_render_width) {
+		return;
+	}
+
+	// we exceeded the render width - we need to truncate
+	TruncateValueIfRequired(table.table_name, table_name_length, max_render_width - 4);
+	// figure out what we need to truncate
+	idx_t total_column_length = PerColumnWidth();
+	if (total_column_length > max_render_width) {
+		// we need to truncate either column names or column types
+		// prefer to keep the name as long as possible - only truncate it if it by itself almost exceeds the
+		// width
+		static constexpr const idx_t MIN_COMPONENT_SIZE = 5;
+		idx_t component_count = max_component_widths.size();
+		idx_t min_leftover_size = component_count * MIN_COMPONENT_SIZE;
+		if (max_component_widths[0] + min_leftover_size > max_render_width) {
+			max_component_widths[0] = max_render_width - min_leftover_size;
+		}
+		total_column_length = PerColumnWidth();
+		if (total_column_length > max_render_width) {
+			// truncate other components if required
+			for (idx_t i = 1; i < max_component_widths.size(); i++) {
+				if (max_component_widths[i] <= MIN_COMPONENT_SIZE) {
+					// cannot truncate below the min component size
+					continue;
+				}
+				idx_t truncate_amount = duckdb::MinValue<idx_t>(total_column_length - max_render_width,
+				                                                max_component_widths[i] - MIN_COMPONENT_SIZE);
+				max_component_widths[i] -= truncate_amount;
+				total_column_length -= truncate_amount;
+				if (total_column_length <= render_width) {
+					break;
+				}
+			}
+		}
+		// truncate all column components that we need to truncate
+		for (auto &column_render : column_renders) {
+			for (auto &col : column_render.columns) {
+				for (idx_t component_idx = 0; component_idx < col.components.size(); component_idx++) {
+					auto &component = col.components[component_idx];
+					TruncateValueIfRequired(component.text, component.render_width,
+					                        max_component_widths[component_idx]);
+				}
+			}
+		}
+	}
+	render_width = max_render_width;
+}
+
+void RenderLineDisplay(ShellHighlight &highlight, string &text, idx_t total_render_width,
+                       HighlightElementType element_type) {
+	auto render_size = ShellState::RenderLength(text);
+	ShellTableRenderInfo::TruncateValueIfRequired(text, render_size, total_render_width - 4);
+
+	duckdb::BoxRendererConfig config;
+	idx_t total_lines = total_render_width - render_size - 4;
+	idx_t lline = total_lines / 2;
+	idx_t rline = total_lines - lline;
+
+	string middle_line;
+	middle_line += " ";
+	for (idx_t i = 0; i < lline; i++) {
+		middle_line += config.HORIZONTAL;
+	}
+	middle_line += " ";
+	middle_line += text;
+	middle_line += " ";
+	for (idx_t i = 0; i < rline; i++) {
+		middle_line += config.HORIZONTAL;
+	}
+	middle_line += " ";
+	middle_line += "\n";
+	highlight.PrintText(middle_line, PrintOutput::STDOUT, element_type);
+}
+
+void ShellState::RenderTableMetadata(vector<ShellTableInfo> &tables) {
+	idx_t max_render_width = max_width == 0 ? duckdb::Printer::TerminalWidth() : max_width;
+	if (max_render_width < 80) {
+		max_render_width = 80;
+	}
+	duckdb::BoxRendererConfig config;
+	// figure out the render width of each table
+	vector<ShellTableRenderInfo> table_list;
+	for (auto &table : tables) {
+		table_list.emplace_back(table, config.decimal_separator);
+	}
+	for (auto &table : table_list) {
+		// optionally truncate each table if we exceed the max render width
+		table.Truncate(max_render_width);
+	}
+	// try to split up large tables
+	for (auto &table : table_list) {
+		static constexpr const idx_t SPLIT_THRESHOLD = 20;
+		D_ASSERT(table.column_renders.size() == 1);
+		if (table.column_renders[0].columns.size() <= SPLIT_THRESHOLD) {
+			// not that many columns - keep it as-is
+			continue;
+		}
+		// try to split up columns if possible
+		idx_t max_split_count = (table.column_renders[0].columns.size() + SPLIT_THRESHOLD - 1) / SPLIT_THRESHOLD;
+		idx_t width_per_split = table.PerColumnWidth();
+		idx_t max_splits = max_render_width / width_per_split;
+		D_ASSERT(max_split_count > 1);
+		if (max_splits <= 1) {
+			// too wide - cannot split
+			continue;
+		}
+		idx_t split_count = duckdb::MinValue<idx_t>(max_split_count, max_splits);
+
+		vector<ColumnRenderRow> new_renders;
+		new_renders.resize(split_count);
+		idx_t split_idx = 0;
+		for (auto &col : table.column_renders[0].columns) {
+			new_renders[split_idx % split_count].columns.push_back(std::move(col));
+			split_idx++;
+		}
+		table.column_renders = std::move(new_renders);
+		table.render_width = duckdb::MaxValue<idx_t>(table.render_width, split_count * width_per_split);
+	}
+
+	// group tables by db + schema
+	duckdb::map<string, duckdb::map<string, vector<ShellTableRenderInfo>>> grouped_tables;
+	for (auto &entry : table_list) {
+		grouped_tables[entry.table.database_name][entry.table.schema_name].push_back(std::move(entry));
+	}
+
+	vector<TableMetadataDisplayInfo> metadata_displays;
+	// for each set of table groups - make a list of displays
+	for (auto &db_entry : grouped_tables) {
+		for (auto &schema_entry : db_entry.second) {
+			auto &result = schema_entry.second;
+			// sort from biggest to smallest table
+			std::sort(result.begin(), result.end(), [](const ShellTableRenderInfo &a, const ShellTableRenderInfo &b) {
+				return a.LineCount() > b.LineCount();
+			});
+
+			// try to colocate different tables on the same lines in a greedy manner
+			duckdb::unordered_set<idx_t> displayed_tables;
+			for (idx_t table_idx = 0; table_idx < result.size(); table_idx++) {
+				if (displayed_tables.find(table_idx) != displayed_tables.end()) {
+					// already displayed
+					continue;
+				}
+				displayed_tables.insert(table_idx);
+				auto &initial_table = result[table_idx];
+				TableMetadataDisplayInfo metadata_display;
+				// the first line always has only one table (this table)
+				metadata_display.database_name = initial_table.table.database_name;
+				metadata_display.schema_name = initial_table.table.schema_name;
+				TableMetadataLine initial_line;
+				initial_line.tables.push_back(table_idx);
+				metadata_display.render_width = initial_line.render_width = initial_table.render_width;
+				metadata_display.render_height = initial_line.render_height = initial_table.LineCount();
+				initial_line.max_component_widths = initial_table.max_component_widths;
+				metadata_display.display_lines.push_back(std::move(initial_line));
+
+				// now for each table, check if we can co-locate it
+				for (idx_t next_idx = table_idx + 1; next_idx < result.size(); next_idx++) {
+					auto &current_table = result[next_idx];
+					auto render_width = current_table.render_width;
+					auto render_height = current_table.LineCount();
+					// we have two choices with co-locating
+					// we can EITHER add a new line
+					// OR add it to an existing line
+					// we prefer to add it to an existing line if possible
+					if (render_height > metadata_display.render_height) {
+						// if this table is bigger than the current render height we can never add it - so just skip it
+						continue;
+					}
+					bool added = false;
+					for (auto &existing_line : metadata_display.display_lines) {
+						if (existing_line.render_height + render_height > metadata_display.render_height) {
+							// does not fit!
+							continue;
+						}
+						// adding the table here works out height wise - we could potentially add it here
+						// however, we do need to "stretch" the line to fit the current unit
+						// get the maximum render width
+						vector<idx_t> new_max_component_widths;
+						for (idx_t component_idx = 0; component_idx < existing_line.max_component_widths.size();
+						     component_idx++) {
+							new_max_component_widths.push_back(
+							    duckdb::MaxValue<idx_t>(existing_line.max_component_widths[component_idx],
+							                            current_table.max_component_widths[component_idx]));
+						}
+						idx_t new_column_render_width = 3;
+						for (auto &component_width : new_max_component_widths) {
+							new_column_render_width += component_width + 1;
+						}
+						idx_t new_rendering_width = duckdb::MaxValue<idx_t>(render_width, existing_line.render_width);
+						new_rendering_width = duckdb::MaxValue<idx_t>(new_rendering_width, new_column_render_width);
+
+						D_ASSERT(new_rendering_width >= existing_line.render_width);
+						idx_t extra_width = new_rendering_width - existing_line.render_width;
+
+						if (metadata_display.render_width + extra_width > max_render_width) {
+							// the extra width makes us exceed the rendering width limit - we cannot add it here
+							continue;
+						}
+						// we can add it here! extend the line and add the table
+						existing_line.max_component_widths = std::move(new_max_component_widths);
+						existing_line.render_width += extra_width;
+						existing_line.render_height += render_height;
+						existing_line.tables.push_back(next_idx);
+						added = true;
+						break;
+					}
+					if (!added) {
+						// if we couldn't add it to an existing line we might still be able to add a new line
+						// but only if that fits width wise
+						if (metadata_display.render_width + render_width <= max_render_width) {
+							// it does! add an extra line
+							TableMetadataLine new_line;
+							new_line.tables.push_back(next_idx);
+							new_line.render_width = render_width;
+							new_line.render_height = render_height;
+							new_line.max_component_widths = current_table.max_component_widths;
+							metadata_display.render_width += render_width;
+							metadata_display.display_lines.push_back(std::move(new_line));
+							added = true;
+						}
+					}
+					if (added) {
+						// we added this table for rendering - add to the displayed tables list
+						displayed_tables.insert(next_idx);
+					}
+				}
+				std::sort(metadata_display.display_lines.begin(), metadata_display.display_lines.end(),
+				          [](TableMetadataLine &a, TableMetadataLine &b) { return a.render_height > b.render_height; });
+				metadata_displays.push_back(std::move(metadata_display));
+			}
+		}
+	}
+
+	idx_t line_count = 0;
+	for (auto &display_line : metadata_displays) {
+		line_count += display_line.render_height;
+	}
+	unique_ptr<PagerState> pager_setup;
+	if (ShouldUsePager(line_count)) {
+		// we should use a pager
+		pager_setup = SetupPager();
+	}
+	// render the metadata
+	ShellHighlight highlight(*this);
+	string last_displayed_database;
+	string last_displayed_schema;
+	for (auto &metadata_display : metadata_displays) {
+		// check if we should render the database and/or schema name for this batch of tables
+		if (!metadata_display.database_name.empty() && last_displayed_database != metadata_display.database_name) {
+			RenderLineDisplay(highlight, metadata_display.database_name, metadata_display.render_width,
+			                  HighlightElementType::DATABASE_NAME);
+			last_displayed_database = metadata_display.database_name;
+			last_displayed_schema = string();
+		}
+		if (!metadata_display.schema_name.empty() && last_displayed_schema != metadata_display.schema_name) {
+			RenderLineDisplay(highlight, metadata_display.schema_name, metadata_display.render_width,
+			                  HighlightElementType::SCHEMA_NAME);
+			last_displayed_schema = metadata_display.schema_name;
+		}
+		for (idx_t line_idx = 0; line_idx < metadata_display.render_height; line_idx++) {
+			// construct the line
+			for (idx_t table_line_idx = 0; table_line_idx < metadata_display.display_lines.size(); table_line_idx++) {
+				bool is_last = table_line_idx + 1 == metadata_display.display_lines.size();
+				auto &group_table_list = grouped_tables[metadata_display.database_name][metadata_display.schema_name];
+				metadata_display.display_lines[table_line_idx].RenderLine(highlight, group_table_list, line_idx,
+				                                                          is_last);
+			}
+			highlight.PrintText("\n", PrintOutput::STDOUT, HighlightElementType::LAYOUT);
+		}
+	}
+}
+
+} // namespace duckdb_shell
diff --git a/tools/shell/tests/test_read_from_stdin.py b/tools/shell/tests/test_read_from_stdin.py
index 2cd570a446..350c8cf348 100644
--- a/tools/shell/tests/test_read_from_stdin.py
+++ b/tools/shell/tests/test_read_from_stdin.py
@@ -10,6 +10,22 @@ import os
 
 @pytest.mark.skipif(os.name == 'nt', reason="Skipped on windows")
 class TestReadFromStdin(object):
+
+    def test_read_stdin_direct(self, shell):
+        test = (
+            ShellTest(shell)
+            .input_file('data/csv/test/test.csv')
+            .statement("""create table mytable as select * from read_text('/dev/stdin')""")
+            .statement("select length(content) as len from mytable;")
+            .add_argument(
+                '-csv',
+                ':memory:'
+            )
+        )
+        result = test.run()
+        result.check_stdout("len")
+        result.check_stdout('77780')
+
     def test_read_stdin_csv(self, shell):
         test = (
             ShellTest(shell)
diff --git a/tools/shell/tests/test_shell_basics.py b/tools/shell/tests/test_shell_basics.py
index df17fa0a14..2df39a910c 100644
--- a/tools/shell/tests/test_shell_basics.py
+++ b/tools/shell/tests/test_shell_basics.py
@@ -414,7 +414,9 @@ def test_tables(shell):
         .statement(".tables")
     )
     result = test.run()
-    result.check_stdout("asda  bsdf  csda")
+    result.check_stdout("asda")
+    result.check_stdout("bsdf")
+    result.check_stdout("csda")
 
 def test_tables_pattern(shell):
     test = (
@@ -425,7 +427,8 @@ def test_tables_pattern(shell):
         .statement(".tables %da")
     )
     result = test.run()
-    result.check_stdout("asda  csda")
+    result.check_stdout("asda")
+    result.check_stdout("csda")
 
 def test_tables_schema_disambiguation(shell):
     test = (
@@ -437,7 +440,7 @@ def test_tables_schema_disambiguation(shell):
         .statement(".tables")
     )
     result = test.run()
-    result.check_stdout("a.foobar  b.foobar")
+    result.check_stdout("foobar")
 
 def test_tables_schema_filtering(shell):
     test = (
@@ -451,7 +454,8 @@ def test_tables_schema_filtering(shell):
         .statement(".tables a.%")
     )
     result = test.run()
-    result.check_stdout("foobar        unique_table")
+    result.check_stdout("foobar")
+    result.check_stdout("unique_table")
 
 def test_tables_backward_compatibility(shell):
     test = (
@@ -461,7 +465,8 @@ def test_tables_backward_compatibility(shell):
         .statement(".tables")
     )
     result = test.run()
-    result.check_stdout("main_table    unique_table")
+    result.check_stdout("main_table")
+    result.check_stdout("unique_table")
 
 def test_tables_with_views(shell):
     test = (
@@ -475,7 +480,10 @@ def test_tables_with_views(shell):
         .statement(".tables")
     )
     result = test.run()
-    result.check_stdout("a.foobar     a.test_view  b.foobar     b.test_view")
+    result.check_stdout("foobar")
+    result.check_stdout("test_view")
+    result.check_stdout("foobar")
+    result.check_stdout("test_view")
 
 def test_indexes(shell):
     test = (
diff --git a/tools/shell/tests/test_table_metadata_rendering.py b/tools/shell/tests/test_table_metadata_rendering.py
new file mode 100644
index 0000000000..82e388285b
--- /dev/null
+++ b/tools/shell/tests/test_table_metadata_rendering.py
@@ -0,0 +1,102 @@
+# fmt: off
+
+import pytest
+import subprocess
+import sys
+from typing import List
+from conftest import ShellTest
+import os
+from pathlib import Path
+
+
+def test_many_tables(shell):
+    test = (
+        ShellTest(shell)
+        .statement('''
+CREATE TABLE customer(c_custkey BIGINT NOT NULL, c_name VARCHAR NOT NULL, c_address VARCHAR NOT NULL, c_nationkey INTEGER NOT NULL, c_phone VARCHAR NOT NULL, c_acctbal DECIMAL(15,2) NOT NULL, c_mktsegment VARCHAR NOT NULL, c_comment VARCHAR NOT NULL);
+CREATE TABLE lineitem(l_orderkey BIGINT NOT NULL, l_partkey BIGINT NOT NULL, l_suppkey BIGINT NOT NULL, l_linenumber BIGINT NOT NULL, l_quantity DECIMAL(15,2) NOT NULL, l_extendedprice DECIMAL(15,2) NOT NULL, l_discount DECIMAL(15,2) NOT NULL, l_tax DECIMAL(15,2) NOT NULL, l_returnflag VARCHAR NOT NULL, l_linestatus VARCHAR NOT NULL, l_shipdate DATE NOT NULL, l_commitdate DATE NOT NULL, l_receiptdate DATE NOT NULL, l_shipinstruct VARCHAR NOT NULL, l_shipmode VARCHAR NOT NULL, l_comment VARCHAR NOT NULL);
+CREATE TABLE nation(n_nationkey INTEGER NOT NULL, n_name VARCHAR NOT NULL, n_regionkey INTEGER NOT NULL, n_comment VARCHAR NOT NULL);
+CREATE TABLE orders(o_orderkey BIGINT NOT NULL, o_custkey BIGINT NOT NULL, o_orderstatus VARCHAR NOT NULL, o_totalprice DECIMAL(15,2) NOT NULL, o_orderdate DATE NOT NULL, o_orderpriority VARCHAR NOT NULL, o_clerk VARCHAR NOT NULL, o_shippriority INTEGER NOT NULL, o_comment VARCHAR NOT NULL);
+CREATE TABLE part(p_partkey BIGINT NOT NULL, p_name VARCHAR NOT NULL, p_mfgr VARCHAR NOT NULL, p_brand VARCHAR NOT NULL, p_type VARCHAR NOT NULL, p_size INTEGER NOT NULL, p_container VARCHAR NOT NULL, p_retailprice DECIMAL(15,2) NOT NULL, p_comment VARCHAR NOT NULL);
+CREATE TABLE partsupp(ps_partkey BIGINT NOT NULL, ps_suppkey BIGINT NOT NULL, ps_availqty BIGINT NOT NULL, ps_supplycost DECIMAL(15,2) NOT NULL, ps_comment VARCHAR NOT NULL);
+CREATE TABLE region(r_regionkey INTEGER NOT NULL, r_name VARCHAR NOT NULL, r_comment VARCHAR NOT NULL);
+CREATE TABLE supplier(s_suppkey BIGINT NOT NULL, s_name VARCHAR NOT NULL, s_address VARCHAR NOT NULL, s_nationkey INTEGER NOT NULL, s_phone VARCHAR NOT NULL, s_acctbal DECIMAL(15,2) NOT NULL, s_comment VARCHAR NOT NULL);''')
+        .statement('.tables')
+        .statement('describe lineitem')
+        .statement('show tables')
+    )
+
+    result = test.run()
+    result.check_stdout("decimal")
+    result.check_stdout("not null")
+
+def test_long_table_name(shell):
+    test = (
+        ShellTest(shell)
+        .statement(".maxwidth 80")
+        .statement(f'create table "abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyz"(i int);')
+        .statement('.tables')
+    )
+
+    result = test.run()
+    result.check_stdout("")
+
+def test_long_column_name(shell):
+    test = (
+        ShellTest(shell)
+        .statement(".maxwidth 80")
+        .statement(f'describe select 42 as "abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyz";')
+    )
+
+    result = test.run()
+    result.check_stdout("")
+
+def test_long_type_name(shell):
+    test = (
+        ShellTest(shell)
+        .statement(".maxwidth 80")
+        .statement('describe select {"abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyz": 42} as a;')
+    )
+
+    result = test.run()
+    result.check_stdout("")
+
+def test_long_type_and_column_name(shell):
+    test = (
+        ShellTest(shell)
+        .statement(".maxwidth 80")
+        .statement('describe select {"abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyz": 42} as "abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyz";')
+    )
+
+    result = test.run()
+    result.check_stdout("")
+
+def test_table_rendering_db(shell):
+    test = (
+        ShellTest(shell)
+        .statement('create schema s1')
+        .statement("attach ':memory:' as mydb")
+        .statement('create schema mydb.s2')
+        .statement('create table s1.my_tbl(i integer)')
+        .statement('create table mydb.s2.other_tbl(i integer)')
+        .statement('.tables')
+    )
+
+    result = test.run()
+    result.check_stdout("s1")
+    result.check_stdout("mydb")
+    result.check_stdout("s2")
+    result.check_stdout("other_tbl")
+
+def test_search_path_influences_table_name(shell):
+    test = (
+        ShellTest(shell)
+        .statement("attach ':memory:' as mydb")
+        .statement('create schema mydb.s2')
+        .statement('create table mydb.s2.other_tbl(i integer)')
+        .statement('use mydb.s2')
+        .statement('.tables')
+    )
+
+    result = test.run()
+    result.check_not_exist("mydb.s2")
